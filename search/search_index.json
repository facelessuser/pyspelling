{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Basic Usage \ue157 Overview \ue157 PySpelling is a module to help with automating spell checking in a project with Aspell or Hunspell . It is essentially a wrapper around the command line utility of these two spell checking tools, and allows you to setup different spelling tasks for different file types. You can apply specific and different filters and options to each task. PySpelling can also be used in CI environments to fail the build if there are misspelled words. Aspell and Hunspell are very good spell checking tools. Aspell particularly comes with a couple of filters, but the filters are limited in types and aren't extremely flexible. PySpelling was created to work around Aspell's and Hunspell's filtering shortcomings by creating a wrapper around them that could be extended to handle more kinds of file formats and provide more advanced filtering. If you need to filter out specific HTML tags with specific IDs or class names, PySpelling can do it. If you want to scan Python files for docstrings, but also avoid specific content within the docstring, you can do that as well. If PySpelling doesn't have a filter you need, with access to so many available Python modules, you can easily write your own. Computer:pyspelling facelessuser$ pyspelling Misspelled words: <html-content> site/index.html: P -------------------------------------------------------------------------------- cheking particularlly -------------------------------------------------------------------------------- Misspelled words: <context> pyspelling/__meta__.py(41): Pep440Version -------------------------------------------------------------------------------- Accessors accessor -------------------------------------------------------------------------------- !!!Spelling check failed!!! Prerequisites \ue157 PySpelling is a wrapper around either Aspell or Hunspell. If you do not have a working Aspell or Hunspell on your system, PySpelling will not work. It is up to the user to either build locally or acquire via a package manager a working spell checker installation. PySpelling pre-processes files with Python filters, and then sends the resulting text to the preferred spell checker via command line. Installing \ue157 Installation is easy with pip: pip install pyspelling If you want to manually install it, run python setup.py build and python setup.py install . Command Line Usage \ue157 usage: spellcheck [-h] [--version] [--verbose] [--name NAME] [--binary BINARY] [--config CONFIG] [--spellchecker SPELLCHECKER] Spell checking tool. optional arguments: -h, --help show this help message and exit --version show program's version number and exit --verbose, -v Verbosity level. --name NAME, -n NAME Specific spelling task by name to run. --binary BINARY, -b BINARY Provide path to spell checker's binary. --config CONFIG, -c CONFIG Spelling config. --spellchecker SPELLCHECKER, -s SPELLCHECKER Choose between aspell and hunspell PySpelling can be run with the command below (assuming your Python bin/script folder is in your path). By default it will look for the spelling configuration file .pyspelling.yml . pyspelling If you have multiple Python versions, you can run the PySpelling associated with that Python version by appending the Python major and minor version: pyspelling3.7 To specify a specific configuration other than the default, or even point to a different location: pyspelling -c myconfig.yml To run a specific spelling task in your configuration file by name, you can use the name option. You can even specify multiple names if desired. You cannot use name and group together: pyspelling -n my_task -n my_task2 If you've specified groups for your tasks, you can run all tasks in a group with the group option. You can specify multiple groups if desired. You cannot use name and group together. pyspelling -g my_group -g my_group2 If you've specified exactly one name via the name option, you can override that named task's source patterns with the source option. You can specify multiple source patterns if desired. pyspelling -n my_task -S \"this/specific/file.txt\" -S \"these/specific/files_{a,b}.txt\" To run a more verbose output, use the -v flag. You can increase verbosity level by including more v s: -vv . You can currently go up to four levels. pyspelling -v If the binary for your spell checker is not found in your path, you can provide a path to the binary. pyspelling -b \"path/to/aspell\" You can specify the spell checker type by specifying it on the command line. PySpelling supports hunspell and aspell , but defaults to aspell . This will override the preferred spellchecker setting in the configuration file. pyspelling -s hunspell Supported Spell Check Versions \ue157 PySpelling is tested with Hunspell 1.6+, and recommends using only 1.6 and above. Some lower versions might work, but none have been tested, and related issues will probably not be addressed. I usually patch the English Hunspell dictionary that I use to add apostrophes, if not present. Apostrophe support is a must for me. I also prefer to not include numbers as word characters (like Aspell) does as I find them problematic, but this is just my personal preference. Below is a patch I use on an OpenOffice dictionary set ( git://anongit.freedesktop.org/libreoffice/dictionaries ). diff --git a/en/en_US.aff b/en/en_US.aff index d0cccb3..4258f85 100644 --- a/en/en_US.aff +++ b/en/en_US.aff @@ -14,7 +14,7 @@ ONLYINCOMPOUND c COMPOUNDRULE 2 COMPOUNDRULE n*1t COMPOUNDRULE n*mp -WORDCHARS 0123456789 +WORDCHARS \u2019 PFX A Y 1 PFX A 0 re PySpelling is also tested on Aspell 0.60+ (which is recommended), but should also work on the 0.50 series. 0.60+ is recommended as spell checking is better in the 0.60 series. PySpelling disables all native Aspell filters by default. If you need to enable Aspell's native filters, you can do so via Aspell's builtin options. For more information, see Aspell configuration options . New in 2.4.0 Starting in 2.4.0, PySpelling ensures filters that are native to the spell checker are disabled by default. Usage in Linux \ue157 Aspell and Hunspell is most likely available in your distro's package manager. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually. Ubuntu Aspell install example: sudo apt-get install aspell aspell-en Ubuntu Hunspell install example: sudo apt-get install hunspell hunspell-en-us Usage in macOS \ue157 Aspell and Hunspell can be included via package managers such as Homebrew. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually. Homebrew Aspell install examples: brew install aspell Homebrew Hunspell install examples: brew install hunspell Don't forget to download dictionaries and put them to /Library/Spelling/ . Usage in Windows \ue157 Installing Aspell and/or Hunspell in Windows is traditionally done through either a Cygwin or MSYS2/MinGW environment. If using MYSYS2/MinGW, you can usually install both packages via Pacman. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually. Pacman Aspell install example: pacman -S mingw-w64-x86_64-aspell mingw-w64-x86_64-aspell-en For Aspell, it has been noted that the way the default configuration is configured, builtin Aspell filters are often inaccessible as the configuration seems to configure paths with mixed, incompatible slash style (backslash and forward slash). By creating your own override configuration, and using forward slashes only can fix the issue. You must manually specify a proper data-dir and dict-dir override path. This is done in our appveyor.yml file for our own personal tests, so you can check it out to see what is done. After fixing the configuration file, you should have everything working. Pacman Hunspell install example: pacman -S mingw-w64-x86_64-hunspell mingw-w64-x86_64-hunspell-en If you are dealing with Unicode text, Windows often has difficulty showing it in the console. Using Windows Unicode Console to patch your Windows install can help. On Python 3.6+ it might not be needed at all. Certain specialty consoles on Windows may report confusing information related to what encoding is used in the console. It is left to the user to resolve console Unicode issues, though proposals for better ways to handle this would be considered.","title":"Basic Usage"},{"location":"#basic-usage","text":"","title":"Basic Usage"},{"location":"#overview","text":"PySpelling is a module to help with automating spell checking in a project with Aspell or Hunspell . It is essentially a wrapper around the command line utility of these two spell checking tools, and allows you to setup different spelling tasks for different file types. You can apply specific and different filters and options to each task. PySpelling can also be used in CI environments to fail the build if there are misspelled words. Aspell and Hunspell are very good spell checking tools. Aspell particularly comes with a couple of filters, but the filters are limited in types and aren't extremely flexible. PySpelling was created to work around Aspell's and Hunspell's filtering shortcomings by creating a wrapper around them that could be extended to handle more kinds of file formats and provide more advanced filtering. If you need to filter out specific HTML tags with specific IDs or class names, PySpelling can do it. If you want to scan Python files for docstrings, but also avoid specific content within the docstring, you can do that as well. If PySpelling doesn't have a filter you need, with access to so many available Python modules, you can easily write your own. Computer:pyspelling facelessuser$ pyspelling Misspelled words: <html-content> site/index.html: P -------------------------------------------------------------------------------- cheking particularlly -------------------------------------------------------------------------------- Misspelled words: <context> pyspelling/__meta__.py(41): Pep440Version -------------------------------------------------------------------------------- Accessors accessor -------------------------------------------------------------------------------- !!!Spelling check failed!!!","title":"Overview"},{"location":"#prerequisites","text":"PySpelling is a wrapper around either Aspell or Hunspell. If you do not have a working Aspell or Hunspell on your system, PySpelling will not work. It is up to the user to either build locally or acquire via a package manager a working spell checker installation. PySpelling pre-processes files with Python filters, and then sends the resulting text to the preferred spell checker via command line.","title":"Prerequisites"},{"location":"#installing","text":"Installation is easy with pip: pip install pyspelling If you want to manually install it, run python setup.py build and python setup.py install .","title":"Installing"},{"location":"#command-line-usage","text":"usage: spellcheck [-h] [--version] [--verbose] [--name NAME] [--binary BINARY] [--config CONFIG] [--spellchecker SPELLCHECKER] Spell checking tool. optional arguments: -h, --help show this help message and exit --version show program's version number and exit --verbose, -v Verbosity level. --name NAME, -n NAME Specific spelling task by name to run. --binary BINARY, -b BINARY Provide path to spell checker's binary. --config CONFIG, -c CONFIG Spelling config. --spellchecker SPELLCHECKER, -s SPELLCHECKER Choose between aspell and hunspell PySpelling can be run with the command below (assuming your Python bin/script folder is in your path). By default it will look for the spelling configuration file .pyspelling.yml . pyspelling If you have multiple Python versions, you can run the PySpelling associated with that Python version by appending the Python major and minor version: pyspelling3.7 To specify a specific configuration other than the default, or even point to a different location: pyspelling -c myconfig.yml To run a specific spelling task in your configuration file by name, you can use the name option. You can even specify multiple names if desired. You cannot use name and group together: pyspelling -n my_task -n my_task2 If you've specified groups for your tasks, you can run all tasks in a group with the group option. You can specify multiple groups if desired. You cannot use name and group together. pyspelling -g my_group -g my_group2 If you've specified exactly one name via the name option, you can override that named task's source patterns with the source option. You can specify multiple source patterns if desired. pyspelling -n my_task -S \"this/specific/file.txt\" -S \"these/specific/files_{a,b}.txt\" To run a more verbose output, use the -v flag. You can increase verbosity level by including more v s: -vv . You can currently go up to four levels. pyspelling -v If the binary for your spell checker is not found in your path, you can provide a path to the binary. pyspelling -b \"path/to/aspell\" You can specify the spell checker type by specifying it on the command line. PySpelling supports hunspell and aspell , but defaults to aspell . This will override the preferred spellchecker setting in the configuration file. pyspelling -s hunspell","title":"Command Line Usage"},{"location":"#supported-spell-check-versions","text":"PySpelling is tested with Hunspell 1.6+, and recommends using only 1.6 and above. Some lower versions might work, but none have been tested, and related issues will probably not be addressed. I usually patch the English Hunspell dictionary that I use to add apostrophes, if not present. Apostrophe support is a must for me. I also prefer to not include numbers as word characters (like Aspell) does as I find them problematic, but this is just my personal preference. Below is a patch I use on an OpenOffice dictionary set ( git://anongit.freedesktop.org/libreoffice/dictionaries ). diff --git a/en/en_US.aff b/en/en_US.aff index d0cccb3..4258f85 100644 --- a/en/en_US.aff +++ b/en/en_US.aff @@ -14,7 +14,7 @@ ONLYINCOMPOUND c COMPOUNDRULE 2 COMPOUNDRULE n*1t COMPOUNDRULE n*mp -WORDCHARS 0123456789 +WORDCHARS \u2019 PFX A Y 1 PFX A 0 re PySpelling is also tested on Aspell 0.60+ (which is recommended), but should also work on the 0.50 series. 0.60+ is recommended as spell checking is better in the 0.60 series. PySpelling disables all native Aspell filters by default. If you need to enable Aspell's native filters, you can do so via Aspell's builtin options. For more information, see Aspell configuration options . New in 2.4.0 Starting in 2.4.0, PySpelling ensures filters that are native to the spell checker are disabled by default.","title":"Supported Spell Check Versions"},{"location":"#usage-in-linux","text":"Aspell and Hunspell is most likely available in your distro's package manager. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually. Ubuntu Aspell install example: sudo apt-get install aspell aspell-en Ubuntu Hunspell install example: sudo apt-get install hunspell hunspell-en-us","title":"Usage in Linux"},{"location":"#usage-in-macos","text":"Aspell and Hunspell can be included via package managers such as Homebrew. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually. Homebrew Aspell install examples: brew install aspell Homebrew Hunspell install examples: brew install hunspell Don't forget to download dictionaries and put them to /Library/Spelling/ .","title":"Usage in macOS"},{"location":"#usage-in-windows","text":"Installing Aspell and/or Hunspell in Windows is traditionally done through either a Cygwin or MSYS2/MinGW environment. If using MYSYS2/MinGW, you can usually install both packages via Pacman. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually. Pacman Aspell install example: pacman -S mingw-w64-x86_64-aspell mingw-w64-x86_64-aspell-en For Aspell, it has been noted that the way the default configuration is configured, builtin Aspell filters are often inaccessible as the configuration seems to configure paths with mixed, incompatible slash style (backslash and forward slash). By creating your own override configuration, and using forward slashes only can fix the issue. You must manually specify a proper data-dir and dict-dir override path. This is done in our appveyor.yml file for our own personal tests, so you can check it out to see what is done. After fixing the configuration file, you should have everything working. Pacman Hunspell install example: pacman -S mingw-w64-x86_64-hunspell mingw-w64-x86_64-hunspell-en If you are dealing with Unicode text, Windows often has difficulty showing it in the console. Using Windows Unicode Console to patch your Windows install can help. On Python 3.6+ it might not be needed at all. Certain specialty consoles on Windows may report confusing information related to what encoding is used in the console. It is left to the user to resolve console Unicode issues, though proposals for better ways to handle this would be considered.","title":"Usage in Windows"},{"location":"api/","text":"Plugin API \ue157 Filters \ue157 When writing a Filter plugin, there are two classes to be aware: Filter and SourceText . Both classes are found in pyspelling.filters . Each chunk returned by a filter is a SourceText object. These objects contain the desired, filtered text from the previous filter along with some metadata: encoding, display context, and a category that describes what kind of text the data is. After all filters have processed the text, each SourceText 's content is finally passed to the spell checker. The text data in a SourceText object is always Unicode, but during the filtering process, the filter can decode the Unicode if required as long as it is returned as Unicode at the end of the step. Filter \ue157 Filter plugins are subclassed from the Filter class. You'll often want to specify the defaulted value for default_encoding in the __init__ . Simply give it a default value as shown below. from .. import filters class MyFilter ( filters . Filter ): \"\"\"Spelling Filter.\"\"\" def __init__ ( self , options , default_encoding = 'utf-8' ): \"\"\"Initialization.\"\"\" super () . __init__ ( options , default_encoding ) Filter.get_default_config \ue157 get_default_config is where you should specify your default configuration file. This should contain all accepted options and their default value. All user options that are passed in will override the defaults. If an option is passed in that is not found in the defaults, an error will be raised. def get_default_config ( self ): \"\"\"Get default configuration.\"\"\" return { \"enable_something\" : True , \"list_of_stuff\" : [ 'some' , 'stuff' ] } New 2.0 get_default_confg was added in version 2.0 . Filter.validate_options \ue157 validate_options is where you can specify validation of your options. By default, basic validation is done on incoming options. For instance, if you specify a default as a bool , the default validator will ensure the passed user options match. Checking is performed on bool , str , list , dict , int , and float types. Nothing beyond simple type checking is performed, so if you had some custom validation, or simply wanted to bypass the default validator with your own, you should override validate_options . def validate_options ( self , k , v ): \"\"\"Validate options.\"\"\" # Call the basic validator super () . validate_options ( k , v ) # Perform custom validation if k == \"foo\" and v != \"bar\" : raise ValueError ( \"Value should be 'bar' for 'foo'\" ) New 2.0 validate_options was added in version 2.0 . Filter.setup \ue157 setup is were basic setup can be performed post-validation. At this point, you can access the merged and validated configuration via self.config . def setup ( self ): \"\"\"Setup.\"\"\" self . enable_foo = self . config [ 'foo' ] New 2.0 setup was added in version 2.0 . Filter.reset \ue157 reset is called on every new call to the plugin. It allows you to clean up states from previous calls. def reset ( self ): \"\"\"Reset\"\"\" self . counter = 0 self . tracked_stuff = [] New 2.0 reset was added in version 2.0 . Filter.has_bom \ue157 has_bom takes a file stream and is usually used to check the first few bytes. While BOM checking could be performed in header_check , this mainly provided as UTF BOMs are quite common in many file types, so a specific test was dedicated to it. Additionally, this replaces the old, less flexible CHECK_BOM attribute that was deprecated in version 1.2 . This is useful if you want to handle binary parsing, or a file type that has a custom BOM in the header. When returning encoding in any of the encoding check functions, None means no encoding was detecting, an empty string means binary data (encoding validation is skipped), and anything else will be validated and passed through. Just be sure to include a sensible encoding in your SourceText object when your plugin returns file content. def has_bom ( self , filestream ): \"\"\"Check if has BOM.\"\"\" content = filestream . read ( 2 ) if content == b 'PK \\x03\\x04 ' : # Zip file found. # Return `BINARY_ENCODE` as content is binary type, # but don't return None which means we don't know what we have. return filters . BINARY_ENCODE # Not a zip file, so pass it on to the normal file checker. return super () . has_bom ( filestream ) New 2.0 has_bom was added in version 2.0 . Deprecation 2.0 CHECK_BOM has been deprecated since 2.0 . Filter.header_check \ue157 header_check is a function that receives the first 1024 characters of the file via content that can be scanned for an encoding header. A string with the encoding name should be returned or None if a valid encoding header cannot be found. def header_check ( self , content ): \"\"\"Special encode check.\"\"\" return None Filter.content_check \ue157 content_check receives a file object which allows you to check the entire file buffer to determine encoding. A string with the encoding name should be returned or None if a valid encoding header cannot be found. def content_check ( self , filestream ): \"\"\"File content check.\"\"\" return None Filter.filter \ue157 filter is called when the Filter object is the first in the chain. This means the file has not been read from disk yet, so we must handle opening the file before applying the filter and then return a list of SourceText objects. The first filter in the chain is handled differently in order to give the opportunity to handle files that require more complex methods to acquire the Unicode strings. You can read the file in binary format or directly to Unicode. You can run parsers or anything else you need in order to get the required Unicode text for the SourceText objects. You can create as many SourceText objects as you desired and assign them categories so that other Filter objects can avoid them if desired. Below is the default which reads the entire file into a single object providing the file name as the context, the encoding, and the category text . def filter ( self , source_file , encoding ): # noqa A001 \"\"\"Open and filter the file from disk.\"\"\" with codecs . open ( source_file , 'r' , encoding = encoding ) as f : text = f . read () return [ SourceText ( text , source_file , encoding , 'text' )] Filter.sfilter \ue157 sfilter is called for all Filter objects following the first. The function is passed a SourceText object from which the text, context, encoding can all be extracted. Here you can manipulate the text back to bytes if needed, wrap the text in an io.StreamIO object to act as a file stream, run parsers, or anything you need to manipulate the buffer to filter the Unicode text for the SourceText objects. def sfilter ( self , source ): \"\"\"Execute filter.\"\"\" return [ SourceText ( source . text , source . context , source . encoding , 'text' )] If a filter only works either as the first in the chain, or only as a secondary filter in the chain, you could raise an exception if needed. In most cases, you should be able to have an appropriate filter and sfilter , but there are most likely cases (particular when dealing with binary data) where only a filter method could be provided. Check out the default filter plugins provided with the source to see real world examples. get_plugin \ue157 And don't forget to provide a function in the file called get_plugin ! get_plugin is the entry point and should return your Filter object. def get_plugin (): \"\"\"Return the filter.\"\"\" return HtmlFilter SourceText \ue157 As previously mentioned, filters must return a list of SourceText objects. class SourceText ( namedtuple ( 'SourceText' , [ 'text' , 'context' , 'encoding' , 'category' , 'error' ])): \"\"\"Source text.\"\"\" Each object should contain a Unicode string ( text ), some context on the given text hunk ( context ), the encoding which the Unicode text was originally in ( encoding ), and a category that is used to omit certain hunks from other filters in the chain ( category ). SourceText should not contain byte strings, and if they do, they will not be passed to additional filters. error is optional and is only provided message when something goes wrong. When receiving a SourceText object in your plugin, you can access the the content via attributes with the same name as the parameters above: >>> source . text 'Some Text' >>> source . context 'foo.txt' >>> source . encoding 'utf-8' >>> source . category 'some-category' Be mindful when adjusting the context in subsequent items in the pipeline chain. Generally you should only append additional context so as not to wipe out previous contextual data. It may not always make sense to append additional data, so some filters might just pass the previous context as the new context. If you have a particular chunk of text that has a problem, you can return an error in the SourceText object. Errors really only need a context and the error as they won't be passed to the spell checker or to any subsequent steps in the pipeline. Errors are only used to alert the user that something went wrong. SourceText objects with errors will not be passed down the chain and will not be passed to the spell checker. if error : content = [ SourceText ( '' , source_file , '' , '' , error )] Flow Control \ue157 FlowControl plugins are simple plugins that take the category from a SourceText object, and simply returns either the directive HALT , SKIP , or ALLOW . This controls whether the associated SourceText object's progress is halted in the pipeline, skips the next filter, or is explicitly allowed in the next filter. The directives and FlowControl class are found in pyspelling.flow_control . FlowControl \ue157 FlowControl plugins should be subclassed from FlowControl . If you need to you can override the __init__ , but remember to call the original with super to ensure options are handled. class MyFlowControl ( flow_control . FlowControl ): \"\"\"Flow control plugin.\"\"\" def __init__ ( self , config ): \"\"\"Initialization.\"\"\" super () . __init__ ( config ) FlowControl.get_default_config \ue157 get_default_config is where you should specify your default configuration file. This should contain all accepted options and their default value. All user options that are passed in will override the defaults. If an option is passed in that is not found in the defaults, an error will be raised. def get_default_config ( self ): \"\"\"Get default configuration.\"\"\" return { \"enable_something\" : True , \"list_of_stuff\" : [ 'some' , 'stuff' ] } New 2.0 get_default_confg was added in version 2.0 . FlowControl.validate_options \ue157 validate_options is where you can specify validation of your options. By default, basic validation is done on incoming options. For instance, if you specify a default as a bool , the default validator will ensure the passed user options match. Checking is performed on bool , str , list , dict , int , and float types. Nothing beyond simple type checking is performed, so if you had some custom validation, or simply wanted to bypass the default validator with your own, you should override validate_options . def validate_options ( self , k , v ): \"\"\"Validate options.\"\"\" # Call the basic validator super () . validate_options ( k , v ) # Perform custom validation if k == \"foo\" and v != \"bar\" : raise ValueError ( \"Value should be 'bar' for 'foo'\" ) New 2.0 validate_options was added in version 2.0 . FlowControl.setup \ue157 setup is were basic setup can be performed post-validation. At this point, you can access the merged and validated configuration via self.config . def setup ( self ): \"\"\"Setup.\"\"\" self . enable_foo = self . config [ 'foo' ] New 2.0 setup was added in version 2.0 . FlowControl.reset \ue157 reset is called on every new call to the plugin. It allows you to clean up states from previous calls. def reset ( self ): \"\"\"Reset\"\"\" self . counter = 0 self . tracked_stuff = [] New 2.0 reset was added in version 2.0 . FlowControl.adjust_flow \ue157 After handling the options, there is only one other function available for overrides: adjust_flow . Adjust flow receives the category from the SourceText being passed down the pipeline. Here the decision is made to as to what must be done with the object. Simply return HALT , SKIP , or ALLOW to control the flow for that SourceText object. def adjust_flow ( self , category ): \"\"\"Adjust the flow of source control objects.\"\"\" status = flow_control . SKIP for allow in self . allow : if fnmatch . fnmatch ( category , allow , flags = self . FNMATCH_FLAGS ): status = flow_control . ALLOW for skip in self . skip : if fnmatch . fnmatch ( category , skip , flags = self . FNMATCH_FLAGS ): status = flow_control . SKIP for halt in self . halt : if fnmatch . fnmatch ( category , halt , flags = self . FNMATCH_FLAGS ): status = flow_control . HALT if status != flow_control . ALLOW : break return status Check out the default flow control plugins provided with the source to see real world examples. get_plugin \ue157 And don't forget to provide a function in the file called get_plugin ! get_plugin is the entry point and should return your FlowControl object. def get_plugin (): \"\"\"Get flow controller.\"\"\" return WildcardFlowControl","title":"Plugin API"},{"location":"api/#plugin-api","text":"","title":"Plugin API"},{"location":"api/#filters","text":"When writing a Filter plugin, there are two classes to be aware: Filter and SourceText . Both classes are found in pyspelling.filters . Each chunk returned by a filter is a SourceText object. These objects contain the desired, filtered text from the previous filter along with some metadata: encoding, display context, and a category that describes what kind of text the data is. After all filters have processed the text, each SourceText 's content is finally passed to the spell checker. The text data in a SourceText object is always Unicode, but during the filtering process, the filter can decode the Unicode if required as long as it is returned as Unicode at the end of the step.","title":"Filters"},{"location":"api/#filter","text":"Filter plugins are subclassed from the Filter class. You'll often want to specify the defaulted value for default_encoding in the __init__ . Simply give it a default value as shown below. from .. import filters class MyFilter ( filters . Filter ): \"\"\"Spelling Filter.\"\"\" def __init__ ( self , options , default_encoding = 'utf-8' ): \"\"\"Initialization.\"\"\" super () . __init__ ( options , default_encoding )","title":"Filter"},{"location":"api/#filterget_default_config","text":"get_default_config is where you should specify your default configuration file. This should contain all accepted options and their default value. All user options that are passed in will override the defaults. If an option is passed in that is not found in the defaults, an error will be raised. def get_default_config ( self ): \"\"\"Get default configuration.\"\"\" return { \"enable_something\" : True , \"list_of_stuff\" : [ 'some' , 'stuff' ] } New 2.0 get_default_confg was added in version 2.0 .","title":"Filter.get_default_config"},{"location":"api/#filtervalidate_options","text":"validate_options is where you can specify validation of your options. By default, basic validation is done on incoming options. For instance, if you specify a default as a bool , the default validator will ensure the passed user options match. Checking is performed on bool , str , list , dict , int , and float types. Nothing beyond simple type checking is performed, so if you had some custom validation, or simply wanted to bypass the default validator with your own, you should override validate_options . def validate_options ( self , k , v ): \"\"\"Validate options.\"\"\" # Call the basic validator super () . validate_options ( k , v ) # Perform custom validation if k == \"foo\" and v != \"bar\" : raise ValueError ( \"Value should be 'bar' for 'foo'\" ) New 2.0 validate_options was added in version 2.0 .","title":"Filter.validate_options"},{"location":"api/#filtersetup","text":"setup is were basic setup can be performed post-validation. At this point, you can access the merged and validated configuration via self.config . def setup ( self ): \"\"\"Setup.\"\"\" self . enable_foo = self . config [ 'foo' ] New 2.0 setup was added in version 2.0 .","title":"Filter.setup"},{"location":"api/#filterreset","text":"reset is called on every new call to the plugin. It allows you to clean up states from previous calls. def reset ( self ): \"\"\"Reset\"\"\" self . counter = 0 self . tracked_stuff = [] New 2.0 reset was added in version 2.0 .","title":"Filter.reset"},{"location":"api/#filterhas_bom","text":"has_bom takes a file stream and is usually used to check the first few bytes. While BOM checking could be performed in header_check , this mainly provided as UTF BOMs are quite common in many file types, so a specific test was dedicated to it. Additionally, this replaces the old, less flexible CHECK_BOM attribute that was deprecated in version 1.2 . This is useful if you want to handle binary parsing, or a file type that has a custom BOM in the header. When returning encoding in any of the encoding check functions, None means no encoding was detecting, an empty string means binary data (encoding validation is skipped), and anything else will be validated and passed through. Just be sure to include a sensible encoding in your SourceText object when your plugin returns file content. def has_bom ( self , filestream ): \"\"\"Check if has BOM.\"\"\" content = filestream . read ( 2 ) if content == b 'PK \\x03\\x04 ' : # Zip file found. # Return `BINARY_ENCODE` as content is binary type, # but don't return None which means we don't know what we have. return filters . BINARY_ENCODE # Not a zip file, so pass it on to the normal file checker. return super () . has_bom ( filestream ) New 2.0 has_bom was added in version 2.0 . Deprecation 2.0 CHECK_BOM has been deprecated since 2.0 .","title":"Filter.has_bom"},{"location":"api/#filterheader_check","text":"header_check is a function that receives the first 1024 characters of the file via content that can be scanned for an encoding header. A string with the encoding name should be returned or None if a valid encoding header cannot be found. def header_check ( self , content ): \"\"\"Special encode check.\"\"\" return None","title":"Filter.header_check"},{"location":"api/#filtercontent_check","text":"content_check receives a file object which allows you to check the entire file buffer to determine encoding. A string with the encoding name should be returned or None if a valid encoding header cannot be found. def content_check ( self , filestream ): \"\"\"File content check.\"\"\" return None","title":"Filter.content_check"},{"location":"api/#filterfilter","text":"filter is called when the Filter object is the first in the chain. This means the file has not been read from disk yet, so we must handle opening the file before applying the filter and then return a list of SourceText objects. The first filter in the chain is handled differently in order to give the opportunity to handle files that require more complex methods to acquire the Unicode strings. You can read the file in binary format or directly to Unicode. You can run parsers or anything else you need in order to get the required Unicode text for the SourceText objects. You can create as many SourceText objects as you desired and assign them categories so that other Filter objects can avoid them if desired. Below is the default which reads the entire file into a single object providing the file name as the context, the encoding, and the category text . def filter ( self , source_file , encoding ): # noqa A001 \"\"\"Open and filter the file from disk.\"\"\" with codecs . open ( source_file , 'r' , encoding = encoding ) as f : text = f . read () return [ SourceText ( text , source_file , encoding , 'text' )]","title":"Filter.filter"},{"location":"api/#filtersfilter","text":"sfilter is called for all Filter objects following the first. The function is passed a SourceText object from which the text, context, encoding can all be extracted. Here you can manipulate the text back to bytes if needed, wrap the text in an io.StreamIO object to act as a file stream, run parsers, or anything you need to manipulate the buffer to filter the Unicode text for the SourceText objects. def sfilter ( self , source ): \"\"\"Execute filter.\"\"\" return [ SourceText ( source . text , source . context , source . encoding , 'text' )] If a filter only works either as the first in the chain, or only as a secondary filter in the chain, you could raise an exception if needed. In most cases, you should be able to have an appropriate filter and sfilter , but there are most likely cases (particular when dealing with binary data) where only a filter method could be provided. Check out the default filter plugins provided with the source to see real world examples.","title":"Filter.sfilter"},{"location":"api/#get_plugin","text":"And don't forget to provide a function in the file called get_plugin ! get_plugin is the entry point and should return your Filter object. def get_plugin (): \"\"\"Return the filter.\"\"\" return HtmlFilter","title":"get_plugin"},{"location":"api/#sourcetext","text":"As previously mentioned, filters must return a list of SourceText objects. class SourceText ( namedtuple ( 'SourceText' , [ 'text' , 'context' , 'encoding' , 'category' , 'error' ])): \"\"\"Source text.\"\"\" Each object should contain a Unicode string ( text ), some context on the given text hunk ( context ), the encoding which the Unicode text was originally in ( encoding ), and a category that is used to omit certain hunks from other filters in the chain ( category ). SourceText should not contain byte strings, and if they do, they will not be passed to additional filters. error is optional and is only provided message when something goes wrong. When receiving a SourceText object in your plugin, you can access the the content via attributes with the same name as the parameters above: >>> source . text 'Some Text' >>> source . context 'foo.txt' >>> source . encoding 'utf-8' >>> source . category 'some-category' Be mindful when adjusting the context in subsequent items in the pipeline chain. Generally you should only append additional context so as not to wipe out previous contextual data. It may not always make sense to append additional data, so some filters might just pass the previous context as the new context. If you have a particular chunk of text that has a problem, you can return an error in the SourceText object. Errors really only need a context and the error as they won't be passed to the spell checker or to any subsequent steps in the pipeline. Errors are only used to alert the user that something went wrong. SourceText objects with errors will not be passed down the chain and will not be passed to the spell checker. if error : content = [ SourceText ( '' , source_file , '' , '' , error )]","title":"SourceText"},{"location":"api/#flow-control","text":"FlowControl plugins are simple plugins that take the category from a SourceText object, and simply returns either the directive HALT , SKIP , or ALLOW . This controls whether the associated SourceText object's progress is halted in the pipeline, skips the next filter, or is explicitly allowed in the next filter. The directives and FlowControl class are found in pyspelling.flow_control .","title":"Flow Control"},{"location":"api/#flowcontrol","text":"FlowControl plugins should be subclassed from FlowControl . If you need to you can override the __init__ , but remember to call the original with super to ensure options are handled. class MyFlowControl ( flow_control . FlowControl ): \"\"\"Flow control plugin.\"\"\" def __init__ ( self , config ): \"\"\"Initialization.\"\"\" super () . __init__ ( config )","title":"FlowControl"},{"location":"api/#flowcontrolget_default_config","text":"get_default_config is where you should specify your default configuration file. This should contain all accepted options and their default value. All user options that are passed in will override the defaults. If an option is passed in that is not found in the defaults, an error will be raised. def get_default_config ( self ): \"\"\"Get default configuration.\"\"\" return { \"enable_something\" : True , \"list_of_stuff\" : [ 'some' , 'stuff' ] } New 2.0 get_default_confg was added in version 2.0 .","title":"FlowControl.get_default_config"},{"location":"api/#flowcontrolvalidate_options","text":"validate_options is where you can specify validation of your options. By default, basic validation is done on incoming options. For instance, if you specify a default as a bool , the default validator will ensure the passed user options match. Checking is performed on bool , str , list , dict , int , and float types. Nothing beyond simple type checking is performed, so if you had some custom validation, or simply wanted to bypass the default validator with your own, you should override validate_options . def validate_options ( self , k , v ): \"\"\"Validate options.\"\"\" # Call the basic validator super () . validate_options ( k , v ) # Perform custom validation if k == \"foo\" and v != \"bar\" : raise ValueError ( \"Value should be 'bar' for 'foo'\" ) New 2.0 validate_options was added in version 2.0 .","title":"FlowControl.validate_options"},{"location":"api/#flowcontrolsetup","text":"setup is were basic setup can be performed post-validation. At this point, you can access the merged and validated configuration via self.config . def setup ( self ): \"\"\"Setup.\"\"\" self . enable_foo = self . config [ 'foo' ] New 2.0 setup was added in version 2.0 .","title":"FlowControl.setup"},{"location":"api/#flowcontrolreset","text":"reset is called on every new call to the plugin. It allows you to clean up states from previous calls. def reset ( self ): \"\"\"Reset\"\"\" self . counter = 0 self . tracked_stuff = [] New 2.0 reset was added in version 2.0 .","title":"FlowControl.reset"},{"location":"api/#flowcontroladjust_flow","text":"After handling the options, there is only one other function available for overrides: adjust_flow . Adjust flow receives the category from the SourceText being passed down the pipeline. Here the decision is made to as to what must be done with the object. Simply return HALT , SKIP , or ALLOW to control the flow for that SourceText object. def adjust_flow ( self , category ): \"\"\"Adjust the flow of source control objects.\"\"\" status = flow_control . SKIP for allow in self . allow : if fnmatch . fnmatch ( category , allow , flags = self . FNMATCH_FLAGS ): status = flow_control . ALLOW for skip in self . skip : if fnmatch . fnmatch ( category , skip , flags = self . FNMATCH_FLAGS ): status = flow_control . SKIP for halt in self . halt : if fnmatch . fnmatch ( category , halt , flags = self . FNMATCH_FLAGS ): status = flow_control . HALT if status != flow_control . ALLOW : break return status Check out the default flow control plugins provided with the source to see real world examples.","title":"FlowControl.adjust_flow"},{"location":"api/#get_plugin_1","text":"And don't forget to provide a function in the file called get_plugin ! get_plugin is the entry point and should return your FlowControl object. def get_plugin (): \"\"\"Get flow controller.\"\"\" return WildcardFlowControl","title":"get_plugin"},{"location":"configuration/","text":"Configuration \ue157 Configuration File \ue157 PySpelling requires a YAML configuration file. The file defines the various spelling tasks along with their individual filters and options. You can optionally specify the preferred spell checker as a global option ( aspell is the default if not specified). This can be overridden on the command line. spellchecker : hunspell All of the spelling tasks are contained under the keyword matrix and are organized in a list: matrix : - task1 - task2 Each task requires, at the very least, a name and sources to search. Depending on your setup, you may need to set the dictionary to use as well. Each spell checker specifies their dictionary/language differently which is covered in more details in Spell Checker Options . matrix : - name : Python Source aspell : lang : en sources : - pyspelling/**/*.py You can also define more complicated tasks which will run your text through various filters before performing the spell checking by providing a custom pipeline. You can also add your own custom wordlists to extend the dictionary. matrix : - name : Python Source sources : - pyspelling/**/*.py aspell : lang : en dictionary : wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/python.dic pipeline : - pyspelling.filters.python : - pyspelling.filters.context : context_visible_first : true escapes : \\\\[\\\\`~] delimiters : # Ignore multiline content between fences (fences can have 3 or more back ticks) # ``` # content # ``` - open : '(?s)^(?P<open> *`{3,})$' close : '^(?P=open)$' # Ignore text between inline back ticks - open : '(?P<open>`+)' close : '(?P=open)' Name \ue157 Each spelling tasks should have a unique name and is defined with the name key. When using the command line --name option, the task with the matching name will be run. matrix : - name : python New Behavior 2.0 In 1.0 , names doubled as identifiers and groups. It became apparent for certain features that a unique name is desirable for targeting different tasks, while a group specifier should be implemented separately. In 2.0 , if multiple tasks have the same name, the last defined one will be the targeted task when requesting a named task. Use groups to target multiple grouped tasks. Groups \ue157 Each task can be assigned to a group. The group name can be shared with multiple tasks. All tasks in a group can be run by specifying the --group option with the name of the group on the command line. This option is only available in version 1.1 of the configuration file. matrix : - name : python group : some_name New 2.0 group was added in version 2.0 . Hidden \ue157 All tasks in a configuration file will be run if no name is specified. In version 1.1 of the configuration file, If a task enables the option hidden by setting it to true , that task will not be run automatically when no name is specified. hidden tasks will only be run if they are specifically mentioned by name . matrix : - name : python hidden : true New 2.0 group was added in version 2.0 . Default Encoding \ue157 When parsing a file, the encoding detection and translation of the data into Unicode is performed by the first filter in the pipeline. For instance, if HTML is the first, it may check for a BOM or look at the file's header to find the meta tag that specifies the file's encoding. If all encoding checks fail, the filter will usually apply an appropriate default encoding for the file content type (usually UTF-8, but check the specific filter's documentation to be sure). If needed, the filter's default encoding can be overridden in the task via the default_encoding key. After the first step in the pipeline, the text is passed around as Unicode which requires no Unicode detection. matrix : - name : markdown pipeline : - pyspelling.filters.text sources : - '**/*.md' default_encoding : utf-8 Once all filtering is complete, the text will be passed to the spell checker as byte strings, usually with the originally detected encoding (unless a filter specifically alters the encoding). The supported spell checkers are limited to very specific encodings, so if your file is using an unsupported encoding, it will fail. UTF-16 and UTF-32 is not really supported by Aspell and Hunspell, so at the end of the spell check pipeline, Unicode strings that have the associated encoding of UTF-16 or UTF-32 will be encoded with the compatible UTF-8. This does not apply to files being processed with a the pipeline disabled. When the pipeline is disabled, files are sent directly to the spell checker with no modifications. Unsupported Encodings If you are trying to spell check a file in an unsupported encoding, you can use the builtin text filter to convert the content to a more appropriate encoding. In general, it is recommended to work in, or convert to UTF-8. Sources \ue157 Each spelling task must define a list of sources to search via the sources key. Each source should be a glob pattern that should match one or more files. PySpelling will perform a search with these patterns to determine which files should be spell checked. You can also have multiple patterns on one line separated with | . When multiple patterns are defined like this, they are evaluated simultaneously. This is useful if you'd like to provide an exclusion pattern along with your file pattern. For instance, if we wanted to scan all python files in our folder, but exclude any in the build folder, we could provide the following pattern: **/*.py|!build/* . PySpelling uses Wildcard Match's glob library to perform the file globbing. By default, it uses the NEGATE , GLOBSTAR , and BRACE flags, but you can override the flag options with the glob_flags option. You can specify the flags by either their long name GLOBSTAR or their short name G . See Wildcard Match's documentation for more information on the available flags and what they do. matrix : - name : python pipeline : - pyspelling.filters.python : comments : false glob_flags : N|G|B sources : - pyspelling/**/*.py Expect Match \ue157 When processing the sources field it is expected to find at least one matching file. If no files are located it can be helpful to raise an error and this is the default behaviour. If it is not expected to always find a file then the expect_match configuration can be used to suppress the error. matrix : - name : markdown pipeline : - pyspelling.filters.text sources : - '**/*.md' expect_match : false default_encoding : utf-8 Pipeline \ue157 PySpelling allows you to define tasks that outline what kind of files you want to spell check, and then sends them down a pipeline that filters the content returning chunks of text with some associated context. Each chunk is sent down each step of the pipeline until it reaches the final step, the spell check step. Between filter steps, you can also insert flow control steps that allow you to have certain text chunks skip specific steps. All of this is done with pipeline plugins . Let's say you had some Markdown files and wanted to convert them to HTML, and then filter out specific tags. You could just use the Markdown filter to convert the file to HTML and then pass it through the HTML filter to extract the text from the HTML tags. matrix : - name : markdown sources : - README.md pipeline : - pyspelling.filters.markdown : - pyspelling.filters.html : comments : false attributes : - title - alt ignores : - code - pre If needed, you can also insert flow control steps before certain filter steps. Each text chunk that is passed between filters has a category assigned to it from the previous filter. Flow control steps allow you to restrict the next filter to specific categories, or exclude specific categories from the next step. This is covered in more depth in Flow Control . If for some reason you need to send the file directly to the spell checker without using PySpelling's pipeline, simply set pipeline to null . This sends file directly to the spell checker without evaluating the encoding or passing through any filters. Specifically with Hunspell, it also sends the spell checker the filename instead of piping the content as Hunspell has certain features that don't work when piping the data, such as OpenOffice ODF input. Below is an example where we send an OpenOffice ODF file directly to Hunspell in order to use Hunspell's -O option to parse the ODF file. Keep in mind that when doing this, no encoding is sent to the spell checker unless you define default_encoding . If default_encoding is not defined, PySpelling will decode the returned content with the terminal's encoding (or what it thinks the terminal's encoding is). matrix : - name : openoffice_ODF sources : - file.odt hunspell : d : en_US O : true pipeline : null Dictionaries and Personal Wordlists \ue157 By default, PySpelling sets your main dictionary to en for Aspell and en_US for Hunspell. If you do not desire an American English dictionary, or these dictionaries are not installed in their expected default locations, you will need to configure PySpelling so it can find your preferred dictionary. Since dictionary configuring varies for each spell checker, the main dictionary is configuration (and virtually any spell checker specific option) is performed via Spell Checker Options . For Aspell, you would use the command line option --lang or -l , which in the YAML configuration file is lang or l respectively. You can see we are just removing the leading - characters. matrix : - name : python aspell : lang : en For Hunspell, you would use the command line option -d , which in the YAML configuration file is d : matrix : - name : python hunspell : d : en_US While the dictionaries cover a number of commonly used words, they are usually not sufficient. Luckily, both Aspell and Hunspell allow for adding custom wordlists. You can have as many wordlists as you like, and they can be included in a list under the key wordlists which is also found under the key dictionary . While Hunspell doesn't directly compile the wordlists, Aspell does, and it uses the main dictionary that you have specified to accomplish this. All the wordlists are combined into one custom dictionary file whose output name and location is defined via the output key which is also found under the dictionary key. Lastly, you can set the encoding to be used during compilation via the encoding under dictionary . The encoding should generally match the encoding of your main dictionary. The default encoding is utf-8 , and only Aspell uses this option. matrix : - name : python sources : - pyspelling/**/*.py aspell : lang : en dictionary : wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/python.dic encoding : utf-8 pipeline : - pyspelling.filters.python : comments : false Spell Checker Options \ue157 Since PySpelling is a wrapper around both Aspell and Hunspell, there are a number of spell checker specific options. As only a few options are present in both, it was decided to expose them via spell checker specific keywords: aspell and hunspell for Aspell and Hunspell respectively. Here you can set options like the default dictionary and search options. Not all options are exposed though, only relevant search options are passed directly to the spell checker. Things like replace options (which aren't relevant in PySpelling) and encoding (which are handled internally by PySpelling) are not accessible. Spell checker specific options basically translate directly to the spell checker's command line options and only requires you to remove the leading - s you would normally specify on the command line. For instance, a short form option such as -l would simply be represented with the keyword l , and the long name form of the same option --lang would be represented as lang . Following the key, you would provide the appropriate value depending on it's requirement. Boolean flags would be set to true . matrix : - name : html sources : - docs/**/*.html aspell : H : true pipeline : - pyspelling.filters.html Other options would be set to a string or an integer value. matrix : - name : python sources : - pyspelling/**/*.py aspell : lang : en pipeline : - pyspelling.filters.python : strings : false comments : false Lastly, if you have an option that can be used multiple times, just set the value up as an array, and the option will be added for each value in the array. Assuming you had multiple pre-compiled dictionaries, you could add them under Aspell's --add-extra-dicts option: matrix : - name : Python Source sources : - pyspelling/**/*.py aspell : add-extra-dicts : - my-dictionary.dic - my-other-dictionary.dic pipeline : The above options would be equivalent to doing this from the command line: aspell --add-extra-dicts my-dictionary.doc --add-extra-dicts my-other-dictionary.dic","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#configuration-file","text":"PySpelling requires a YAML configuration file. The file defines the various spelling tasks along with their individual filters and options. You can optionally specify the preferred spell checker as a global option ( aspell is the default if not specified). This can be overridden on the command line. spellchecker : hunspell All of the spelling tasks are contained under the keyword matrix and are organized in a list: matrix : - task1 - task2 Each task requires, at the very least, a name and sources to search. Depending on your setup, you may need to set the dictionary to use as well. Each spell checker specifies their dictionary/language differently which is covered in more details in Spell Checker Options . matrix : - name : Python Source aspell : lang : en sources : - pyspelling/**/*.py You can also define more complicated tasks which will run your text through various filters before performing the spell checking by providing a custom pipeline. You can also add your own custom wordlists to extend the dictionary. matrix : - name : Python Source sources : - pyspelling/**/*.py aspell : lang : en dictionary : wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/python.dic pipeline : - pyspelling.filters.python : - pyspelling.filters.context : context_visible_first : true escapes : \\\\[\\\\`~] delimiters : # Ignore multiline content between fences (fences can have 3 or more back ticks) # ``` # content # ``` - open : '(?s)^(?P<open> *`{3,})$' close : '^(?P=open)$' # Ignore text between inline back ticks - open : '(?P<open>`+)' close : '(?P=open)'","title":"Configuration File"},{"location":"configuration/#name","text":"Each spelling tasks should have a unique name and is defined with the name key. When using the command line --name option, the task with the matching name will be run. matrix : - name : python New Behavior 2.0 In 1.0 , names doubled as identifiers and groups. It became apparent for certain features that a unique name is desirable for targeting different tasks, while a group specifier should be implemented separately. In 2.0 , if multiple tasks have the same name, the last defined one will be the targeted task when requesting a named task. Use groups to target multiple grouped tasks.","title":"Name"},{"location":"configuration/#groups","text":"Each task can be assigned to a group. The group name can be shared with multiple tasks. All tasks in a group can be run by specifying the --group option with the name of the group on the command line. This option is only available in version 1.1 of the configuration file. matrix : - name : python group : some_name New 2.0 group was added in version 2.0 .","title":"Groups"},{"location":"configuration/#hidden","text":"All tasks in a configuration file will be run if no name is specified. In version 1.1 of the configuration file, If a task enables the option hidden by setting it to true , that task will not be run automatically when no name is specified. hidden tasks will only be run if they are specifically mentioned by name . matrix : - name : python hidden : true New 2.0 group was added in version 2.0 .","title":"Hidden"},{"location":"configuration/#default-encoding","text":"When parsing a file, the encoding detection and translation of the data into Unicode is performed by the first filter in the pipeline. For instance, if HTML is the first, it may check for a BOM or look at the file's header to find the meta tag that specifies the file's encoding. If all encoding checks fail, the filter will usually apply an appropriate default encoding for the file content type (usually UTF-8, but check the specific filter's documentation to be sure). If needed, the filter's default encoding can be overridden in the task via the default_encoding key. After the first step in the pipeline, the text is passed around as Unicode which requires no Unicode detection. matrix : - name : markdown pipeline : - pyspelling.filters.text sources : - '**/*.md' default_encoding : utf-8 Once all filtering is complete, the text will be passed to the spell checker as byte strings, usually with the originally detected encoding (unless a filter specifically alters the encoding). The supported spell checkers are limited to very specific encodings, so if your file is using an unsupported encoding, it will fail. UTF-16 and UTF-32 is not really supported by Aspell and Hunspell, so at the end of the spell check pipeline, Unicode strings that have the associated encoding of UTF-16 or UTF-32 will be encoded with the compatible UTF-8. This does not apply to files being processed with a the pipeline disabled. When the pipeline is disabled, files are sent directly to the spell checker with no modifications. Unsupported Encodings If you are trying to spell check a file in an unsupported encoding, you can use the builtin text filter to convert the content to a more appropriate encoding. In general, it is recommended to work in, or convert to UTF-8.","title":"Default Encoding"},{"location":"configuration/#sources","text":"Each spelling task must define a list of sources to search via the sources key. Each source should be a glob pattern that should match one or more files. PySpelling will perform a search with these patterns to determine which files should be spell checked. You can also have multiple patterns on one line separated with | . When multiple patterns are defined like this, they are evaluated simultaneously. This is useful if you'd like to provide an exclusion pattern along with your file pattern. For instance, if we wanted to scan all python files in our folder, but exclude any in the build folder, we could provide the following pattern: **/*.py|!build/* . PySpelling uses Wildcard Match's glob library to perform the file globbing. By default, it uses the NEGATE , GLOBSTAR , and BRACE flags, but you can override the flag options with the glob_flags option. You can specify the flags by either their long name GLOBSTAR or their short name G . See Wildcard Match's documentation for more information on the available flags and what they do. matrix : - name : python pipeline : - pyspelling.filters.python : comments : false glob_flags : N|G|B sources : - pyspelling/**/*.py","title":"Sources"},{"location":"configuration/#expect-match","text":"When processing the sources field it is expected to find at least one matching file. If no files are located it can be helpful to raise an error and this is the default behaviour. If it is not expected to always find a file then the expect_match configuration can be used to suppress the error. matrix : - name : markdown pipeline : - pyspelling.filters.text sources : - '**/*.md' expect_match : false default_encoding : utf-8","title":"Expect Match"},{"location":"configuration/#pipeline","text":"PySpelling allows you to define tasks that outline what kind of files you want to spell check, and then sends them down a pipeline that filters the content returning chunks of text with some associated context. Each chunk is sent down each step of the pipeline until it reaches the final step, the spell check step. Between filter steps, you can also insert flow control steps that allow you to have certain text chunks skip specific steps. All of this is done with pipeline plugins . Let's say you had some Markdown files and wanted to convert them to HTML, and then filter out specific tags. You could just use the Markdown filter to convert the file to HTML and then pass it through the HTML filter to extract the text from the HTML tags. matrix : - name : markdown sources : - README.md pipeline : - pyspelling.filters.markdown : - pyspelling.filters.html : comments : false attributes : - title - alt ignores : - code - pre If needed, you can also insert flow control steps before certain filter steps. Each text chunk that is passed between filters has a category assigned to it from the previous filter. Flow control steps allow you to restrict the next filter to specific categories, or exclude specific categories from the next step. This is covered in more depth in Flow Control . If for some reason you need to send the file directly to the spell checker without using PySpelling's pipeline, simply set pipeline to null . This sends file directly to the spell checker without evaluating the encoding or passing through any filters. Specifically with Hunspell, it also sends the spell checker the filename instead of piping the content as Hunspell has certain features that don't work when piping the data, such as OpenOffice ODF input. Below is an example where we send an OpenOffice ODF file directly to Hunspell in order to use Hunspell's -O option to parse the ODF file. Keep in mind that when doing this, no encoding is sent to the spell checker unless you define default_encoding . If default_encoding is not defined, PySpelling will decode the returned content with the terminal's encoding (or what it thinks the terminal's encoding is). matrix : - name : openoffice_ODF sources : - file.odt hunspell : d : en_US O : true pipeline : null","title":"Pipeline"},{"location":"configuration/#dictionaries-and-personal-wordlists","text":"By default, PySpelling sets your main dictionary to en for Aspell and en_US for Hunspell. If you do not desire an American English dictionary, or these dictionaries are not installed in their expected default locations, you will need to configure PySpelling so it can find your preferred dictionary. Since dictionary configuring varies for each spell checker, the main dictionary is configuration (and virtually any spell checker specific option) is performed via Spell Checker Options . For Aspell, you would use the command line option --lang or -l , which in the YAML configuration file is lang or l respectively. You can see we are just removing the leading - characters. matrix : - name : python aspell : lang : en For Hunspell, you would use the command line option -d , which in the YAML configuration file is d : matrix : - name : python hunspell : d : en_US While the dictionaries cover a number of commonly used words, they are usually not sufficient. Luckily, both Aspell and Hunspell allow for adding custom wordlists. You can have as many wordlists as you like, and they can be included in a list under the key wordlists which is also found under the key dictionary . While Hunspell doesn't directly compile the wordlists, Aspell does, and it uses the main dictionary that you have specified to accomplish this. All the wordlists are combined into one custom dictionary file whose output name and location is defined via the output key which is also found under the dictionary key. Lastly, you can set the encoding to be used during compilation via the encoding under dictionary . The encoding should generally match the encoding of your main dictionary. The default encoding is utf-8 , and only Aspell uses this option. matrix : - name : python sources : - pyspelling/**/*.py aspell : lang : en dictionary : wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/python.dic encoding : utf-8 pipeline : - pyspelling.filters.python : comments : false","title":"Dictionaries and Personal Wordlists"},{"location":"configuration/#spell-checker-options","text":"Since PySpelling is a wrapper around both Aspell and Hunspell, there are a number of spell checker specific options. As only a few options are present in both, it was decided to expose them via spell checker specific keywords: aspell and hunspell for Aspell and Hunspell respectively. Here you can set options like the default dictionary and search options. Not all options are exposed though, only relevant search options are passed directly to the spell checker. Things like replace options (which aren't relevant in PySpelling) and encoding (which are handled internally by PySpelling) are not accessible. Spell checker specific options basically translate directly to the spell checker's command line options and only requires you to remove the leading - s you would normally specify on the command line. For instance, a short form option such as -l would simply be represented with the keyword l , and the long name form of the same option --lang would be represented as lang . Following the key, you would provide the appropriate value depending on it's requirement. Boolean flags would be set to true . matrix : - name : html sources : - docs/**/*.html aspell : H : true pipeline : - pyspelling.filters.html Other options would be set to a string or an integer value. matrix : - name : python sources : - pyspelling/**/*.py aspell : lang : en pipeline : - pyspelling.filters.python : strings : false comments : false Lastly, if you have an option that can be used multiple times, just set the value up as an array, and the option will be added for each value in the array. Assuming you had multiple pre-compiled dictionaries, you could add them under Aspell's --add-extra-dicts option: matrix : - name : Python Source sources : - pyspelling/**/*.py aspell : add-extra-dicts : - my-dictionary.dic - my-other-dictionary.dic pipeline : The above options would be equivalent to doing this from the command line: aspell --add-extra-dicts my-dictionary.doc --add-extra-dicts my-other-dictionary.dic","title":"Spell Checker Options"},{"location":"pipeline/","text":"Spelling Pipeline \ue157 Overview \ue157 PySpelling's pipeline utilizes special plugins to provide text filtering and to control the flow of the text down the pipeline. The plugins can be arranged in any order and even included multiple times, the only restriction is that you can't start the pipeline with FlowControl plugins, the first plugin must be a Filter plugin. A number of plugins are included with PySpelling, but additional plugins can be written using the plugin API . Filter \ue157 Filter plugins are used to augment and/or filter a given chunk of text returning only the portions that are desired. Once a plugin is done with the text, it passes it down the pipeline. A filter may return one or many chunks, each with a little contextual information. Some filters may return only one chunk of text that is the entirety of the file, and some may return context specific chunks: one for each docstring, one for each comment, etc. The metadata associated with the chunks can also be used by FlowControl plugins to allow certain types of text to skip certain filters. Aside from filtering the text, the first filter in the pipeline is always responsible for initially reading the file from disk and getting the file content into a Unicode buffer that PySpelling can work with. It is also responsible for setting the default encoding and/or identifying the encoding from the file header if there is special logic to determine such things. The following Filter plugins are included: Name Include Path Context pyspelling.filters.context CPP pyspelling.filters.cpp HTML pyspelling.filters.html JavaScript pyspelling.filters.javascript Markdown pyspelling.filters.markdown ODF pyspelling.filters.odf OOXML pyspelling.filters.ooxml Python pyspelling.filters.python Stylesheets pyspelling.filters.stylesheets Text pyspelling.filters.text URL pyspelling.filters.url XML pyspelling filters.xml Flow Control \ue157 FlowControl plugins are responsible for controlling the flow of the text down the pipeline. The category of a text chunk is passed to the plugin, and it will return one of three directives: ALLOW : the chunk(s) of text is allowed to be evaluated by the next filter. SKIP : the chunk(s) of text should skip the next filter. HALT : halts the progress of the text chunk(s) down the pipeline and sends it directly to the spell checker. The following FlowControl plugins are included: Name Include Path Wildcard pyspelling.flow_control.wildcard","title":"Spelling Pipeline"},{"location":"pipeline/#spelling-pipeline","text":"","title":"Spelling Pipeline"},{"location":"pipeline/#overview","text":"PySpelling's pipeline utilizes special plugins to provide text filtering and to control the flow of the text down the pipeline. The plugins can be arranged in any order and even included multiple times, the only restriction is that you can't start the pipeline with FlowControl plugins, the first plugin must be a Filter plugin. A number of plugins are included with PySpelling, but additional plugins can be written using the plugin API .","title":"Overview"},{"location":"pipeline/#filter","text":"Filter plugins are used to augment and/or filter a given chunk of text returning only the portions that are desired. Once a plugin is done with the text, it passes it down the pipeline. A filter may return one or many chunks, each with a little contextual information. Some filters may return only one chunk of text that is the entirety of the file, and some may return context specific chunks: one for each docstring, one for each comment, etc. The metadata associated with the chunks can also be used by FlowControl plugins to allow certain types of text to skip certain filters. Aside from filtering the text, the first filter in the pipeline is always responsible for initially reading the file from disk and getting the file content into a Unicode buffer that PySpelling can work with. It is also responsible for setting the default encoding and/or identifying the encoding from the file header if there is special logic to determine such things. The following Filter plugins are included: Name Include Path Context pyspelling.filters.context CPP pyspelling.filters.cpp HTML pyspelling.filters.html JavaScript pyspelling.filters.javascript Markdown pyspelling.filters.markdown ODF pyspelling.filters.odf OOXML pyspelling.filters.ooxml Python pyspelling.filters.python Stylesheets pyspelling.filters.stylesheets Text pyspelling.filters.text URL pyspelling.filters.url XML pyspelling filters.xml","title":"Filter"},{"location":"pipeline/#flow-control","text":"FlowControl plugins are responsible for controlling the flow of the text down the pipeline. The category of a text chunk is passed to the plugin, and it will return one of three directives: ALLOW : the chunk(s) of text is allowed to be evaluated by the next filter. SKIP : the chunk(s) of text should skip the next filter. HALT : halts the progress of the text chunk(s) down the pipeline and sends it directly to the spell checker. The following FlowControl plugins are included: Name Include Path Wildcard pyspelling.flow_control.wildcard","title":"Flow Control"},{"location":"about/changelog/","text":"Changelog \ue157 2.5.1 \ue157 FIX : Add workaround for wcmatch version 5.0 . 2.5.0 \ue157 NEW : Add expect_match option to prevent a rule from failing if it finds no matching files. NEW : Formally support Python 3.8. 2.4.0 \ue157 NEW : Disable Aspell filters by default. Users must explicitly set the mode parameter under the aspell option to enable default Aspell filters. New : Throw an exception with a message if no configuration is found or there is some other issue. New : Throw an exception with a message when no tasks are found in the matrix or when no tasks match a given name or group. New : Throw an exception with a message when a task is run but no files are found. 2.3.1 \ue157 FIX : Properly handle docstring content and detection in files that have single line functions. 2.3.0 \ue157 NEW : Support new wcmatch glob feature flags and upgrade to wcmatch 4.0. FIX : Don't use recursion when parsing XML or HTML documents. 2.2.6 \ue157 FIX : Require wcmatch 3.0 for glob related fixes. 2.2.5 \ue157 FIX : Rework comment extraction in XML plugin. FIX : Newer versions of Soup Sieve will not compile an empty string, so adjust XML and HTML plugin logic to account for this behavior. 2.2.4 \ue157 FIX : Explicitly require Beautiful Soup 4 dependency. 2.2.3 \ue157 FIX : There is no need to un-escape content for HTML/XML as it is already un-escaped in the bs4 objects. FIX : Upgrade to latest beta of Soup Sieve. 2.2.2 \ue157 FIX : Fix :empty and :root and :nth-* selectors not working properly without a tag name specified before. This is now done via our external lib called soupsieve which is the same homegrown CSS library that we were using internally. FIX : Potential infinite loop when using :nth-child() . 2.2.1 \ue157 FIX : Comments in HTML/XML should be returned regardless of whether they are in an ignored tag or not. 2.2.0 \ue157 NEW : Add support for CSS4 selectors: :empty , :first-child , :last-child , :only-child , :first-of-type , :last-of-type , :only-of-type , :nth-child(an+b [of S]?) , :nth-last-child(an+b [of S]?) , :nth-of-type(an+b) , and :nth-last-of-type(an+b) . ( #58 ) 2.1.1 \ue157 FIX : CSS4 allows :not() , :has() , and :is() to be nested in :not() . ( #62 ) 2.1.0 \ue157 NEW : Add support for div p , div>p , div+p , div~p in the HTML/XML filter's CSS selectors. ( #51 ) NEW : Add support for the :root CSS selector. ( #57 ) NEW : Add support for experimental :has() selector. ( #54 ) FIX : According to CSS4 specification, :is() is the final name for :matches() but the :matches() is an allowed alias. ( #53 ) FIX : Allow :not() to be nested in :is() / :matches() . ( #56 ) 2.0.0 \ue157 NEW : (Breaking change) Task names should be unique and using --name from the command line will only target one name (the last task defined with that name). If you were not using name to run a group of tasks, you will not notice any changes. NEW : Task option group has been added to target multiple tasks with the --group command line option. group name can be shared across different tasks. NEW : Add XML filter (PySpelling now has a dependency on lxml ). NEW : Add Open Document Format (ODF) filter for .odt , .ods , and .odp files. NEW : Add Office Open XML format (newer Microsoft document format) for .docx , .xlsx , and .pptx files. NEW : CSS selectors in XML and HTML filters now support :not() and :matches() pseudo class. NEW : CSS selectors now support , in patterns. NEW : CSS selectors now support i in attribute selectors: [attr=value i] . NEW : CSS selectors now support namespaces (some configuration required). NEW : For better HTML context, display a tag's ancestry (just tag name of parents). NEW : Captured tags are now configurable via captures , but tags that are not captured still have their children crawled unless they are under ignores . NEW : Support modes added for HTML filter: html , html5 , and xhtml . NEW : CHECK_BOM plugin attribute has been deprecated in favor of overriding the exposed has_bom function. NEW : Tasks can be hidden with the hidden configuration option. Tasks with hidden enabled will only run if they are explicitly called by name. NEW : Add normal string support to Python filter. NEW : Add string and template literal support for JavaScript filter. NEW : Add string support for CPP filter. NEW : Add generic_mode option to CPP to allow for generic C/C++ comment style capture from non C/C++ file types. NEW : Context will normalize line endings before applying context (can be disabled). NEW : CPP, Stylesheet, and JavaScript plugins now normalize line endings of block comments. NEW : UTF-16 and UTF-32 is not really supported by Aspell and Hunspell, so at the end of the pipeline, Unicode strings that have the associated encoding of UTF-16 or UTF-32 will encoding with the compatible UTF-8. This does not apply to files being processed with a disabled pipeline. When the pipeline is disabled, files are sent directly to the spell checker with no modifications. FIX : Case related issues when comparing tags and attributes in HTML. FIX : CSS selectors should only compare case insensitive for ASCII characters A-Z and a-z. FIX : Allow CSS escapes in selectors. FIX : Don't send empty (or strings that are just whitespace) to spell checker to prevent Aspell 0.50 series from crashing (also to increase performance). FIX : Catch and bubble up errors better. FIX : Fix issue where Python module docstrings would not get spell checked if they followed a shebang. 1.1.0 \ue157 NEW : Add URL/email address filter. ( #30 ) NEW : If pipeline configuration key is set to null , do not use any filters, and send the filename, not the content, to the spell checker. NEW : Add encoding option to dictionary configuration for the purpose of communicating what encoding the main dictionary is when compiling wordlists (only Aspell takes advantage of this). FIX : Fix Hunspell -O option which was mistakenly -o . ( #31 ) 1.0.0 \ue157 NEW : Allow multiple names on command line via: pyspelling -n name1 -n name2 . FIX : Fix empty HTML tags not properly having their attributes evaluated. FIX : Fix case where a deprecation warning for filters is shown when it shouldn't. FIX : Better docstring recognition in Python filter. FIX : Catch comments outside of the <HTML> tag. FIX : Filter out Doctype , CData , and other XML or non-content type information. 1.0.0b2 \ue157 FIX : Fix CPP comment regular expression. 1.0.0b1 \ue157 NEW : Better context for HTML elements. HTML is now returned by block level elements, and the elements selector is given as context. Attributes also return a selector as context and are returned individually. HTML comments are returned as individual hunks. NEW : Add Stylesheet and CPP filters ( #17 ) NEW : JavaScript is now derived from CPP. NEW : PySpelling looks for .spelling.yml or .pyspelling.yml with a priority for the latter. ( #12 ) NEW : Spelling pipeline adjustments: you can now explicitly allow only certain categories, skip categories, or halt them in the pipeline. Pipeline flow control is now done via a new FlowControl plugin. When avoiding, including, or skipping categories, they are now done with wildcard patterns. ( #16 ) NEW : Drop scanning python normal strings in plugin. NEW : Use get_plugin instead of get_filter , but allow a backwards compatible path for now. NEW : In configuration, documents is now matrix and filters is now pipeline , but a deprecation path has been added. ( #15 ) NEW : Provide a class attribute that will cause a Filter object to avoid BOM detection if it is not appropriate for the given file. NEW : Wordlists should get the desired language/dictionary from the spell checker specific options. NEW : Add global configuration option to specify the preferred spell checker, but it is still overridable via command line. FIX : Internal cleanup in regards to error handling and debug. FIX : Fix context issue when no escapes are defined. 0.2.0a4 \ue157 NEW : Text filter can handle Unicode normalization and converting to other encodings. NEW : Default encoding is now utf-8 for all filters. FIX : Internal encoding handling. 0.2.0a3 \ue157 FIX : Text filter was returning old Parser name instead of new Filter name. 0.2.0a2 \ue157 NEW : Incorporate the Decoder class into the filter class. NEW : Add Hunspell support. NEW : Drop specifying spell checker in configuration file. It must be set from command line. FIX : Add missing documentation about Context filter. 0.2.0a1 \ue157 NEW : Better filters (combine filters and parsers into just filters). NEW : Drop Python 2 support. NEW : Better Python encoding detection. NEW : Better HTML encoding detection. NEW : Drop file_extensions option and parser option. NEW : Filters no longer define file extensions. Sources must specify a wildcard path that matches desired files. NEW : Drop regular expression support for sources. NEW : Drop raw filter. 0.1.0a3 \ue157 NEW : Add JavaScript parser. 0.1.0a2 \ue157 NEW : Add option to group consecutive Python comments. FIX : Properly return error. FIX : Only retry with default encoding if exception thrown was a UnicodeDecodeError . 0.1.0a1 \ue157 NEW : Initial alpha release.","title":"Changelog"},{"location":"about/changelog/#changelog","text":"","title":"Changelog"},{"location":"about/changelog/#251","text":"FIX : Add workaround for wcmatch version 5.0 .","title":"2.5.1"},{"location":"about/changelog/#250","text":"NEW : Add expect_match option to prevent a rule from failing if it finds no matching files. NEW : Formally support Python 3.8.","title":"2.5.0"},{"location":"about/changelog/#240","text":"NEW : Disable Aspell filters by default. Users must explicitly set the mode parameter under the aspell option to enable default Aspell filters. New : Throw an exception with a message if no configuration is found or there is some other issue. New : Throw an exception with a message when no tasks are found in the matrix or when no tasks match a given name or group. New : Throw an exception with a message when a task is run but no files are found.","title":"2.4.0"},{"location":"about/changelog/#231","text":"FIX : Properly handle docstring content and detection in files that have single line functions.","title":"2.3.1"},{"location":"about/changelog/#230","text":"NEW : Support new wcmatch glob feature flags and upgrade to wcmatch 4.0. FIX : Don't use recursion when parsing XML or HTML documents.","title":"2.3.0"},{"location":"about/changelog/#226","text":"FIX : Require wcmatch 3.0 for glob related fixes.","title":"2.2.6"},{"location":"about/changelog/#225","text":"FIX : Rework comment extraction in XML plugin. FIX : Newer versions of Soup Sieve will not compile an empty string, so adjust XML and HTML plugin logic to account for this behavior.","title":"2.2.5"},{"location":"about/changelog/#224","text":"FIX : Explicitly require Beautiful Soup 4 dependency.","title":"2.2.4"},{"location":"about/changelog/#223","text":"FIX : There is no need to un-escape content for HTML/XML as it is already un-escaped in the bs4 objects. FIX : Upgrade to latest beta of Soup Sieve.","title":"2.2.3"},{"location":"about/changelog/#222","text":"FIX : Fix :empty and :root and :nth-* selectors not working properly without a tag name specified before. This is now done via our external lib called soupsieve which is the same homegrown CSS library that we were using internally. FIX : Potential infinite loop when using :nth-child() .","title":"2.2.2"},{"location":"about/changelog/#221","text":"FIX : Comments in HTML/XML should be returned regardless of whether they are in an ignored tag or not.","title":"2.2.1"},{"location":"about/changelog/#220","text":"NEW : Add support for CSS4 selectors: :empty , :first-child , :last-child , :only-child , :first-of-type , :last-of-type , :only-of-type , :nth-child(an+b [of S]?) , :nth-last-child(an+b [of S]?) , :nth-of-type(an+b) , and :nth-last-of-type(an+b) . ( #58 )","title":"2.2.0"},{"location":"about/changelog/#211","text":"FIX : CSS4 allows :not() , :has() , and :is() to be nested in :not() . ( #62 )","title":"2.1.1"},{"location":"about/changelog/#210","text":"NEW : Add support for div p , div>p , div+p , div~p in the HTML/XML filter's CSS selectors. ( #51 ) NEW : Add support for the :root CSS selector. ( #57 ) NEW : Add support for experimental :has() selector. ( #54 ) FIX : According to CSS4 specification, :is() is the final name for :matches() but the :matches() is an allowed alias. ( #53 ) FIX : Allow :not() to be nested in :is() / :matches() . ( #56 )","title":"2.1.0"},{"location":"about/changelog/#200","text":"NEW : (Breaking change) Task names should be unique and using --name from the command line will only target one name (the last task defined with that name). If you were not using name to run a group of tasks, you will not notice any changes. NEW : Task option group has been added to target multiple tasks with the --group command line option. group name can be shared across different tasks. NEW : Add XML filter (PySpelling now has a dependency on lxml ). NEW : Add Open Document Format (ODF) filter for .odt , .ods , and .odp files. NEW : Add Office Open XML format (newer Microsoft document format) for .docx , .xlsx , and .pptx files. NEW : CSS selectors in XML and HTML filters now support :not() and :matches() pseudo class. NEW : CSS selectors now support , in patterns. NEW : CSS selectors now support i in attribute selectors: [attr=value i] . NEW : CSS selectors now support namespaces (some configuration required). NEW : For better HTML context, display a tag's ancestry (just tag name of parents). NEW : Captured tags are now configurable via captures , but tags that are not captured still have their children crawled unless they are under ignores . NEW : Support modes added for HTML filter: html , html5 , and xhtml . NEW : CHECK_BOM plugin attribute has been deprecated in favor of overriding the exposed has_bom function. NEW : Tasks can be hidden with the hidden configuration option. Tasks with hidden enabled will only run if they are explicitly called by name. NEW : Add normal string support to Python filter. NEW : Add string and template literal support for JavaScript filter. NEW : Add string support for CPP filter. NEW : Add generic_mode option to CPP to allow for generic C/C++ comment style capture from non C/C++ file types. NEW : Context will normalize line endings before applying context (can be disabled). NEW : CPP, Stylesheet, and JavaScript plugins now normalize line endings of block comments. NEW : UTF-16 and UTF-32 is not really supported by Aspell and Hunspell, so at the end of the pipeline, Unicode strings that have the associated encoding of UTF-16 or UTF-32 will encoding with the compatible UTF-8. This does not apply to files being processed with a disabled pipeline. When the pipeline is disabled, files are sent directly to the spell checker with no modifications. FIX : Case related issues when comparing tags and attributes in HTML. FIX : CSS selectors should only compare case insensitive for ASCII characters A-Z and a-z. FIX : Allow CSS escapes in selectors. FIX : Don't send empty (or strings that are just whitespace) to spell checker to prevent Aspell 0.50 series from crashing (also to increase performance). FIX : Catch and bubble up errors better. FIX : Fix issue where Python module docstrings would not get spell checked if they followed a shebang.","title":"2.0.0"},{"location":"about/changelog/#110","text":"NEW : Add URL/email address filter. ( #30 ) NEW : If pipeline configuration key is set to null , do not use any filters, and send the filename, not the content, to the spell checker. NEW : Add encoding option to dictionary configuration for the purpose of communicating what encoding the main dictionary is when compiling wordlists (only Aspell takes advantage of this). FIX : Fix Hunspell -O option which was mistakenly -o . ( #31 )","title":"1.1.0"},{"location":"about/changelog/#100","text":"NEW : Allow multiple names on command line via: pyspelling -n name1 -n name2 . FIX : Fix empty HTML tags not properly having their attributes evaluated. FIX : Fix case where a deprecation warning for filters is shown when it shouldn't. FIX : Better docstring recognition in Python filter. FIX : Catch comments outside of the <HTML> tag. FIX : Filter out Doctype , CData , and other XML or non-content type information.","title":"1.0.0"},{"location":"about/changelog/#100b2","text":"FIX : Fix CPP comment regular expression.","title":"1.0.0b2"},{"location":"about/changelog/#100b1","text":"NEW : Better context for HTML elements. HTML is now returned by block level elements, and the elements selector is given as context. Attributes also return a selector as context and are returned individually. HTML comments are returned as individual hunks. NEW : Add Stylesheet and CPP filters ( #17 ) NEW : JavaScript is now derived from CPP. NEW : PySpelling looks for .spelling.yml or .pyspelling.yml with a priority for the latter. ( #12 ) NEW : Spelling pipeline adjustments: you can now explicitly allow only certain categories, skip categories, or halt them in the pipeline. Pipeline flow control is now done via a new FlowControl plugin. When avoiding, including, or skipping categories, they are now done with wildcard patterns. ( #16 ) NEW : Drop scanning python normal strings in plugin. NEW : Use get_plugin instead of get_filter , but allow a backwards compatible path for now. NEW : In configuration, documents is now matrix and filters is now pipeline , but a deprecation path has been added. ( #15 ) NEW : Provide a class attribute that will cause a Filter object to avoid BOM detection if it is not appropriate for the given file. NEW : Wordlists should get the desired language/dictionary from the spell checker specific options. NEW : Add global configuration option to specify the preferred spell checker, but it is still overridable via command line. FIX : Internal cleanup in regards to error handling and debug. FIX : Fix context issue when no escapes are defined.","title":"1.0.0b1"},{"location":"about/changelog/#020a4","text":"NEW : Text filter can handle Unicode normalization and converting to other encodings. NEW : Default encoding is now utf-8 for all filters. FIX : Internal encoding handling.","title":"0.2.0a4"},{"location":"about/changelog/#020a3","text":"FIX : Text filter was returning old Parser name instead of new Filter name.","title":"0.2.0a3"},{"location":"about/changelog/#020a2","text":"NEW : Incorporate the Decoder class into the filter class. NEW : Add Hunspell support. NEW : Drop specifying spell checker in configuration file. It must be set from command line. FIX : Add missing documentation about Context filter.","title":"0.2.0a2"},{"location":"about/changelog/#020a1","text":"NEW : Better filters (combine filters and parsers into just filters). NEW : Drop Python 2 support. NEW : Better Python encoding detection. NEW : Better HTML encoding detection. NEW : Drop file_extensions option and parser option. NEW : Filters no longer define file extensions. Sources must specify a wildcard path that matches desired files. NEW : Drop regular expression support for sources. NEW : Drop raw filter.","title":"0.2.0a1"},{"location":"about/changelog/#010a3","text":"NEW : Add JavaScript parser.","title":"0.1.0a3"},{"location":"about/changelog/#010a2","text":"NEW : Add option to group consecutive Python comments. FIX : Properly return error. FIX : Only retry with default encoding if exception thrown was a UnicodeDecodeError .","title":"0.1.0a2"},{"location":"about/changelog/#010a1","text":"NEW : Initial alpha release.","title":"0.1.0a1"},{"location":"about/contributing/","text":"Contributing & Support \ue157 Bug Reports \ue157 Please read the documentation and search the issue tracker to try and find the answer to your question before posting an issue. When creating an issue on the repository, please provide as much information as possible: Version being used. Operating system. Version of Python. Errors in console. Detailed description of the problem. Examples for reproducing the error. You can post pictures, but if specific text or code is required to reproduce the issue, please provide the text in a plain text format for easy copy/paste. The more info provided the greater the chance someone will take the time to answer, implement, or fix the issue. Be prepared to answer questions and provide additional information if required. Issues in which the creator refuses to respond to follow up questions will be marked as stale and closed. Reviewing Code \ue157 Take part in reviewing pull requests and/or reviewing direct commits. Make suggestions to improve the code and discuss solutions to overcome weakness in the algorithm. Answer Questions in Issues \ue157 Take time and answer questions and offer suggestions to people who've created issues in the issue tracker. Often people will have questions that you might have an answer for. Or maybe you know how to help them accomplish a specific task they are asking about. Feel free to share your experience to help others. Pull Requests \ue157 Pull requests are welcome, and a great way to help fix bugs and add new features. If you you are interested in directly contributing to the code, please check out Development for more information on the environment and processes. Documentation Improvements \ue157 A ton of time has been spent not only creating and supporting this tool and related extensions, but also spent making this documentation. If you feel it is still lacking, show your appreciation for the tool by helping to improve the documentation. Check out Development for more info on documentation.","title":"Contributing &amp; Support"},{"location":"about/contributing/#contributing--support","text":"","title":"Contributing &amp; Support"},{"location":"about/contributing/#bug-reports","text":"Please read the documentation and search the issue tracker to try and find the answer to your question before posting an issue. When creating an issue on the repository, please provide as much information as possible: Version being used. Operating system. Version of Python. Errors in console. Detailed description of the problem. Examples for reproducing the error. You can post pictures, but if specific text or code is required to reproduce the issue, please provide the text in a plain text format for easy copy/paste. The more info provided the greater the chance someone will take the time to answer, implement, or fix the issue. Be prepared to answer questions and provide additional information if required. Issues in which the creator refuses to respond to follow up questions will be marked as stale and closed.","title":"Bug Reports"},{"location":"about/contributing/#reviewing-code","text":"Take part in reviewing pull requests and/or reviewing direct commits. Make suggestions to improve the code and discuss solutions to overcome weakness in the algorithm.","title":"Reviewing Code"},{"location":"about/contributing/#answer-questions-in-issues","text":"Take time and answer questions and offer suggestions to people who've created issues in the issue tracker. Often people will have questions that you might have an answer for. Or maybe you know how to help them accomplish a specific task they are asking about. Feel free to share your experience to help others.","title":"Answer Questions in Issues"},{"location":"about/contributing/#pull-requests","text":"Pull requests are welcome, and a great way to help fix bugs and add new features. If you you are interested in directly contributing to the code, please check out Development for more information on the environment and processes.","title":"Pull Requests"},{"location":"about/contributing/#documentation-improvements","text":"A ton of time has been spent not only creating and supporting this tool and related extensions, but also spent making this documentation. If you feel it is still lacking, show your appreciation for the tool by helping to improve the documentation. Check out Development for more info on documentation.","title":"Documentation Improvements"},{"location":"about/development/","text":"Development \ue157 Project Layout \ue157 There are a number of files for build, test, and continuous integration in the root of the project, but in general, the project is broken up like so. \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 src \u2502 \u251c\u2500\u2500 dictionary \u2502 \u2514\u2500\u2500 markdown \u251c\u2500\u2500 pyspelling \u2514\u2500\u2500 requirements Directory Description docs/src/dictionary This contains the spell check wordlist(s) for the project. docs/src/markdown This contains the content for the documentation. pyspelling This contains the source code for the project. requirements This contains files with lists of dependencies required dependencies for continuous integration. Coding Standards \ue157 When writing code, the code should roughly conform to PEP8 and PEP257 suggestions. The project utilizes the Flake8 linter (with some additional plugins) to ensure code conforms (give or take some of the rules). When in doubt, follow the formatting hints of existing code when adding or modifying files. existing files. Listed below are the modules used: pycqa/flake8 pycqa/flake8-docstrings pycqa/pep8-naming ebeweber/flake8-mutable gforcada/flake8-builtins Usually this can be automated with Tox (assuming it is installed): tox -elint . Building and Editing Documents \ue157 Documents are in Markdown (with with some additional syntax provided by extensions) and are converted to HTML via Python Markdown. If you would like to build and preview the documentation, you must have these packages installed: Python-Markdown/markdown : the Markdown parser. mkdocs/mkdocs : the document site generator. squidfunk/mkdocs-material : a material theme for MkDocs. facelessuser/pymdown-extensions : this Python Markdown extension bundle. In order to build and preview the documents, just run the command below from the root of the project and you should be able to view the documents at localhost:8000 in your browser. After that, you should be able to update the documents and have your browser preview update live. mkdocs serve Spell Checking Documents \ue157 During validation we build the docs and spell check various files in the project. This project is used to spell check itself, but Aspell must be installed. Currently this project uses one of the more recent versions of Aspell. Since the latest Aspell is not available on Windows, and this has not been tested with older versions, it is not expected that everyone will install and run Aspell locally, but it will be run in CI tests for pull requests. In order to perform the spell check, it is expected you are setup to build the documents, and that you have Aspell installed in your system path (if needed you can use the --binary option to point to the location of your Aspell binary). It is also expected that you have the en dictionary installed as well. To initiate the spell check, run the following command from the root of the project. You will need to make sure the documents are built first: mkdocs build --clean And then run the spell checker. Using python -m from the project root will load your checked out version of PySpelling instead of your system installed version: python -m pyspelling It should print out the files with the misspelled words if any are found. If you find it prints words that are not misspelled, you can add them in docs/src/dictionary/en-custom.text . Validation Tests \ue157 In order to preserve good code health, a test suite has been put together with pytest ( pytest-dev/pytest ). There are currently two kinds of tests: syntax and targeted. To run these tests, you can use the following command: python run_tests.py Running Validation With Tox \ue157 Tox ( tox-dev/tox ) is a great way to run the validation tests, spelling checks, and linting in virtual environments so as not to mess with your current working environment. Tox will use the specified Python version for the given environment and create a virtual environment and install all the needed requirements (minus Aspell). You could also setup your own virtual environments with the Virtualenv module without Tox, and manually do the same. First, you need to have Tox installed: pip install tox By running Tox, it will walk through all the environments and create them (assuming you have all the python versions on your machine) and run the related tests. See tox.ini to learn more. tox If you don't have all the Python versions needed to test all the environments, those entries will fail. You can ignore those. Spelling will also fail if you don't have the correct version of Aspell. As most people will not have all the Python versions on their machine, it makes more sense to target specific environments. To target a specific environment to test, you use the -e option to select the environment of interest. To select lint: tox -elint To select Python 3.7 unit tests (or other versions \u2013 change accordingly): tox -epy37 To select spelling and document building: tox -edocuments Code Coverage \ue157 When running the validation tests through Tox, it is setup to track code coverage via the Coverage ( ned/coveragepy ) module. Coverage is run on each pyxx environment. If you've made changes to the code, you can clear the old coverage data: coverage erase Then run each unit test environment to and coverage will be calculated. All the data from each run is merged together. HTML is output for each file in .tox/pyXX/tmp . You can use these to see areas that are not covered/exercised yet with testing. You can checkout tox.ini to see how this is accomplished.","title":"Development"},{"location":"about/development/#development","text":"","title":"Development"},{"location":"about/development/#project-layout","text":"There are a number of files for build, test, and continuous integration in the root of the project, but in general, the project is broken up like so. \u251c\u2500\u2500 docs \u2502 \u2514\u2500\u2500 src \u2502 \u251c\u2500\u2500 dictionary \u2502 \u2514\u2500\u2500 markdown \u251c\u2500\u2500 pyspelling \u2514\u2500\u2500 requirements Directory Description docs/src/dictionary This contains the spell check wordlist(s) for the project. docs/src/markdown This contains the content for the documentation. pyspelling This contains the source code for the project. requirements This contains files with lists of dependencies required dependencies for continuous integration.","title":"Project Layout"},{"location":"about/development/#coding-standards","text":"When writing code, the code should roughly conform to PEP8 and PEP257 suggestions. The project utilizes the Flake8 linter (with some additional plugins) to ensure code conforms (give or take some of the rules). When in doubt, follow the formatting hints of existing code when adding or modifying files. existing files. Listed below are the modules used: pycqa/flake8 pycqa/flake8-docstrings pycqa/pep8-naming ebeweber/flake8-mutable gforcada/flake8-builtins Usually this can be automated with Tox (assuming it is installed): tox -elint .","title":"Coding Standards"},{"location":"about/development/#building-and-editing-documents","text":"Documents are in Markdown (with with some additional syntax provided by extensions) and are converted to HTML via Python Markdown. If you would like to build and preview the documentation, you must have these packages installed: Python-Markdown/markdown : the Markdown parser. mkdocs/mkdocs : the document site generator. squidfunk/mkdocs-material : a material theme for MkDocs. facelessuser/pymdown-extensions : this Python Markdown extension bundle. In order to build and preview the documents, just run the command below from the root of the project and you should be able to view the documents at localhost:8000 in your browser. After that, you should be able to update the documents and have your browser preview update live. mkdocs serve","title":"Building and Editing Documents"},{"location":"about/development/#spell-checking-documents","text":"During validation we build the docs and spell check various files in the project. This project is used to spell check itself, but Aspell must be installed. Currently this project uses one of the more recent versions of Aspell. Since the latest Aspell is not available on Windows, and this has not been tested with older versions, it is not expected that everyone will install and run Aspell locally, but it will be run in CI tests for pull requests. In order to perform the spell check, it is expected you are setup to build the documents, and that you have Aspell installed in your system path (if needed you can use the --binary option to point to the location of your Aspell binary). It is also expected that you have the en dictionary installed as well. To initiate the spell check, run the following command from the root of the project. You will need to make sure the documents are built first: mkdocs build --clean And then run the spell checker. Using python -m from the project root will load your checked out version of PySpelling instead of your system installed version: python -m pyspelling It should print out the files with the misspelled words if any are found. If you find it prints words that are not misspelled, you can add them in docs/src/dictionary/en-custom.text .","title":"Spell Checking Documents"},{"location":"about/development/#validation-tests","text":"In order to preserve good code health, a test suite has been put together with pytest ( pytest-dev/pytest ). There are currently two kinds of tests: syntax and targeted. To run these tests, you can use the following command: python run_tests.py","title":"Validation Tests"},{"location":"about/development/#running-validation-with-tox","text":"Tox ( tox-dev/tox ) is a great way to run the validation tests, spelling checks, and linting in virtual environments so as not to mess with your current working environment. Tox will use the specified Python version for the given environment and create a virtual environment and install all the needed requirements (minus Aspell). You could also setup your own virtual environments with the Virtualenv module without Tox, and manually do the same. First, you need to have Tox installed: pip install tox By running Tox, it will walk through all the environments and create them (assuming you have all the python versions on your machine) and run the related tests. See tox.ini to learn more. tox If you don't have all the Python versions needed to test all the environments, those entries will fail. You can ignore those. Spelling will also fail if you don't have the correct version of Aspell. As most people will not have all the Python versions on their machine, it makes more sense to target specific environments. To target a specific environment to test, you use the -e option to select the environment of interest. To select lint: tox -elint To select Python 3.7 unit tests (or other versions \u2013 change accordingly): tox -epy37 To select spelling and document building: tox -edocuments","title":"Running Validation With Tox"},{"location":"about/development/#code-coverage","text":"When running the validation tests through Tox, it is setup to track code coverage via the Coverage ( ned/coveragepy ) module. Coverage is run on each pyxx environment. If you've made changes to the code, you can clear the old coverage data: coverage erase Then run each unit test environment to and coverage will be calculated. All the data from each run is merged together. HTML is output for each file in .tox/pyXX/tmp . You can use these to see areas that are not covered/exercised yet with testing. You can checkout tox.ini to see how this is accomplished.","title":"Code Coverage"},{"location":"about/license/","text":"License \ue157 MIT License Copyright \u00a9 2017 - 2019 Isaac Muse isaacmuse@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/license/#license","text":"MIT License Copyright \u00a9 2017 - 2019 Isaac Muse isaacmuse@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"filters/context/","text":"Context \ue157 Usage \ue157 The Context filter is used to create regular expression context delimiters for filtering out content you want from content you don't want. Depending on how the filter is configured, the opening delimiter will swap from ignoring text to gathering text. When the closing delimiter is met, the filter will swap back from gathering text to ignoring text. If context_visible_first is set to true , the logic will be reversed. Regular expressions are compiled with the MULTILINE flag so that ^ represents the start of a line and $ represents the end of a line. \\A and \\Z would represent the start and end of the buffer. Delimiters require an open and a close pattern. You can optionally define a content pattern, but if you don't, it is .*? by default. The three patterns are used to create a single regular expression in the form r ' {0} (?P<special_group_name> {1} )(?: {2} |\\Z)' , where {0} is the opening pattern is inserted, {1} is the content pattern , and {2} is the closing pattern. special_group_name is randomly generated to ensure it doesn't conflict with anything in the pattern. Keeping in mind that these are all compiled into one pattern, you are able to define capture group names in opening, and reference them in the closing if needed. You can also able to define a global escape pattern to prevent escaped delimiters from being captured. The filter can be included via pyspelling.filters.context . matrix : - name : python sources : - pyspelling pipeline : - pyspelling.filters.python : comments : false - pyspelling.filters.context_filter : context_visible_first : true escapes : '\\\\[\\\\`~]' delimiters : # Ignore multiline content between fences (fences can have 3 or more back ticks) # ``` # content # ``` - open : '(?s)^(?P<open> *`{3,})$' close : '^(?P=open)$' # Ignore text between inline back ticks - open : '(?P<open>`+)' close : '(?P=open)' Options \ue157 Options Type Default Description escapes string '' Regular expression pattern for character escapes outside delimiters. context_visible_first bool False Context will start as invisible and will be invisible between delimiters. delimiters [dict] [] A list of dicts that define the regular expression delimiters. Categories \ue157 Context returns text with the following categories. Category Description context Text captured by the via context.","title":"Context"},{"location":"filters/context/#context","text":"","title":"Context"},{"location":"filters/context/#usage","text":"The Context filter is used to create regular expression context delimiters for filtering out content you want from content you don't want. Depending on how the filter is configured, the opening delimiter will swap from ignoring text to gathering text. When the closing delimiter is met, the filter will swap back from gathering text to ignoring text. If context_visible_first is set to true , the logic will be reversed. Regular expressions are compiled with the MULTILINE flag so that ^ represents the start of a line and $ represents the end of a line. \\A and \\Z would represent the start and end of the buffer. Delimiters require an open and a close pattern. You can optionally define a content pattern, but if you don't, it is .*? by default. The three patterns are used to create a single regular expression in the form r ' {0} (?P<special_group_name> {1} )(?: {2} |\\Z)' , where {0} is the opening pattern is inserted, {1} is the content pattern , and {2} is the closing pattern. special_group_name is randomly generated to ensure it doesn't conflict with anything in the pattern. Keeping in mind that these are all compiled into one pattern, you are able to define capture group names in opening, and reference them in the closing if needed. You can also able to define a global escape pattern to prevent escaped delimiters from being captured. The filter can be included via pyspelling.filters.context . matrix : - name : python sources : - pyspelling pipeline : - pyspelling.filters.python : comments : false - pyspelling.filters.context_filter : context_visible_first : true escapes : '\\\\[\\\\`~]' delimiters : # Ignore multiline content between fences (fences can have 3 or more back ticks) # ``` # content # ``` - open : '(?s)^(?P<open> *`{3,})$' close : '^(?P=open)$' # Ignore text between inline back ticks - open : '(?P<open>`+)' close : '(?P=open)'","title":"Usage"},{"location":"filters/context/#options","text":"Options Type Default Description escapes string '' Regular expression pattern for character escapes outside delimiters. context_visible_first bool False Context will start as invisible and will be invisible between delimiters. delimiters [dict] [] A list of dicts that define the regular expression delimiters.","title":"Options"},{"location":"filters/context/#categories","text":"Context returns text with the following categories. Category Description context Text captured by the via context.","title":"Categories"},{"location":"filters/cpp/","text":"CPP \ue157 Usage \ue157 The CPP plugin is designed to find and return C/C++ style comments. When first in the chain, the CPP filter uses no special encoding detection. It will assume utf-8 if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text: block, inline, or string (if enabled). When the strings option is enabled, content will be extracted from strings (not character constants). Support is available for all the modern C++ strings shown below. CPP will also handle decoding string escapes as well, but as string character width and encoding can be dependent on implementation and configuration, some additional setup may be required via option . Strings will be returned with the specified encoding, even if it differs from the file's encoding (this is the associated encoding specified in the SourceText object, the content itself is still in Unicode). auto s0 = \"Narrow character string\" ; // char auto s1 = L \"Wide character string\" ; // wchar_t auto s2 = u8 \"UTF-8 strings\" ; // char auto s3 = u \"UTF-16 strings\" ; // char16_t auto s4 = U \"UTF-32 strings\" ; // char32_t auto R0 = R \" ( \"Raw strings\" ) \" ; // const char* auto R1 = R \" delim( \"Raw strings with delimiters\" )delim \" ; // const char* auto R3 = LR \"(\" Raw wide character strings \")\" ; // const wchar_t* auto R4 = u8R \"(\" Raw UTF - 8 strings \")\" ; // const char*, encoded as UTF-8 auto R5 = uR \"(\" Raw UTF - 16 strings \")\" ; // const char16_t*, encoded as UTF-16 auto R6 = UR \"(\" Raw UTF - 32 strings \")\" ; // const char32_t*, encoded as UTF-32 As C++ style comments are fairly common convention in other languages, this filter can often be used for other languages as well using generic_mode . In Generic Mode, many C/C++ specific considerations and options will be disabled. See [Generic Mode] for more information. matrix : - name : cpp pipeline : - pyspelling.filters.cpp line_comments : false sources : - js_files/**/*.{cpp,hpp,c,h} Filtering String types \ue157 When strings is enabled, you can specify which strings you want to allow via the string_types option. Valid string types are S for standard, L for long/wide, U for Unicode (all variants), and R for raw. Case is not important, and the default value is sul . If specifying R , you must also specify either U , L , or S as raw strings are also either S , L , or S strings. Selecting UR will select both Unicode strings and Unicode raw strings. If you need to target just raw strings, you can use R* which will target all raw string types: raw Unicode, raw wide, and raw standard. You can use * for other types as well. You can also just specify * by itself to target all string types. Generic Mode \ue157 C/C++ style comments are not exclusive to C/C++. Many different file types have adopted similar style comments. The CPP filter has a generic mode which allows for a C/C++ style comment extraction without all the C/C++ specific considerations. Simply enable generic_mode via the options . Generic Mode disables the C/C++ specific nuance of allowing multiline comments via escaping newlines. This is a very C/C++ specific thing that is rarely carried over by others that have adopted C/C++ style comments: // Generic mode will \\ not allow this. Generic Mode will not decode any character escapes in strings when enabled. C/C++ has very specific rules for handling string escapes, only a handful of which may translate to other languages. Generic Mode is mainly meant for comments and not strings, but will return content of single quoted and double quoted strings if strings is enabled. All related escape decoding options do not apply to Generic Mode. Trigraphs are very C/C++ specific, and will never be evaluated in Generic Mode. Lastly, when using this filter in Generic Mode, you can also adjust the category prefix from cpp to whatever you would like via the prefix option . Options \ue157 Options Type Default Description block_comments bool True Return SourceText entries for each block comment. line_comments bool True Return SourceText entries for each line comment. strings bool False Return SourceText entries for each string. group_comments bool False Group consecutive inline comments as one SourceText entry. trigraphs bool False Account for trigraphs in C/C++ code. Trigraphs are never evaluated in Generic Mode . generic_mode bool False Parses files with a generic C++ like mode for parsing C++ style comments from non C++ files. See Generic Mode for more info. decode_escapes bool True Enable/disable string escape decoding. Strings are never decoded in Generic Mode . charset_size int 1 Set normal string character byte width. exec_charset string 'utf-8 Set normal string encoding. wide_charset_size int 4 Set wide string character byte width. wide_exec_charset string 'utf-32 Set wide string encoding. string_types string \"sul\" Set the allowed string types to capture: standard strings ( s ), wide ( l ), Unicode ( u ), and raw ( r ). * captures all strings, or when used with a type, captures all variants of that type r* . prefix string 'cpp' Change the category prefix. Categories \ue157 CPP returns text with the following categories. cpp prefix can be changed via the prefix option . Category Description cpp-block-comment Text captured from C++ style block comments. cpp-line-comment Text captured from C++ style line comments. cpp-string Text captured from strings.","title":"CPP"},{"location":"filters/cpp/#cpp","text":"","title":"CPP"},{"location":"filters/cpp/#usage","text":"The CPP plugin is designed to find and return C/C++ style comments. When first in the chain, the CPP filter uses no special encoding detection. It will assume utf-8 if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text: block, inline, or string (if enabled). When the strings option is enabled, content will be extracted from strings (not character constants). Support is available for all the modern C++ strings shown below. CPP will also handle decoding string escapes as well, but as string character width and encoding can be dependent on implementation and configuration, some additional setup may be required via option . Strings will be returned with the specified encoding, even if it differs from the file's encoding (this is the associated encoding specified in the SourceText object, the content itself is still in Unicode). auto s0 = \"Narrow character string\" ; // char auto s1 = L \"Wide character string\" ; // wchar_t auto s2 = u8 \"UTF-8 strings\" ; // char auto s3 = u \"UTF-16 strings\" ; // char16_t auto s4 = U \"UTF-32 strings\" ; // char32_t auto R0 = R \" ( \"Raw strings\" ) \" ; // const char* auto R1 = R \" delim( \"Raw strings with delimiters\" )delim \" ; // const char* auto R3 = LR \"(\" Raw wide character strings \")\" ; // const wchar_t* auto R4 = u8R \"(\" Raw UTF - 8 strings \")\" ; // const char*, encoded as UTF-8 auto R5 = uR \"(\" Raw UTF - 16 strings \")\" ; // const char16_t*, encoded as UTF-16 auto R6 = UR \"(\" Raw UTF - 32 strings \")\" ; // const char32_t*, encoded as UTF-32 As C++ style comments are fairly common convention in other languages, this filter can often be used for other languages as well using generic_mode . In Generic Mode, many C/C++ specific considerations and options will be disabled. See [Generic Mode] for more information. matrix : - name : cpp pipeline : - pyspelling.filters.cpp line_comments : false sources : - js_files/**/*.{cpp,hpp,c,h}","title":"Usage"},{"location":"filters/cpp/#filtering-string-types","text":"When strings is enabled, you can specify which strings you want to allow via the string_types option. Valid string types are S for standard, L for long/wide, U for Unicode (all variants), and R for raw. Case is not important, and the default value is sul . If specifying R , you must also specify either U , L , or S as raw strings are also either S , L , or S strings. Selecting UR will select both Unicode strings and Unicode raw strings. If you need to target just raw strings, you can use R* which will target all raw string types: raw Unicode, raw wide, and raw standard. You can use * for other types as well. You can also just specify * by itself to target all string types.","title":"Filtering String types"},{"location":"filters/cpp/#generic-mode","text":"C/C++ style comments are not exclusive to C/C++. Many different file types have adopted similar style comments. The CPP filter has a generic mode which allows for a C/C++ style comment extraction without all the C/C++ specific considerations. Simply enable generic_mode via the options . Generic Mode disables the C/C++ specific nuance of allowing multiline comments via escaping newlines. This is a very C/C++ specific thing that is rarely carried over by others that have adopted C/C++ style comments: // Generic mode will \\ not allow this. Generic Mode will not decode any character escapes in strings when enabled. C/C++ has very specific rules for handling string escapes, only a handful of which may translate to other languages. Generic Mode is mainly meant for comments and not strings, but will return content of single quoted and double quoted strings if strings is enabled. All related escape decoding options do not apply to Generic Mode. Trigraphs are very C/C++ specific, and will never be evaluated in Generic Mode. Lastly, when using this filter in Generic Mode, you can also adjust the category prefix from cpp to whatever you would like via the prefix option .","title":"Generic Mode"},{"location":"filters/cpp/#options","text":"Options Type Default Description block_comments bool True Return SourceText entries for each block comment. line_comments bool True Return SourceText entries for each line comment. strings bool False Return SourceText entries for each string. group_comments bool False Group consecutive inline comments as one SourceText entry. trigraphs bool False Account for trigraphs in C/C++ code. Trigraphs are never evaluated in Generic Mode . generic_mode bool False Parses files with a generic C++ like mode for parsing C++ style comments from non C++ files. See Generic Mode for more info. decode_escapes bool True Enable/disable string escape decoding. Strings are never decoded in Generic Mode . charset_size int 1 Set normal string character byte width. exec_charset string 'utf-8 Set normal string encoding. wide_charset_size int 4 Set wide string character byte width. wide_exec_charset string 'utf-32 Set wide string encoding. string_types string \"sul\" Set the allowed string types to capture: standard strings ( s ), wide ( l ), Unicode ( u ), and raw ( r ). * captures all strings, or when used with a type, captures all variants of that type r* . prefix string 'cpp' Change the category prefix.","title":"Options"},{"location":"filters/cpp/#categories","text":"CPP returns text with the following categories. cpp prefix can be changed via the prefix option . Category Description cpp-block-comment Text captured from C++ style block comments. cpp-line-comment Text captured from C++ style line comments. cpp-string Text captured from strings.","title":"Categories"},{"location":"filters/html/","text":"HTML \ue157 Usage \ue157 The HTML filter is designed to capture HTML content, comments, and even attributes. It allows for filtering out specific tags, and you can even filter them out with basic selectors. When first in the chain, the HTML filter will look for the encoding of the HTML in its header and convert the buffer to Unicode. It will assume utf-8 if no encoding header is found, and the user has not overridden the fallback encoding. The HTML filter uses BeautifulSoup4 to convert the Unicode content to HTML. Content is returned in individual chunks by block tags. While this causes more overhead, as each block is processed individually through the command line tool, it provides context for where the spelling errors are. If enabled, the HTML filter will also return chunks for comments and even attributes. Each type of text chunk is returned with their own category type. Tags can be captured or ignored with the captures and ignores options. These options work by employing CSS selectors to target the tags. The CSS selectors are based on a limited subset of CSS4 selectors. matrix : - name : html pipeline : - pyspelling.filters.html : comments : false attributes : - title - alt ignores : - :matches(code, pre) - a:matches(.magiclink-compare, .magiclink-commit) - span.keys - :matches(.MathJax_Preview, .md-nav__link, .md-footer-custom-text, .md-source__repository, .headerlink, .md-icon) sources : - site/*.html Supported CSS Selectors \ue157 The CSS selectors are based on a limited subset of CSS4 selectors. Support is provided via Soup Sieve. Please reference Soup Sieve's documentation for more info. Options \ue157 Options Type Default Description comments bool True Include comment text in the output. attributes [string] [] Attributes whose content should be included in the output. ignores [string] [] CSS style selectors that identify tags to ignore. Child tags will not be crawled. captures [string] [ '*|*:not(script,style)' ] CSS style selectors used to narrow which tags that text is collected from. Unlike ignores , tags which text is not captured from still have their children crawled. mode string 'html Mode to use when parsing HTML: html , xhtml , html5 . namespaces dict {} Dictionary containing key value pairs of namespaces to use for CSS selectors (equivalent to @namespace in CSS). Use the an empty string for the key to define default the namespace. See below for example. break_tags [string] [] Additional tags (in addition to the default, defined block tags), to break on for context. Useful for new or currently unsupported block tags. Namespace example matrix: - name: html pipeline: - pyspelling.filters.html: mode: xhtml namespaces: \"\": http://www.w3.org/1999/xhtml svg: http://www.w3.org/2000/svg xlink: http://www.w3.org/1999/xlink Categories \ue157 HTML returns text with the following categories. Category Description html-content Text captured from HTML blocks. html-attribute Text captured from HTML attributes. html-comment Text captured from HTML comments.","title":"HTML"},{"location":"filters/html/#html","text":"","title":"HTML"},{"location":"filters/html/#usage","text":"The HTML filter is designed to capture HTML content, comments, and even attributes. It allows for filtering out specific tags, and you can even filter them out with basic selectors. When first in the chain, the HTML filter will look for the encoding of the HTML in its header and convert the buffer to Unicode. It will assume utf-8 if no encoding header is found, and the user has not overridden the fallback encoding. The HTML filter uses BeautifulSoup4 to convert the Unicode content to HTML. Content is returned in individual chunks by block tags. While this causes more overhead, as each block is processed individually through the command line tool, it provides context for where the spelling errors are. If enabled, the HTML filter will also return chunks for comments and even attributes. Each type of text chunk is returned with their own category type. Tags can be captured or ignored with the captures and ignores options. These options work by employing CSS selectors to target the tags. The CSS selectors are based on a limited subset of CSS4 selectors. matrix : - name : html pipeline : - pyspelling.filters.html : comments : false attributes : - title - alt ignores : - :matches(code, pre) - a:matches(.magiclink-compare, .magiclink-commit) - span.keys - :matches(.MathJax_Preview, .md-nav__link, .md-footer-custom-text, .md-source__repository, .headerlink, .md-icon) sources : - site/*.html","title":"Usage"},{"location":"filters/html/#supported-css-selectors","text":"The CSS selectors are based on a limited subset of CSS4 selectors. Support is provided via Soup Sieve. Please reference Soup Sieve's documentation for more info.","title":"Supported CSS Selectors"},{"location":"filters/html/#options","text":"Options Type Default Description comments bool True Include comment text in the output. attributes [string] [] Attributes whose content should be included in the output. ignores [string] [] CSS style selectors that identify tags to ignore. Child tags will not be crawled. captures [string] [ '*|*:not(script,style)' ] CSS style selectors used to narrow which tags that text is collected from. Unlike ignores , tags which text is not captured from still have their children crawled. mode string 'html Mode to use when parsing HTML: html , xhtml , html5 . namespaces dict {} Dictionary containing key value pairs of namespaces to use for CSS selectors (equivalent to @namespace in CSS). Use the an empty string for the key to define default the namespace. See below for example. break_tags [string] [] Additional tags (in addition to the default, defined block tags), to break on for context. Useful for new or currently unsupported block tags. Namespace example matrix: - name: html pipeline: - pyspelling.filters.html: mode: xhtml namespaces: \"\": http://www.w3.org/1999/xhtml svg: http://www.w3.org/2000/svg xlink: http://www.w3.org/1999/xlink","title":"Options"},{"location":"filters/html/#categories","text":"HTML returns text with the following categories. Category Description html-content Text captured from HTML blocks. html-attribute Text captured from HTML attributes. html-comment Text captured from HTML comments.","title":"Categories"},{"location":"filters/javascript/","text":"JavaScript \ue157 Usage \ue157 When first in the chain, the JavaScript filter uses no special encoding detection. It will assume utf-8 if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text. The filter can return JSDoc comments, block comment, inline comment, string, and template literal content. matrix : - name : javascript pipeline : - pyspelling.filters.javascript jsdocs : true line_comments : false block_comments : false sources : - js_files/**/*.js Options \ue157 Options Type Default Description block_comments bool True Return SourceText entries for each block comment. line_comments bool True Return SourceText entries for each line comment. jsdocs bool False Return SourceText entries for each JSDoc comment. strings bool False Return SourceText entries for each string. group_comments bool False Group consecutive inline JavaScript comments as one SourceText entry. decode_escapes bool True Enable/disable decoding of string escapes. Categories \ue157 JavaScript returns text with the following categories. Category Description js-block-comment Text captured from JavaScript block comments. js-line-comment Text captured from JavaScript line comments. js-docs Text captured from JSDoc comments. js-string Text captured from strings and template literals.","title":"JavaScript"},{"location":"filters/javascript/#javascript","text":"","title":"JavaScript"},{"location":"filters/javascript/#usage","text":"When first in the chain, the JavaScript filter uses no special encoding detection. It will assume utf-8 if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text. The filter can return JSDoc comments, block comment, inline comment, string, and template literal content. matrix : - name : javascript pipeline : - pyspelling.filters.javascript jsdocs : true line_comments : false block_comments : false sources : - js_files/**/*.js","title":"Usage"},{"location":"filters/javascript/#options","text":"Options Type Default Description block_comments bool True Return SourceText entries for each block comment. line_comments bool True Return SourceText entries for each line comment. jsdocs bool False Return SourceText entries for each JSDoc comment. strings bool False Return SourceText entries for each string. group_comments bool False Group consecutive inline JavaScript comments as one SourceText entry. decode_escapes bool True Enable/disable decoding of string escapes.","title":"Options"},{"location":"filters/javascript/#categories","text":"JavaScript returns text with the following categories. Category Description js-block-comment Text captured from JavaScript block comments. js-line-comment Text captured from JavaScript line comments. js-docs Text captured from JSDoc comments. js-string Text captured from strings and template literals.","title":"Categories"},{"location":"filters/markdown/","text":"Markdown \ue157 Usage \ue157 The Markdown filter converts a text file's buffer using Python Markdown and returns a single SourceText object containing the text as HTML. It can be included via pyspelling.filters.markdown . When first in the chain, the file's default, assumed encoding is utf-8 unless otherwise overridden by the user. matrix : - name : markdown pipeline : - pyspelling.parsers.markdown_parser : markdown_extensions : - markdown.extensions.toc : slugify : !!python/name:pymdownx.slugs.uslugify permalink : \" \\ue157 \" - markdown.extensions.admonition - markdown.extensions.smarty source : - ** /*.md Options \ue157 Options Type Default Description markdown_extensions [string/dict] [] A list of strings defining markdown extensions to use. You can substitute the string with a dict that defines the extension as the key and the value as a dictionary of options. Categories \ue157 Markdown returns text with the following categories. Category Description markdown Text rendered in HTML.","title":"Markdown"},{"location":"filters/markdown/#markdown","text":"","title":"Markdown"},{"location":"filters/markdown/#usage","text":"The Markdown filter converts a text file's buffer using Python Markdown and returns a single SourceText object containing the text as HTML. It can be included via pyspelling.filters.markdown . When first in the chain, the file's default, assumed encoding is utf-8 unless otherwise overridden by the user. matrix : - name : markdown pipeline : - pyspelling.parsers.markdown_parser : markdown_extensions : - markdown.extensions.toc : slugify : !!python/name:pymdownx.slugs.uslugify permalink : \" \\ue157 \" - markdown.extensions.admonition - markdown.extensions.smarty source : - ** /*.md","title":"Usage"},{"location":"filters/markdown/#options","text":"Options Type Default Description markdown_extensions [string/dict] [] A list of strings defining markdown extensions to use. You can substitute the string with a dict that defines the extension as the key and the value as a dictionary of options.","title":"Options"},{"location":"filters/markdown/#categories","text":"Markdown returns text with the following categories. Category Description markdown Text rendered in HTML.","title":"Categories"},{"location":"filters/odf/","text":"ODF \ue157 Usage \ue157 The ODF filter provides support for the Open Document Format. It supports documents ( odt ), spreadsheets ( ods ), and presentations ( odp ). It also supports their flat format as well: fodt , fods , and fodp . In general, it will return one chunk containing all the checkable strings in the file. In the case of presentations, it will actually send multiple chunks, one for each slide. - name : odf sources : - '**/*.{odt,fodt,ods,odp}' pipeline : - pyspelling.filters.odf : Options \ue157 There are currently no additional options when using the ODF filter. Categories \ue157 HTML returns text with the following categories. Category Description odt-content Text captured from document files. odp-content Text captured from presentation files. ods-content Text captured from spreadsheet files.","title":"ODF"},{"location":"filters/odf/#odf","text":"","title":"ODF"},{"location":"filters/odf/#usage","text":"The ODF filter provides support for the Open Document Format. It supports documents ( odt ), spreadsheets ( ods ), and presentations ( odp ). It also supports their flat format as well: fodt , fods , and fodp . In general, it will return one chunk containing all the checkable strings in the file. In the case of presentations, it will actually send multiple chunks, one for each slide. - name : odf sources : - '**/*.{odt,fodt,ods,odp}' pipeline : - pyspelling.filters.odf :","title":"Usage"},{"location":"filters/odf/#options","text":"There are currently no additional options when using the ODF filter.","title":"Options"},{"location":"filters/odf/#categories","text":"HTML returns text with the following categories. Category Description odt-content Text captured from document files. odp-content Text captured from presentation files. ods-content Text captured from spreadsheet files.","title":"Categories"},{"location":"filters/ooxml/","text":"OOXML \ue157 Usage \ue157 The OOXML filter provides support for the Office Open XML format (latest format for Microsoft Office files). It supports documents ( docx ), spreadsheets ( xlsx ), and presentations ( pptx ). In general, it will return one chunk containing all the checkable strings in the file. In the case of presentations, it will actually send multiple chunks, one for each slide. Documents may return additional chunks for headers, footers, etc. - name : ooxml sources : - '**/*.{docx,pptx,xlsx}' pipeline : - pyspelling.filters.ooxml : Options \ue157 There are currently no additional options when using the OOXML filter. Categories \ue157 HTML returns text with the following categories. Category Description docx-content Text captured from document files. pptx-content Text captured from presentation files. xlsx-content Text captured from spreadsheet files.","title":"OOXML"},{"location":"filters/ooxml/#ooxml","text":"","title":"OOXML"},{"location":"filters/ooxml/#usage","text":"The OOXML filter provides support for the Office Open XML format (latest format for Microsoft Office files). It supports documents ( docx ), spreadsheets ( xlsx ), and presentations ( pptx ). In general, it will return one chunk containing all the checkable strings in the file. In the case of presentations, it will actually send multiple chunks, one for each slide. Documents may return additional chunks for headers, footers, etc. - name : ooxml sources : - '**/*.{docx,pptx,xlsx}' pipeline : - pyspelling.filters.ooxml :","title":"Usage"},{"location":"filters/ooxml/#options","text":"There are currently no additional options when using the OOXML filter.","title":"Options"},{"location":"filters/ooxml/#categories","text":"HTML returns text with the following categories. Category Description docx-content Text captured from document files. pptx-content Text captured from presentation files. xlsx-content Text captured from spreadsheet files.","title":"Categories"},{"location":"filters/python/","text":"Python \ue157 Usage \ue157 When first in the chain, the Python filter will look for the encoding of the file in the header, and convert to Unicode accordingly. It will assume utf-8 if no encoding header is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the captured text. Each docstring, inline comment, or normal string is returned as their own chunk. In general, regardless of Python version, strings should be parsed almost identically. PySpelling, unless configured otherwise, will decode string escapes and strip out format variables from f-strings. Decoding of escapes and stripping format variables is not dependent on what version of Python PySpelling is run on, but is based on the string prefixes that PySpelling encounters. There are two cases that may cause quirks related to Python version: PySpelling doesn't support being run from Python 2, but it will still find strings and comments in Python 2 code as many Python 3 projects support Python 2 as well. If you run this on Python 2 code that is not using from __future__ import unicode_literals , it will still treat the default strings in Python 2 code as Unicode as it has no way of knowing that a file is specifically meant for Python 2 parsing only. In general, you should use unicode_literals if you are supporting both Python 2 and 3. Use of \\N{NAMED UNICODE} might produce different results if one Python version defines a specific Unicode name while another does not. I'm not sure how greatly the named Unicode database varies from Python version to Python version, but if this is experienced, and is problematic, you can always disable decode_escapes in the options for a more consistent behavior. matrix : - name : python pipeline : - pyspelling.filters.python : strings : false comments : false sources : - pyspelling/**/*.py Filtering String types \ue157 When strings is enabled, you can specify which strings you want to allow via the string_types option. Valid string types are b for bytes, f for format, u for Unicode, and r for raw. f refers to f-strings, not strings in the form \"my string {} \" . format ( value ) \" , and though f-strings are Unicode, they are treated as a separate string type from Unicode. Case is not important, and the default value is fu . If specifying r , you must also specify either u , b , or f as raw strings are also either u , b , or f strings. Selecting ur will select both Unicode strings and Unicode raw strings. If you need to target just raw strings, you can use r* which will target all raw strings types: raw Unicode, raw format, and raw bytes. You can use * for other types as well. You can also just specify * by itself to target all string types. Options \ue157 Options Type Default Description comments bool True Return SourceText entries for each comment. docstrings bool True Return SourceText entries for each docstrings. group_comments bool False Group consecutive Python comments as one SourceText entry. decode_escapes bool True Decode escapes and strip out format variables. Behavior is based on the string type that is encountered. This affects both docstrings and non-docstrings. strings string False Return SourceText entries for each string (non-docstring). string_types string fu Specifies which string types strings searches: bytes ( b ), format ( f ), raw ( r ), and Unicode ( u ). * captures all strings, or when used with a type, captures all variants of that type r* . This does not affect docstrings. When docstrings is enabled, all docstrings are parsed. Categories \ue157 Python returns text with the following categories. Category Description py-comments Text captured from inline comments. py-docstring Text captured from docstrings. py-string Text captured from strings.","title":"Python"},{"location":"filters/python/#python","text":"","title":"Python"},{"location":"filters/python/#usage","text":"When first in the chain, the Python filter will look for the encoding of the file in the header, and convert to Unicode accordingly. It will assume utf-8 if no encoding header is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the captured text. Each docstring, inline comment, or normal string is returned as their own chunk. In general, regardless of Python version, strings should be parsed almost identically. PySpelling, unless configured otherwise, will decode string escapes and strip out format variables from f-strings. Decoding of escapes and stripping format variables is not dependent on what version of Python PySpelling is run on, but is based on the string prefixes that PySpelling encounters. There are two cases that may cause quirks related to Python version: PySpelling doesn't support being run from Python 2, but it will still find strings and comments in Python 2 code as many Python 3 projects support Python 2 as well. If you run this on Python 2 code that is not using from __future__ import unicode_literals , it will still treat the default strings in Python 2 code as Unicode as it has no way of knowing that a file is specifically meant for Python 2 parsing only. In general, you should use unicode_literals if you are supporting both Python 2 and 3. Use of \\N{NAMED UNICODE} might produce different results if one Python version defines a specific Unicode name while another does not. I'm not sure how greatly the named Unicode database varies from Python version to Python version, but if this is experienced, and is problematic, you can always disable decode_escapes in the options for a more consistent behavior. matrix : - name : python pipeline : - pyspelling.filters.python : strings : false comments : false sources : - pyspelling/**/*.py","title":"Usage"},{"location":"filters/python/#filtering-string-types","text":"When strings is enabled, you can specify which strings you want to allow via the string_types option. Valid string types are b for bytes, f for format, u for Unicode, and r for raw. f refers to f-strings, not strings in the form \"my string {} \" . format ( value ) \" , and though f-strings are Unicode, they are treated as a separate string type from Unicode. Case is not important, and the default value is fu . If specifying r , you must also specify either u , b , or f as raw strings are also either u , b , or f strings. Selecting ur will select both Unicode strings and Unicode raw strings. If you need to target just raw strings, you can use r* which will target all raw strings types: raw Unicode, raw format, and raw bytes. You can use * for other types as well. You can also just specify * by itself to target all string types.","title":"Filtering String types"},{"location":"filters/python/#options","text":"Options Type Default Description comments bool True Return SourceText entries for each comment. docstrings bool True Return SourceText entries for each docstrings. group_comments bool False Group consecutive Python comments as one SourceText entry. decode_escapes bool True Decode escapes and strip out format variables. Behavior is based on the string type that is encountered. This affects both docstrings and non-docstrings. strings string False Return SourceText entries for each string (non-docstring). string_types string fu Specifies which string types strings searches: bytes ( b ), format ( f ), raw ( r ), and Unicode ( u ). * captures all strings, or when used with a type, captures all variants of that type r* . This does not affect docstrings. When docstrings is enabled, all docstrings are parsed.","title":"Options"},{"location":"filters/python/#categories","text":"Python returns text with the following categories. Category Description py-comments Text captured from inline comments. py-docstring Text captured from docstrings. py-string Text captured from strings.","title":"Categories"},{"location":"filters/stylesheets/","text":"Stylesheets \ue157 Usage \ue157 The Stylesheets plugin is designed to find and return comments in CSS, SCSS, and SASS (CSS does not support inline comments). When first in the chain, the filter uses no special encoding detection. It will assume utf-8 if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text: block or inline. You can specify sass or scss in the option stylesheets if you need to capture inline comments. matrix : - name : scss pipeline : - pyspelling.filters.stylesheets : stylesheets : scss default_encoding : utf-8 sources : - docs/src/scss/*.scss aspell : lang : en dictionary : lang : en wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/scss.dic Options \ue157 Options Type Default Description stylesheets string \"css\" The stylesheet mode. block_comments bool True Return SourceText entries for each block comment. line_comments bool True Return SourceText entries for each line comment. group_comments bool False Group consecutive inline comments as one SourceText entry. Categories \ue157 Stylesheets returns text with the following categories depending on what stylesheet mode is enabled. Categories prefixed with css are for CSS etc. Category Description css-block-comment Text captured from CSS block comments. scss-block-comment Text captured from SCSS block comments. scss-line-comment Text captured from SCSS line comments. sass-block-comment Text captured from SASS block comments. sass-line-comment Text captured from SASS line comments.","title":"Stylesheets"},{"location":"filters/stylesheets/#stylesheets","text":"","title":"Stylesheets"},{"location":"filters/stylesheets/#usage","text":"The Stylesheets plugin is designed to find and return comments in CSS, SCSS, and SASS (CSS does not support inline comments). When first in the chain, the filter uses no special encoding detection. It will assume utf-8 if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text: block or inline. You can specify sass or scss in the option stylesheets if you need to capture inline comments. matrix : - name : scss pipeline : - pyspelling.filters.stylesheets : stylesheets : scss default_encoding : utf-8 sources : - docs/src/scss/*.scss aspell : lang : en dictionary : lang : en wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/scss.dic","title":"Usage"},{"location":"filters/stylesheets/#options","text":"Options Type Default Description stylesheets string \"css\" The stylesheet mode. block_comments bool True Return SourceText entries for each block comment. line_comments bool True Return SourceText entries for each line comment. group_comments bool False Group consecutive inline comments as one SourceText entry.","title":"Options"},{"location":"filters/stylesheets/#categories","text":"Stylesheets returns text with the following categories depending on what stylesheet mode is enabled. Categories prefixed with css are for CSS etc. Category Description css-block-comment Text captured from CSS block comments. scss-block-comment Text captured from SCSS block comments. scss-line-comment Text captured from SCSS line comments. sass-block-comment Text captured from SASS block comments. sass-line-comment Text captured from SASS line comments.","title":"Categories"},{"location":"filters/text/","text":"Text \ue157 Usage \ue157 This is a filter that simply retrieves the buffer's text and returns it as Unicode. It takes a file or file buffer and returns a single SourceText object containing all the text in the file. It is the default filter when there is no filter specified, though it can be manually included via pyspelling.filters.text . When first in the chain, the file's default, assumed encoding is utf-8 unless otherwise overridden by the user. The Text filter can also be used convert from one encoding to another. matrix : - name : text default_encoding : cp1252 pipeline : - pyspelling.filters.text : convert_encoding : utf-8 source : - \"**/*.txt\" Options \ue157 Options Type Default Description normalize string '' Performs Unicode normalization. Valid values are NFC , NFD , NFKC , and NFKD . convert_encoding string '' Assuming a valid encoding, the text will be converted to the specified encoding. errors string 'strict' Specifies what to do when converting the encoding, and a character can't be converted. Valid values are strict , ignore , replace , xmlcharrefreplace , backslashreplace , and namereplace . Categories \ue157 Text returns text with the following categories. Category Description text The extracted text.","title":"Text"},{"location":"filters/text/#text","text":"","title":"Text"},{"location":"filters/text/#usage","text":"This is a filter that simply retrieves the buffer's text and returns it as Unicode. It takes a file or file buffer and returns a single SourceText object containing all the text in the file. It is the default filter when there is no filter specified, though it can be manually included via pyspelling.filters.text . When first in the chain, the file's default, assumed encoding is utf-8 unless otherwise overridden by the user. The Text filter can also be used convert from one encoding to another. matrix : - name : text default_encoding : cp1252 pipeline : - pyspelling.filters.text : convert_encoding : utf-8 source : - \"**/*.txt\"","title":"Usage"},{"location":"filters/text/#options","text":"Options Type Default Description normalize string '' Performs Unicode normalization. Valid values are NFC , NFD , NFKC , and NFKD . convert_encoding string '' Assuming a valid encoding, the text will be converted to the specified encoding. errors string 'strict' Specifies what to do when converting the encoding, and a character can't be converted. Valid values are strict , ignore , replace , xmlcharrefreplace , backslashreplace , and namereplace .","title":"Options"},{"location":"filters/text/#categories","text":"Text returns text with the following categories. Category Description text The extracted text.","title":"Categories"},{"location":"filters/url/","text":"URL \ue157 Usage \ue157 This is a filter that simply strips URLs and/or email address from a file or text buffer. It takes a file or file buffer and returns a single SourceText object containing all the text in the file without URLs or email addresses. When first in the chain, the file's default, assumed encoding is utf-8 unless otherwise overridden by the user. matrix : - name : url pipeline : - pyspelling.filters.url : source : - \"**/*.txt\" Options \ue157 Options Type Default Description urls bool True Enables or disables URL stripping. emails bool True Enables or disables email address stripping. Categories \ue157 Text returns text with the following categories. Category Description url-free The text without URLs and/or emails.","title":"URL"},{"location":"filters/url/#url","text":"","title":"URL"},{"location":"filters/url/#usage","text":"This is a filter that simply strips URLs and/or email address from a file or text buffer. It takes a file or file buffer and returns a single SourceText object containing all the text in the file without URLs or email addresses. When first in the chain, the file's default, assumed encoding is utf-8 unless otherwise overridden by the user. matrix : - name : url pipeline : - pyspelling.filters.url : source : - \"**/*.txt\"","title":"Usage"},{"location":"filters/url/#options","text":"Options Type Default Description urls bool True Enables or disables URL stripping. emails bool True Enables or disables email address stripping.","title":"Options"},{"location":"filters/url/#categories","text":"Text returns text with the following categories. Category Description url-free The text without URLs and/or emails.","title":"Categories"},{"location":"filters/xml/","text":"XML \ue157 Usage \ue157 The XML filter is designed to capture XML content, comments, and even attributes. It allows for filtering out specific tags, and you can even filter them out with CSS selectors (even though this is XML content :slightly_smiling:). When first in the chain, the XML filter will look for the encoding of the file in its header and convert the buffer to Unicode. It will assume utf-8 if no encoding header is found, and the user has not overridden the fallback encoding. The HTML filter uses BeautifulSoup4 to convert the Unicode content to XML structure. Tag content is as one block for the whole file. If enabled, the XML filter will also return chunks for comments and even attributes. Each type of text chunk is returned with their own category type. Tags can be captured or ignored with the captures and ignores options. These options work by employing CSS selectors to target the tags. The CSS selectors are based on a limited subset of CSS4 selectors. matrix : - name : xml pipeline : - pyspelling.filters.xml : comments : false attributes : - some-data ignores : - :matches(ignore_tag, [ignore_attribute]) sources : - site/*.xml Supported CSS Selectors \ue157 The CSS selectors are based on a limited subset of CSS4 selectors. Support is provided via Soup Sieve. Please reference Soup Sieve's documentation for more info. Options \ue157 Options Type Default Description comments bool True Include comment text in the output. attributes [string] [] Attributes whose content should be included in the output. ignores [string] [] CSS style selectors that identify tags to ignore. Child tags will not be crawled. captures [string] [ '*|*' ] CSS style selectors used to narrow which tags that text is collected from. Unlike ignores , tags which text is not captured from still have their children crawled. namespaces dict {} Dictionary containing key value pairs of namespaces to use for CSS selectors (equivalent to @namespace in CSS). Use the an empty string for the key to define default the namespace. See below for example. break_tags [string] [] Tags to break on for context. Causes more calls to the spell checker. Namespace example matrix: - name: xml pipeline: - pyspelling.filters.xml: namespaces: \"\": http://www.w3.org/1999/xhtml svg: http://www.w3.org/2000/svg xlink: http://www.w3.org/1999/xlink Categories \ue157 HTML returns text with the following categories. Category Description xml-content Text captured from XML tags. xml-attribute Text captured from XML attributes. xml-comment Text captured from XML comments.","title":"XML"},{"location":"filters/xml/#xml","text":"","title":"XML"},{"location":"filters/xml/#usage","text":"The XML filter is designed to capture XML content, comments, and even attributes. It allows for filtering out specific tags, and you can even filter them out with CSS selectors (even though this is XML content :slightly_smiling:). When first in the chain, the XML filter will look for the encoding of the file in its header and convert the buffer to Unicode. It will assume utf-8 if no encoding header is found, and the user has not overridden the fallback encoding. The HTML filter uses BeautifulSoup4 to convert the Unicode content to XML structure. Tag content is as one block for the whole file. If enabled, the XML filter will also return chunks for comments and even attributes. Each type of text chunk is returned with their own category type. Tags can be captured or ignored with the captures and ignores options. These options work by employing CSS selectors to target the tags. The CSS selectors are based on a limited subset of CSS4 selectors. matrix : - name : xml pipeline : - pyspelling.filters.xml : comments : false attributes : - some-data ignores : - :matches(ignore_tag, [ignore_attribute]) sources : - site/*.xml","title":"Usage"},{"location":"filters/xml/#supported-css-selectors","text":"The CSS selectors are based on a limited subset of CSS4 selectors. Support is provided via Soup Sieve. Please reference Soup Sieve's documentation for more info.","title":"Supported CSS Selectors"},{"location":"filters/xml/#options","text":"Options Type Default Description comments bool True Include comment text in the output. attributes [string] [] Attributes whose content should be included in the output. ignores [string] [] CSS style selectors that identify tags to ignore. Child tags will not be crawled. captures [string] [ '*|*' ] CSS style selectors used to narrow which tags that text is collected from. Unlike ignores , tags which text is not captured from still have their children crawled. namespaces dict {} Dictionary containing key value pairs of namespaces to use for CSS selectors (equivalent to @namespace in CSS). Use the an empty string for the key to define default the namespace. See below for example. break_tags [string] [] Tags to break on for context. Causes more calls to the spell checker. Namespace example matrix: - name: xml pipeline: - pyspelling.filters.xml: namespaces: \"\": http://www.w3.org/1999/xhtml svg: http://www.w3.org/2000/svg xlink: http://www.w3.org/1999/xlink","title":"Options"},{"location":"filters/xml/#categories","text":"HTML returns text with the following categories. Category Description xml-content Text captured from XML tags. xml-attribute Text captured from XML attributes. xml-comment Text captured from XML comments.","title":"Categories"},{"location":"flow_control/wildcard/","text":"Wildcard \ue157 Usage \ue157 The Wildcard plugin is a flow control plugin. It uses Wildcard Match's fnmatch library to perform wildcard matches on categories passed to it from the pipeline in order to determine if the text should be passed to the next filter. You can define patterns for the following cases: allow : the chunk of text is allowed to be evaluated by the next filter. skip : the chunk of text should skip the next filter. halt : halts the progress of the text chunk(s) down the pipeline and sends it directly to the spell checker. Under each option, you can define a list of different patterns. The plugin will loop through the patterns until it has determined what should be done with the text. The fnmatch library is configured with the NEGATE , BRACE and IGNORECASE flags. It also allows you to specify multiple patterns on one line separated with | . See Wildcard Match's documentation to learn more about its behavior in regards to features and flags. In this example, we wish to specifically target inline Python text and ignore noqa , pragma , and shebang lines. So after the Python step, which returns both docstrings and inline comments, we specify that we only want to allow py-comment categories in the next filter. The context filter removes the lines that start with the aforementioned things, and passes the text down the pipe. The last filter step receives both the context text objects and the py-docstrings from earlier. matrix : - name : python sources : - setup.py - pyspelling/**/*.py aspell : lang : en dictionary : wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/python.dic pipeline : - pyspelling.filters.python : - pyspelling.flow_control.wildcard : allow : - py-comment - pyspelling.filters.context : context_visible_first : true delimiters : # Ignore lint (noqa) and coverage (pragma) as well as shebang (#!) - open : '^(?: *(?:noqa\\b|pragma: no cover)|!)' close : '$' # Ignore Python encoding string -*- encoding stuff -*- - open : '^ *-\\*-' close : '-\\*-$' - pyspelling.filters.context : context_visible_first : true escapes : \\\\[\\\\`~] delimiters : # Ignore multiline content between fences (fences can have 3 or more back ticks) # ``` # content # ``` - open : '(?s)^(?P<open> *`{3,})$' close : '^(?P=open)$' # Ignore text between inline back ticks - open : '(?P<open>`+)' close : '(?P=open)' Options \ue157 Options Type Default Description allow [string] [ \"*\" ] The chunk of text is allowed to be evaluated by the next filter. skip [string] [] The chunk of text should skip the next filter. halt [string] [] Halts the progress of the text chunk down the pipeline and sends it directly to the spell checker.","title":"Wildcard"},{"location":"flow_control/wildcard/#wildcard","text":"","title":"Wildcard"},{"location":"flow_control/wildcard/#usage","text":"The Wildcard plugin is a flow control plugin. It uses Wildcard Match's fnmatch library to perform wildcard matches on categories passed to it from the pipeline in order to determine if the text should be passed to the next filter. You can define patterns for the following cases: allow : the chunk of text is allowed to be evaluated by the next filter. skip : the chunk of text should skip the next filter. halt : halts the progress of the text chunk(s) down the pipeline and sends it directly to the spell checker. Under each option, you can define a list of different patterns. The plugin will loop through the patterns until it has determined what should be done with the text. The fnmatch library is configured with the NEGATE , BRACE and IGNORECASE flags. It also allows you to specify multiple patterns on one line separated with | . See Wildcard Match's documentation to learn more about its behavior in regards to features and flags. In this example, we wish to specifically target inline Python text and ignore noqa , pragma , and shebang lines. So after the Python step, which returns both docstrings and inline comments, we specify that we only want to allow py-comment categories in the next filter. The context filter removes the lines that start with the aforementioned things, and passes the text down the pipe. The last filter step receives both the context text objects and the py-docstrings from earlier. matrix : - name : python sources : - setup.py - pyspelling/**/*.py aspell : lang : en dictionary : wordlists : - docs/src/dictionary/en-custom.txt output : build/dictionary/python.dic pipeline : - pyspelling.filters.python : - pyspelling.flow_control.wildcard : allow : - py-comment - pyspelling.filters.context : context_visible_first : true delimiters : # Ignore lint (noqa) and coverage (pragma) as well as shebang (#!) - open : '^(?: *(?:noqa\\b|pragma: no cover)|!)' close : '$' # Ignore Python encoding string -*- encoding stuff -*- - open : '^ *-\\*-' close : '-\\*-$' - pyspelling.filters.context : context_visible_first : true escapes : \\\\[\\\\`~] delimiters : # Ignore multiline content between fences (fences can have 3 or more back ticks) # ``` # content # ``` - open : '(?s)^(?P<open> *`{3,})$' close : '^(?P=open)$' # Ignore text between inline back ticks - open : '(?P<open>`+)' close : '(?P=open)'","title":"Usage"},{"location":"flow_control/wildcard/#options","text":"Options Type Default Description allow [string] [ \"*\" ] The chunk of text is allowed to be evaluated by the next filter. skip [string] [] The chunk of text should skip the next filter. halt [string] [] Halts the progress of the text chunk down the pipeline and sends it directly to the spell checker.","title":"Options"}]}