{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Setup &amp; Overview","text":"<p>PySpelling is a module to help with automating spell checking in a project with Aspell or Hunspell. It is essentially a wrapper around the command line utility of these two spell checking tools, and allows you to setup different spelling tasks for different file types. You can apply specific and different filters and options to each task. PySpelling can also be used in CI environments to fail the build if there are misspelled words.</p> <p>Aspell and Hunspell are very good spell checking tools. Aspell particularly comes with a couple of filters, but the filters are limited in types and aren't extremely flexible. PySpelling was created to work around Aspell's and Hunspell's filtering shortcomings by creating a wrapper around them that could be extended to handle more kinds of file formats and provide more advanced filtering. If you need to filter out specific HTML tags with specific IDs or class names, PySpelling can do it. If you want to scan Python files for docstrings, but also avoid specific content within the docstring, you can do that as well. If PySpelling doesn't have a filter you need, with access to so many available Python modules, you can easily write your own.</p> <pre><code>Computer:pyspelling facelessuser$ pyspelling\nMisspelled words:\n&lt;html-content&gt; site/index.html: P\n--------------------------------------------------------------------------------\ncheking\nparticularlly\n--------------------------------------------------------------------------------\n\nMisspelled words:\n&lt;context&gt; pyspelling/__meta__.py(41): Pep440Version\n--------------------------------------------------------------------------------\nAccessors\naccessor\n--------------------------------------------------------------------------------\n\n!!!Spelling check failed!!!\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>PySpelling is a wrapper around either Aspell or Hunspell. If you do not have a working Aspell or Hunspell on your system, PySpelling will not work. It is up to the user to either build locally or acquire via a package manager a working spell checker installation. PySpelling pre-processes files with Python filters, and then sends the resulting text to the preferred spell checker via command line.</p>"},{"location":"#installing","title":"Installing","text":"<p>Installation is easy with pip:</p> <pre><code>$ pip install pyspelling\n</code></pre> <p>If you want to manually install it, run <code>python setup.py build</code> and <code>python setup.py install</code>.</p>"},{"location":"#command-line-usage","title":"Command Line Usage","text":"<pre><code>usage: pyspelling [-h] [--version] [--verbose] [--name NAME | --group GROUP] [--binary BINARY] [--jobs JOBS] [--config CONFIG] [--source SOURCE] [--spellchecker SPELLCHECKER]\n\nSpell checking tool.\n\noptions:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  --verbose, -v         Verbosity level.\n  --name NAME, -n NAME  Specific spelling task by name to run.\n  --group GROUP, -g GROUP\n                        Specific spelling task group to run.\n  --binary BINARY, -b BINARY\n                        Provide path to spell checker's binary.\n  --jobs JOBS, -j JOBS  Specify the number of spell checker processes to run in parallel.\n  --config CONFIG, -c CONFIG\n                        Spelling config.\n  --source SOURCE, -S SOURCE\n                        Specify override file pattern. Only applicable when specifying exactly one --name.\n  --spellchecker SPELLCHECKER, -s SPELLCHECKER\n                        Choose between aspell and hunspell\n</code></pre> <p>PySpelling can be run with the command below (assuming your Python bin/script folder is in your path).  By default it will look for the spelling configuration file <code>.pyspelling.yml</code>.</p> <pre><code>$ pyspelling\n</code></pre> <p>If you have multiple Python versions, you can run the PySpelling associated with that Python version by appending the Python major and minor version:</p> <pre><code>$ pyspelling3.11\n</code></pre> <p>To specify a specific configuration other than the default, or even point to a different location:</p> <pre><code>$ pyspelling -c myconfig.yml\n</code></pre> <p>To run a specific spelling task in your configuration file by name, you can use the <code>name</code> option. You can even specify multiple names if desired. You cannot use <code>name</code> and <code>group</code> together:</p> <pre><code>$ pyspelling -n my_task -n my_task2\n</code></pre> <p>If you've specified groups for your tasks, you can run all tasks in a group with the <code>group</code> option. You can specify multiple groups if desired. You cannot use <code>name</code> and <code>group</code> together.</p> <pre><code>$ pyspelling -g my_group -g my_group2\n</code></pre> <p>If you've specified exactly one name via the <code>name</code> option, you can override that named task's source patterns with the <code>source</code> option. You can specify multiple <code>source</code> patterns if desired.</p> <pre><code>$ pyspelling -n my_task -S \"this/specific/file.txt\" -S \"these/specific/files_{a,b}.txt\"\n</code></pre> <p>To run a more verbose output, use the <code>-v</code> flag. You can increase verbosity level by including more <code>v</code>s: <code>-vv</code>.  You can currently go up to four levels.</p> <pre><code>$ pyspelling -v\n</code></pre> <p>If the binary for your spell checker is not found in your path, you can provide a path to the binary.</p> <pre><code>$ pyspelling -b \"path/to/aspell\"\n</code></pre> <p>You can specify the spell checker type by specifying it on the command line. PySpelling supports <code>hunspell</code> and <code>aspell</code>, but defaults to <code>aspell</code>. This will override the preferred <code>spellchecker</code> setting in the configuration file.</p> <pre><code>$ pyspelling -s hunspell\n</code></pre> <p>To run multiple jobs in parallel, you can use the <code>--job</code> or <code>-j</code> option. Processing files in parallel can speed up processing time. Specifying jobs on the command line will override the <code>jobs</code> setting in the configuration file.</p> <pre><code>$ pyspelling -n my_task -j 4\n</code></pre> <p>New 2.10</p> <p>Parallel processing is new in 2.10.</p>"},{"location":"#supported-spell-check-versions","title":"Supported Spell Check Versions","text":"<p>PySpelling is tested with Hunspell 1.6+, and recommends using only 1.6 and above. Some lower versions might work, but none have been tested, and related issues will probably not be addressed.</p> <p>I usually patch the English Hunspell dictionary that I use to add apostrophes, if not present. Apostrophe support is a must for me. I also prefer to not include numbers as word characters (like Aspell) does as I find them problematic, but this is just my personal preference. Below is a patch I use on an OpenOffice dictionary set (<code>git://anongit.freedesktop.org/libreoffice/dictionaries</code>).</p> <pre><code>diff --git a/en/en_US.aff b/en/en_US.aff\nindex d0cccb3..4258f85 100644\n--- a/en/en_US.aff\n+++ b/en/en_US.aff\n@@ -14,7 +14,7 @@ ONLYINCOMPOUND c\n COMPOUNDRULE 2\n COMPOUNDRULE n*1t\n COMPOUNDRULE n*mp\n-WORDCHARS 0123456789\n+WORDCHARS \u2019\n\n PFX A Y 1\n PFX A   0     re   \n</code></pre> <p>PySpelling is also tested on Aspell 0.60+ (which is recommended), but should also work on the 0.50 series. 0.60+ is recommended as spell checking is better in the 0.60 series.</p> <p>PySpelling disables all native Aspell filters by default. If you need to enable Aspell's native filters, you can do so via Aspell's builtin options. For more information, see Aspell configuration options.</p> <p>New in 2.4.0</p> <p>Starting in 2.4.0, PySpelling ensures filters that are native to the spell checker are disabled by default.</p>"},{"location":"#usage-in-linux","title":"Usage in Linux","text":"<p>Aspell and Hunspell is most likely available in your distro's package manager. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually.</p> <p>Ubuntu Aspell install example:</p> <pre><code>$ sudo apt-get install aspell aspell-en\n</code></pre> <p>Ubuntu Hunspell install example:</p> <pre><code>$ sudo apt-get install hunspell hunspell-en-us\n</code></pre>"},{"location":"#usage-in-macos","title":"Usage in macOS","text":"<p>Aspell and Hunspell can be included via package managers such as Homebrew. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually.</p> <p>Homebrew Aspell install examples:</p> <pre><code>$ brew install aspell\n</code></pre> <p>Homebrew Hunspell install examples:</p> <pre><code>$ brew install hunspell\n</code></pre> <p>Don't forget to download dictionaries and put them to <code>/Library/Spelling/</code>.</p>"},{"location":"#usage-in-windows","title":"Usage in Windows","text":"<p>Installing Aspell and/or Hunspell in Windows is traditionally done through either a Cygwin or MSYS2/MinGW environment. If using MYSYS2/MinGW, you can usually install both packages via Pacman. You need to install both the spell checker and the dictionaries, or provide your own custom dictionaries. The option to build manually is always available as well. See your preferred spell checker's manual for more information on building manually.</p> <p>Pacman Aspell install example:</p> <pre><code>$ pacman -S mingw-w64-x86_64-aspell mingw-w64-x86_64-aspell-en\n</code></pre> <p>For Aspell, it has been noted that the way the default configuration is configured, builtin Aspell filters are often inaccessible as the configuration seems to configure paths with mixed, incompatible slash style (backslash and forward slash). By creating your own override configuration, and using forward slashes only can fix the issue. You must manually specify a proper <code>data-dir</code> and <code>dict-dir</code> override path.  This is done in our <code>appveyor.yml</code> file for our own personal tests, so you can check it out to see what is done. After fixing the configuration file, you should have everything working.</p> <p>Pacman Hunspell install example:</p> <pre><code>$ pacman -S mingw-w64-x86_64-hunspell mingw-w64-x86_64-hunspell-en\n</code></pre> <p>If you are dealing with Unicode text, Windows often has difficulty showing it in the console. Using Windows Unicode Console to patch your Windows install can help. On Python 3.6+ it might not be needed at all. Certain specialty consoles on Windows may report confusing information related to what encoding is used in the console. It is left to the user to resolve console Unicode issues, though proposals for better ways to handle this would be considered.</p> <p>Alternatively, you can just setup PySpelling in a Windows Subsystem for Linux environment and just use the Linux instructions.</p>"},{"location":"#usage-in-ci","title":"Usage in CI","text":"<p>PySpelling was originally written so that it could be used to automate tests in a CI environment. Any automated CI can use PySpelling assuming the environment is setup appropriately. On most systems it can be pretty straight forward, on systems like Windows, it may be a bit more complicated as you will have to also setup a Cygwin, MSYS2/MinGW, Windows Subsystem for Linux environment.</p> <p>We won't go into all possible CI environments in this documentation, but we will cover how to get PySpelling up and running on GitHub's CI environment. In the past, PySpelling has successfully been used in Travis CI, AppVeyor, and many others.</p> <p>In order to get running in GitHub's CI, you can follow the steps below:</p> <ol> <li>Create a <code>spelling</code> task under <code>jobs</code>.</li> <li>Specify a Linux environment as it is one of the easiest to configure and get running.</li> <li>Use the <code>actions/checkout</code> action to checkout your project.</li> <li>Setup your Python environment using the <code>actions/setup-python</code> action.</li> <li>Install dependencies you may need. Below, we update <code>pip</code> and <code>setuptools</code> and install <code>pyspelling</code>. You may require     additional dependencies depending on spelling extensions used, or if pre-building of documents is needed.</li> <li>Install Aspell and Aspell dictionaries. You are also free to use Hunspell if preferred.</li> <li>Below we've allowed for a <code>Build documents</code> step where you can build documentation or do any other file     preprocessing that is required for your specific environment.</li> <li>Lastly, run PySpelling.</li> </ol> <pre><code>jobs:\n  spelling:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - name: Set up Python 3.7\n      uses: actions/setup-python@v1\n      with:\n        python-version: 3.7\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip setuptools\n        python -m pip install pyspelling\n        # Install any additional libraries required: additional plugins, documentation building libraries, etc.\n    - name: Install Aspell\n      run: |\n        sudo apt-get install aspell aspell-en\n    - name: Build documents\n      run: |\n        # Perform any documentation building that might be required\n    - name: Spell check\n      run: |\n        python -m pyspelling\n</code></pre> <p>In this project, we actually use <code>tox</code> to make running our tests locally and in CI easier. If you would like to use <code>tox</code> as well, you can check out how this project does it by taking a look at the source.</p>"},{"location":"api/","title":"Plugin API","text":""},{"location":"api/#filters","title":"Filters","text":"<p>When writing a <code>Filter</code> plugin, there are two classes to be aware: <code>Filter</code> and <code>SourceText</code>. Both classes are found in <code>pyspelling.filters</code>.</p> <p>Each chunk returned by a filter is a <code>SourceText</code> object. These objects contain the desired, filtered text from the previous filter along with some metadata: encoding, display context, and a category that describes what kind of text the data is. After all filters have processed the text, each <code>SourceText</code>'s content is finally passed to the spell checker.</p> <p>The text data in a <code>SourceText</code> object is always Unicode, but during the filtering process, the filter can decode the Unicode if required as long as it is returned as Unicode at the end of the step.</p>"},{"location":"api/#filter","title":"<code>Filter</code>","text":"<p><code>Filter</code> plugins are subclassed from the <code>Filter</code> class.  You'll often want to specify the defaulted value for <code>default_encoding</code> in the <code>__init__</code>. Simply give it a default value as shown below.</p> <pre><code>from .. import filters\n\n\nclass MyFilter(filters.Filter):\n    \"\"\"Spelling Filter.\"\"\"\n\n    def __init__(self, options, default_encoding='utf-8'):\n        \"\"\"Initialization.\"\"\"\n\n        super().__init__(options, default_encoding)\n</code></pre>"},{"location":"api/#filterget_default_config","title":"<code>Filter.get_default_config</code>","text":"<p><code>get_default_config</code> is where you should specify your default configuration file. This should contain all accepted options and their default value. All user options that are passed in will override the defaults. If an option is passed in that is not found in the defaults, an error will be raised.</p> <pre><code>    def get_default_config(self):\n        \"\"\"Get default configuration.\"\"\"\n\n        return {\n            \"enable_something\": True,\n            \"list_of_stuff\": ['some', 'stuff']\n        }\n</code></pre> <p>\"New 2.0</p> <p><code>get_default_confg</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#filtervalidate_options","title":"<code>Filter.validate_options</code>","text":"<p><code>validate_options</code> is where you can specify validation of your options. By default, basic validation is done on incoming options. For instance, if you specify a default as a <code>bool</code>, the default validator will ensure the passed user options match. Checking is performed on <code>bool</code>, <code>str</code>, <code>list</code>, <code>dict</code>, <code>int</code>, and <code>float</code> types. Nothing beyond simple type checking is performed, so if you had some custom validation, or simply wanted to bypass the default validator with your own, you should override <code>validate_options</code>.</p> <pre><code>    def validate_options(self, k, v):\n        \"\"\"Validate options.\"\"\"\n\n        # Call the basic validator\n        super().validate_options(k, v)\n\n        # Perform custom validation\n        if k == \"foo\" and v != \"bar\":\n            raise ValueError(\"Value should be 'bar' for 'foo'\")\n</code></pre> <p>New 2.0</p> <p><code>validate_options</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#filtersetup","title":"<code>Filter.setup</code>","text":"<p><code>setup</code> is were basic setup can be performed post-validation. At this point, you can access the merged and validated configuration via <code>self.config</code>.</p> <pre><code>    def setup(self):\n        \"\"\"Setup.\"\"\"\n\n        self.enable_foo = self.config['foo']\n</code></pre> <p>New 2.0</p> <p><code>setup</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#filterreset","title":"<code>Filter.reset</code>","text":"<p><code>reset</code> is called on every new call to the plugin. It allows you to clean up states from previous calls.</p> <pre><code>    def reset(self):\n        \"\"\"Reset\"\"\"\n\n        self.counter = 0\n        self.tracked_stuff = []\n</code></pre> <p>New 2.0</p> <p><code>reset</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#filterhas_bom","title":"<code>Filter.has_bom</code>","text":"<p><code>has_bom</code> takes a file stream and is usually used to check the first few bytes. While BOM checking could be performed in <code>header_check</code>, this mainly provided as <code>UTF</code> BOMs are quite common in many file types, so a specific test was dedicated to it. Additionally, this replaces the old, less flexible <code>CHECK_BOM</code> attribute that was deprecated in version <code>1.2</code>.</p> <p>This is useful if you want to handle binary parsing, or a file type that has a custom BOM in the header. When returning encoding in any of the encoding check functions, <code>None</code> means no encoding was detecting, an empty string means binary data (encoding validation is skipped), and anything else will be validated and passed through. Just be sure to include a sensible encoding in your <code>SourceText</code> object when your plugin returns file content.</p> <pre><code>    def has_bom(self, filestream):\n        \"\"\"Check if has BOM.\"\"\"\n\n        content = filestream.read(2)\n        if content == b'PK\\x03\\x04':\n            # Zip file found.\n            # Return `BINARY_ENCODE` as content is binary type,\n            # but don't return None which means we don't know what we have.\n            return filters.BINARY_ENCODE\n        # Not a zip file, so pass it on to the normal file checker.\n        return super().has_bom(filestream)\n</code></pre> <p>New 2.0</p> <p><code>has_bom</code> was added in version <code>2.0</code>.</p> <p>Deprecation 2.0</p> <p><code>CHECK_BOM</code> has been deprecated since <code>2.0</code>.</p>"},{"location":"api/#filterheader_check","title":"<code>Filter.header_check</code>","text":"<p><code>header_check</code> is a function that receives the first 1024 characters of the file via <code>content</code> that can be scanned for an encoding header. A string with the encoding name should be returned or <code>None</code> if a valid encoding header cannot be found.</p> <pre><code>    def header_check(self, content):\n        \"\"\"Special encode check.\"\"\"\n\n        return None\n</code></pre>"},{"location":"api/#filtercontent_check","title":"<code>Filter.content_check</code>","text":"<p><code>content_check</code> receives a file object which allows you to check the entire file buffer to determine encoding. A string with the encoding name should be returned or <code>None</code> if a valid encoding header cannot be found.</p> <pre><code>    def content_check(self, filestream):\n        \"\"\"File content check.\"\"\"\n\n        return None\n</code></pre>"},{"location":"api/#filterfilter","title":"<code>Filter.filter</code>","text":"<p><code>filter</code> is called when the <code>Filter</code> object is the first in the chain. This means the file has not been read from disk yet, so we must handle opening the file before applying the filter and then return a list of <code>SourceText</code> objects. The first filter in the chain is handled differently in order to give the opportunity to handle files that require more complex methods to acquire the Unicode strings. You can read the file in binary format or directly to Unicode.  You can run parsers or anything else you need in order to get the required Unicode text for the <code>SourceText</code> objects. You can create as many <code>SourceText</code> objects as you desired and assign them categories so that other <code>Filter</code> objects can avoid them if desired. Below is the default which reads the entire file into a single object providing the file name as the context, the encoding, and the category <code>text</code>.</p> <pre><code>    def filter(self, source_file, encoding):  # noqa A001\n        \"\"\"Open and filter the file from disk.\"\"\"\n\n        with codecs.open(source_file, 'r', encoding=encoding) as f:\n            text = f.read()\n        return [SourceText(text, source_file, encoding, 'text')]\n</code></pre>"},{"location":"api/#filtersfilter","title":"<code>Filter.sfilter</code>","text":"<p><code>sfilter</code> is called for all <code>Filter</code> objects following the first.  The function is passed a <code>SourceText</code> object from which the text, context, encoding can all be extracted. Here you can manipulate the text back to bytes if needed, wrap the text in an <code>io.StreamIO</code> object to act as a file stream, run parsers, or anything you need to manipulate the buffer to filter the Unicode text for the <code>SourceText</code> objects.</p> <pre><code>    def sfilter(self, source):\n        \"\"\"Execute filter.\"\"\"\n\n        return [SourceText(source.text, source.context, source.encoding, 'text')]\n</code></pre> <p>If a filter only works either as the first in the chain, or only as a secondary filter in the chain, you could raise an exception if needed.  In most cases, you should be able to have an appropriate <code>filter</code> and <code>sfilter</code>, but there are most likely cases (particular when dealing with binary data) where only a <code>filter</code> method could be provided.</p> <p>Check out the default filter plugins provided with the source to see real world examples.</p>"},{"location":"api/#get_plugin","title":"<code>get_plugin</code>","text":"<p>And don't forget to provide a function in the file called <code>get_plugin</code>! <code>get_plugin</code> is the entry point and should return your <code>Filter</code> object.</p> <pre><code>def get_plugin():\n    \"\"\"Return the filter.\"\"\"\n\n    return HtmlFilter\n</code></pre>"},{"location":"api/#sourcetext","title":"<code>SourceText</code>","text":"<p>As previously mentioned, filters must return a list of <code>SourceText</code> objects.</p> <pre><code>class SourceText(namedtuple('SourceText', ['text', 'context', 'encoding', 'category', 'error'])):\n    \"\"\"Source text.\"\"\"\n</code></pre> <p>Each object should contain a Unicode string (<code>text</code>), some context on the given text hunk (<code>context</code>), the encoding which the Unicode text was originally in (<code>encoding</code>), and a <code>category</code> that is used to omit certain hunks from other filters in the chain (<code>category</code>). <code>SourceText</code> should not contain byte strings, and if they do, they will not be passed to additional filters. <code>error</code> is optional and is only provided message when something goes wrong.</p> <p>When receiving a <code>SourceText</code> object in your plugin, you can access the content via attributes with the same name as the parameters above:</p> <pre><code>&gt;&gt;&gt; source.text\n'Some Text'\n&gt;&gt;&gt; source.context\n'foo.txt'\n&gt;&gt;&gt; source.encoding\n'utf-8'\n&gt;&gt;&gt; source.category\n'some-category'\n</code></pre> <p>Be mindful when adjusting the context in subsequent items in the pipeline chain. Generally you should only append additional context so as not to wipe out previous contextual data. It may not always make sense to append additional data, so some filters might just pass the previous context as the new context.</p> <p>If you have a particular chunk of text that has a problem, you can return an error in the <code>SourceText</code> object.  Errors really only need a context and the error as they won't be passed to the spell checker or to any subsequent steps in the pipeline. Errors are only used to alert the user that something went wrong. <code>SourceText</code> objects with errors will not be passed down the chain and will not be passed to the spell checker.</p> <pre><code>if error:\n    content = [SourceText('', source_file, '', '', error)]\n</code></pre>"},{"location":"api/#flow-control","title":"Flow Control","text":"<p><code>FlowControl</code> plugins are simple plugins that take the category from a <code>SourceText</code> object, and simply returns either the directive <code>HALT</code>, <code>SKIP</code>, or <code>ALLOW</code>. This controls whether the associated <code>SourceText</code> object's progress is halted in the pipeline, skips the next filter, or is explicitly allowed in the next filter. The directives and <code>FlowControl</code> class are found in <code>pyspelling.flow_control</code>.</p>"},{"location":"api/#flowcontrol","title":"<code>FlowControl</code>","text":"<p><code>FlowControl</code> plugins should be subclassed from <code>FlowControl</code>. If you need to you can override the <code>__init__</code>, but remember to call the original with <code>super</code> to ensure options are handled.</p> <pre><code>class MyFlowControl(flow_control.FlowControl):\n    \"\"\"Flow control plugin.\"\"\"\n\n    def __init__(self, config):\n        \"\"\"Initialization.\"\"\"\n\n        super().__init__(config)\n</code></pre>"},{"location":"api/#flowcontrolget_default_config","title":"<code>FlowControl.get_default_config</code>","text":"<p><code>get_default_config</code> is where you should specify your default configuration file. This should contain all accepted options and their default value. All user options that are passed in will override the defaults. If an option is passed in that is not found in the defaults, an error will be raised.</p> <pre><code>    def get_default_config(self):\n        \"\"\"Get default configuration.\"\"\"\n\n        return {\n            \"enable_something\": True,\n            \"list_of_stuff\": ['some', 'stuff']\n        }\n</code></pre> <p>New 2.0</p> <p><code>get_default_confg</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#flowcontrolvalidate_options","title":"<code>FlowControl.validate_options</code>","text":"<p><code>validate_options</code> is where you can specify validation of your options. By default, basic validation is done on incoming options. For instance, if you specify a default as a <code>bool</code>, the default validator will ensure the passed user options match. Checking is performed on <code>bool</code>, <code>str</code>, <code>list</code>, <code>dict</code>, <code>int</code>, and <code>float</code> types. Nothing beyond simple type checking is performed, so if you had some custom validation, or simply wanted to bypass the default validator with your own, you should override <code>validate_options</code>.</p> <pre><code>    def validate_options(self, k, v):\n        \"\"\"Validate options.\"\"\"\n\n        # Call the basic validator\n        super().validate_options(k, v)\n\n        # Perform custom validation\n        if k == \"foo\" and v != \"bar\":\n            raise ValueError(\"Value should be 'bar' for 'foo'\")\n</code></pre> <p>New 2.0</p> <p><code>validate_options</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#flowcontrolsetup","title":"<code>FlowControl.setup</code>","text":"<p><code>setup</code> is were basic setup can be performed post-validation. At this point, you can access the merged and validated configuration via <code>self.config</code>.</p> <pre><code>    def setup(self):\n        \"\"\"Setup.\"\"\"\n\n        self.enable_foo = self.config['foo']\n</code></pre> <p>New 2.0</p> <p><code>setup</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#flowcontrolreset","title":"<code>FlowControl.reset</code>","text":"<p><code>reset</code> is called on every new call to the plugin. It allows you to clean up states from previous calls.</p> <pre><code>    def reset(self):\n        \"\"\"Reset\"\"\"\n\n        self.counter = 0\n        self.tracked_stuff = []\n</code></pre> <p>New 2.0</p> <p><code>reset</code> was added in version <code>2.0</code>.</p>"},{"location":"api/#flowcontroladjust_flow","title":"<code>FlowControl.adjust_flow</code>","text":"<p>After handling the options, there is only one other function available for overrides: <code>adjust_flow</code>.  Adjust flow receives the category from the <code>SourceText</code> being passed down the pipeline. Here the decision is made to as to what must be done with the object. Simply return <code>HALT</code>, <code>SKIP</code>, or <code>ALLOW</code> to control the flow for that <code>SourceText</code> object.</p> <pre><code>    def adjust_flow(self, category):\n        \"\"\"Adjust the flow of source control objects.\"\"\"\n\n        status = flow_control.SKIP\n        for allow in self.allow:\n            if fnmatch.fnmatch(category, allow, flags=self.FNMATCH_FLAGS):\n                status = flow_control.ALLOW\n                for skip in self.skip:\n                    if fnmatch.fnmatch(category, skip, flags=self.FNMATCH_FLAGS):\n                        status = flow_control.SKIP\n                for halt in self.halt:\n                    if fnmatch.fnmatch(category, halt, flags=self.FNMATCH_FLAGS):\n                        status = flow_control.HALT\n                if status != flow_control.ALLOW:\n                    break\n        return status\n</code></pre> <p>Check out the default flow control plugins provided with the source to see real world examples.</p>"},{"location":"api/#get_plugin_1","title":"<code>get_plugin</code>","text":"<p>And don't forget to provide a function in the file called <code>get_plugin</code>! <code>get_plugin</code> is the entry point and should return your <code>FlowControl</code> object.</p> <pre><code>def get_plugin():\n    \"\"\"Get flow controller.\"\"\"\n\n    return WildcardFlowControl\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration-file","title":"Configuration File","text":"<p>PySpelling requires a YAML configuration file. The file defines the various spelling tasks along with their individual filters and options.</p> <p>You can optionally specify the preferred spell checker as a global option (<code>aspell</code> is the default if not specified). This can be overridden on the command line.</p> <pre><code>spellchecker: hunspell\n</code></pre> <p>You can specify the number of parallel jobs to use by setting the global option <code>jobs</code>. This create parallel jobs to process files in a given task.</p> <pre><code>jobs: 4\n</code></pre> <p>New 2.10</p> <p>Parallel processing is new in 2.10.</p> <p>All of the spelling tasks are contained under the keyword <code>matrix</code> and are organized in a list:</p> <pre><code>matrix:\n- task1\n\n- task2\n</code></pre> <p>Each task requires, at the very least, a <code>name</code> and <code>sources</code> to search.</p> <p>Depending on your setup, you may need to set the dictionary to use as well. Each spell checker specifies their dictionary/language differently which is covered in more details in Spell Checker Options.</p> <pre><code>matrix:\n- name: Python Source\n  aspell:\n    lang: en\n    d: en_US\n  sources:\n  - pyspelling/**/*.py\n</code></pre> <p>You can also define more complicated tasks which will run your text through various filters before performing the spell checking by providing a custom pipeline.  You can also add your own custom wordlists to extend the dictionary.</p> <pre><code>matrix:\n- name: Python Source\n  sources:\n  - pyspelling/**/*.py\n  aspell:\n    lang: en\n    d: en_US\n  dictionary:\n    wordlists:\n    - docs/src/dictionary/en-custom.txt\n    output: build/dictionary/python.dic\n  pipeline:\n  - pyspelling.filters.python:\n  - pyspelling.filters.context:\n      context_visible_first: true\n      escapes: \\\\[\\\\`~]\n      delimiters:\n      # Ignore multiline content between fences (fences can have 3 or more back ticks)\n      # ```\n      # content\n      # ```\n      - open: '(?s)^(?P&lt;open&gt; *`{3,})$'\n        close: '^(?P=open)$'\n      # Ignore text between inline back ticks\n      - open: '(?P&lt;open&gt;`+)'\n        close: '(?P=open)'\n</code></pre>"},{"location":"configuration/#name","title":"Name","text":"<p>Each spelling tasks should have a unique name and is defined with the <code>name</code> key.</p> <p>When using the command line <code>--name</code> option, the task with the matching name will be run.</p> <pre><code>matrix:\n- name: python\n</code></pre> <p>New Behavior 2.0</p> <p>In <code>1.0</code>, names doubled as identifiers and groups. It became apparent for certain features that a unique name is desirable for targeting different tasks, while a group specifier should be implemented separately. In <code>2.0</code>, if multiple tasks have the same name, the last defined one will be the targeted task when requesting a named task. Use groups to target multiple grouped tasks.</p>"},{"location":"configuration/#groups","title":"Groups","text":"<p>Each task can be assigned to a group. The group name can be shared with multiple tasks. All tasks in a group can be run by specifying the <code>--group</code> option with the name of the group on the command line. This option is only available in version <code>1.1</code> of the configuration file.</p> <pre><code>matrix:\n- name: python\n  group: some_name\n</code></pre> <p>New 2.0</p> <p><code>group</code> was added in version <code>2.0</code>.</p>"},{"location":"configuration/#hidden","title":"Hidden","text":"<p>All tasks in a configuration file will be run if no <code>name</code> is specified. In version <code>1.1</code> of the configuration file, If a task enables the option <code>hidden</code> by setting it to <code>true</code>, that task will not be run automatically when no <code>name</code> is specified. <code>hidden</code> tasks will only be run if they are specifically mentioned by <code>name</code>.</p> <pre><code>matrix:\n- name: python\n  hidden: true\n</code></pre> <p>New 2.0</p> <p><code>group</code> was added in version <code>2.0</code>.</p>"},{"location":"configuration/#default-encoding","title":"Default Encoding","text":"<p>When parsing a file, the encoding detection and translation of the data into Unicode is performed by the first filter in the pipeline. For instance, if HTML is the first, it may check for a BOM or look at the file's header to find the <code>meta</code> tag that specifies the file's encoding. If all encoding checks fail, the filter will usually apply an appropriate default encoding for the file content type (usually UTF-8, but check the specific filter's documentation to be sure). If needed, the filter's default encoding can be overridden in the task via the <code>default_encoding</code> key. After the first step in the pipeline, the text is passed around as Unicode which requires no Unicode detection.</p> <pre><code>matrix:\n- name: markdown\n  pipeline:\n  - pyspelling.filters.text\n  sources:\n  - '**/*.md'\n  default_encoding: utf-8\n</code></pre> <p>Once all filtering is complete, the text will be passed to the spell checker as byte strings, usually with the originally detected encoding (unless a filter specifically alters the encoding). The supported spell checkers are limited to very specific encodings, so if your file is using an unsupported encoding, it will fail.</p> <p>UTF-16 and UTF-32 is not really supported by Aspell and Hunspell, so at the end of the spell check pipeline, Unicode strings that have the associated encoding of UTF-16 or UTF-32 will be encoded with the compatible UTF-8. This does not apply to files being processed with a the pipeline disabled. When the pipeline is disabled, files are sent directly to the spell checker with no modifications.</p> <p>Unsupported Encodings</p> <p>If you are trying to spell check a file in an unsupported encoding, you can use the builtin text filter to convert the content to a more appropriate encoding. In general, it is recommended to work in, or convert to UTF-8.</p>"},{"location":"configuration/#sources","title":"Sources","text":"<p>Each spelling task must define a list of sources to search via the <code>sources</code> key. Each source should be a glob pattern that should match one or more files. PySpelling will perform a search with these patterns to determine which files should be spell checked.</p> <p>You can also have multiple patterns on one line separated with <code>|</code>. When multiple patterns are defined like this, they are evaluated simultaneously. This is useful if you'd like to provide an exclusion pattern along with your file pattern. For instance, if we wanted to scan all python files in our folder, but exclude any in the build folder, we could provide the following pattern: <code>**/*.py|!build/*</code>.</p> <p>PySpelling uses Wildcard Match's <code>glob</code> library to perform the file globbing.  By default, it uses the <code>NEGATE</code>, <code>GLOBSTAR</code>, and <code>BRACE</code> flags, but you can override the flag options with the <code>glob_flags</code> option. You can specify the flags by either their long name <code>GLOBSTAR</code> or their short name <code>G</code>. See Wildcard Match's documentation for more information on the available flags and what they do.</p> <pre><code>matrix:\n- name: python\n  pipeline:\n  - pyspelling.filters.python:\n      comments: false\n  glob_flags: N|G|B\n  sources:\n  - pyspelling/**/*.py\n</code></pre> <p>By default, to protect against really large pattern sets, such as when using brace expansion: <code>{1..10000000}</code>, there is a pattern limit of <code>1000</code> by default. This can be changed by setting <code>glob_pattern_limit</code> to some other number. If you set it to <code>0</code>, it will disable the pattern limits entirely.</p> <p>New 2.6</p> <p><code>glob_pattern_limit</code> is new in version <code>2.6</code> and only works with <code>wcmatch</code> version <code>6.0</code>.</p>"},{"location":"configuration/#expect-match","title":"Expect Match","text":"<p>When processing the sources field it is expected to find at least one matching file. If no files are located it can be helpful to raise an error and this is the default behavior. If it is not expected to always find a file then the <code>expect_match</code> configuration can be used to suppress the error.</p> <pre><code>matrix:\n- name: markdown\n  pipeline:\n  - pyspelling.filters.text\n  sources:\n  - '**/*.md'\n  expect_match: false\n  default_encoding: utf-8\n</code></pre>"},{"location":"configuration/#pipeline","title":"Pipeline","text":"<p>Note</p> <p>PySpelling's <code>pipeline</code> is designed to provide advanced, custom filtering above and beyond what spell checker's normally provide, but it may often be the case that what the spell checker provides is more than sufficient. It should be noted that <code>pipeline</code> filters are processed before sending the buffer to Aspell or Hunspell. By default, we disable any special modes of the spell checkers.</p> <p>Spellcheckers like Aspell have builtin filtering. If all you need is the builtin filters from Aspell, the <code>pipeline</code> configuration can be omitted. For instance, to use Aspell's builtin Markdown mode, simply set the Aspell option directly and omit the pipeline.</p> <pre><code>- name: markdown\n  group: docs\n  sources:\n  - README.md\n  aspell:\n    lang: en\n    d: en_US\n    mode: markdown\n  dictionary:\n    wordlists:\n    - .spell-dict\n    output: build/dictionary/markdown.dic\n</code></pre> <p>You can also use PySpelling <code>pipeline</code> filters and enable special modes of the underlying spell checker if desired.</p> <p>PySpelling allows you to define tasks that outline what kind of files you want to spell check, and then sends them down a pipeline that filters the content returning chunks of text with some associated context. Each chunk is sent down each step of the pipeline until it reaches the final step, the spell check step. Between filter steps, you can also insert flow control steps that allow you to have certain text chunks skip specific steps. All of this is done with pipeline plugins.</p> <p>Let's say you had some Markdown files and wanted to convert them to HTML, and then filter out specific tags. You could just use the Markdown filter to convert the file to HTML and then pass it through the HTML filter to extract the text from the HTML tags.</p> <pre><code>matrix:\n- name: markdown\n  sources:\n  - README.md\n  pipeline:\n  - pyspelling.filters.markdown:\n  - pyspelling.filters.html:\n      comments: false\n      attributes:\n      - title\n      - alt\n      ignores:\n      - code\n      - pre\n</code></pre> <p>If needed, you can also insert flow control steps before certain filter steps. Each text chunk that is passed between filters has a category assigned to it from the previous filter. Flow control steps allow you to restrict the next filter to specific categories, or exclude specific categories from the next step. This is covered in more depth in Flow Control.</p> <p>If for some reason you need to send the file directly to the spell checker without using PySpelling's pipeline, simply set <code>pipeline</code> to <code>null</code>. This sends file directly to the spell checker without evaluating the encoding or passing through any filters. Specifically with Hunspell, it also sends the spell checker the filename instead of piping the content as Hunspell has certain features that don't work when piping the data, such as OpenOffice ODF input.</p> <p>Below is an example where we send an OpenOffice ODF file directly to Hunspell in order to use Hunspell's <code>-O</code> option to parse the ODF file. Keep in mind that when doing this, no encoding is sent to the spell checker unless you define <code>default_encoding</code>. If <code>default_encoding</code> is not defined, PySpelling will decode the returned content with the terminal's encoding (or what it thinks the terminal's encoding is).</p> <pre><code>matrix:\n- name: openoffice_ODF\n  sources:\n  - file.odt\n  hunspell:\n    d: en_US\n    O: true\n  pipeline: null\n</code></pre>"},{"location":"configuration/#languages","title":"Languages","text":"<p>Languages in both Aspell and Hunspell are controlled by the <code>-d</code> option. In the YAML configuration, we remove any leading <code>-</code>and just use <code>d</code>.</p> <p>For Aspell:</p> <pre><code>matrix:\n- name: python\n  aspell:\n    lang: en\n    d: en_US\n</code></pre> <p>For Hunspell:</p> <pre><code>matrix:\n- name: python\n  hunspell:\n    d: en_US\n</code></pre> <p>Tip</p> <p>It can be noted above that Aspell is setting <code>lang</code> to <code>en</code>  and <code>d</code> to <code>en_US</code>, this isn't strictly necessary to just spell check, but is often needed to compile wordlists of words to ignore when spellchecking. <code>lang</code> points to the actual <code>.dat</code> file used to compile wordlists in Aspell and needs that information to work. There is usually one <code>.dat</code> file that covers a language and its variants. So <code>en_US</code> and <code>en_GB</code> will both build their wordlists against the <code>en.dat</code> file.</p> <p>Since spell checker options vary between both Aspell and Hunspell, spell checker specific options are handled by under special keys named <code>aspell</code> and <code>hunspell</code>. To learn more, check out Spell Checker Options.</p> <p>By default, PySpelling sets your main dictionary to <code>en</code> for Aspell and <code>en_US</code> for Hunspell. If you do not desire an American English dictionary, or these dictionaries are not installed in their expected default locations, you will need to configure PySpelling so it can find your preferred dictionary. Since dictionary configuring varies for each spell checker, the main dictionary (and virtually any spell checker specific option) is performed via Spell Checker Options.</p> <p>International Languages</p> <p>Some languages use special Unicode characters in them. The spell checker in use may be particular about how the Unicode characters are normalized. When PySpelling passes the content to be spellchecked you may want to normalize the Unicode content if it is having trouble. This can be done with the <code>Text</code> filter.</p> <p>For instance, here is how to do so via Aspell with a Czech dictionary.</p> <pre><code>matrix:\n- name: czechstuff\n  sources: *.txt\n  aspell:\n    lang: cs\n    d: cs\n  dictionary:\n    wordlist:\n    - .dictionary\n    output: build/czech.dict\n  pipeline:\n  - pyspelling.filters.text:\n      normalize: nfd\n</code></pre>"},{"location":"configuration/#dictionaries-and-personal-wordlists","title":"Dictionaries and Personal Wordlists","text":"<p>While provided dictionaries cover a number of commonly used words, you may need to specify additional words that are not covered in the default dictionaries. Luckily, both Aspell and Hunspell allow for adding custom wordlists. You can have as many wordlists as you like, and they can be included in a list under the key <code>wordlists</code> which is also found under the key <code>dictionary</code>.</p> <p>All the wordlists are combined into one custom dictionary file whose output name and location is defined via the <code>output</code> key which is also found under the <code>dictionary</code> key.</p> <p>While Hunspell doesn't directly compile the wordlists, Aspell does, and it uses the <code>.dat</code> file for dictionary you are using. While you may be specifying a region specific versions of English with <code>en_US</code> or <code>en_GB</code>, both of these use the <code>en.dat</code> file. So in Aspell, it is recommended to specify both the <code>--lang</code> option (or the alias <code>-l</code>) as well as <code>-d</code>. If <code>lang</code> is not specified, the assumed data file will be <code>en</code>.</p> <pre><code>matrix:\n- name: python\n  sources:\n  - pyspelling/**/*.py\n  aspell:\n    lang: en\n    d: en_US\n  dictionary:\n    wordlists:\n    - docs/src/dictionary/en-custom.txt\n    output: build/dictionary/python.dic\n  pipeline:\n  - pyspelling.filters.python:\n      comments: false\n</code></pre> <p>Hunspell, on the other hand, does not require an additional <code>lang</code> option as custom wordlists are handled differently than when under Aspell:</p> <pre><code>matrix:\n- name: python\n  sources:\n  - pyspelling/**/*.py\n  hunspell:\n    d: en_US\n  dictionary:\n    wordlists:\n    - docs/src/dictionary/en-custom.txt\n    output: build/dictionary/python.dic\n  pipeline:\n  - pyspelling.filters.python:\n      comments: false\n</code></pre> <p>Lastly, you can set the encoding to be used during compilation via the <code>encoding</code> under <code>dictionary</code>. The encoding should generally match the encoding of your main dictionary. The default encoding is <code>utf-8</code>, and only Aspell uses this option.</p> <pre><code>matrix:\n- name: python\n  sources:\n  - pyspelling/**/*.py\n  aspell:\n    lang: en\n    d: en_US\n  dictionary:\n    wordlists:\n    - docs/src/dictionary/en-custom.txt\n    output: build/dictionary/python.dic\n    encoding: utf-8\n  pipeline:\n  - pyspelling.filters.python:\n      comments: false\n</code></pre>"},{"location":"configuration/#spell-checker-options","title":"Spell Checker Options","text":"<p>Since PySpelling is a wrapper around both Aspell and Hunspell, there are a number of spell checker specific options. Spell checker specific options can be set under keywords: <code>aspell</code> and <code>hunspell</code> for Aspell and Hunspell respectively. Here you can set options like the default dictionary and search options.</p> <p>We will not list all available options here. In general we expose any and all options and only exclude those that we are aware of that could be problematic. For instance, we do not have an interface for interactive suggestions, so such options are not allowed with PySpelling.</p> <p>Spell checker specific options basically translate directly to the spell checker's command line options and only requires you to remove the leading <code>-</code>s you would normally specify on the command line. For instance, a short form option such as <code>-l</code> would simply be represented with the keyword <code>l</code>, and the long name form of the same option <code>--lang</code> would be represented as <code>lang</code>. Following the key, you would provide the appropriate value depending on it's requirement.</p> <p>Boolean flags would be set to <code>true</code>.</p> <pre><code>matrix:\n- name: html\n  sources:\n  - docs/**/*.html\n  aspell:\n    H: true\n  pipeline:\n  - pyspelling.filters.html\n</code></pre> <p>Other options would be set to a string or an integer value.</p> <pre><code>matrix:\n- name: python\n  sources:\n  - pyspelling/**/*.py\n  aspell:\n    lang: en\n    d: en_US\n  pipeline:\n  - pyspelling.filters.python:\n      strings: false\n      comments: false\n</code></pre> <p>Lastly, if you have an option that can be used multiple times, just set the value up as an array, and the option will be added for each value in the array. Assuming you had multiple pre-compiled dictionaries, you could add them under Aspell's <code>--add-extra-dicts</code> option:</p> <pre><code>matrix:\n- name: Python Source\n  sources:\n  - pyspelling/**/*.py\n  aspell:\n    add-extra-dicts:\n    - my-dictionary.dic\n    - my-other-dictionary.dic\n  pipeline:\n</code></pre> <p>The above options would be equivalent to doing this from the command line:</p> <pre><code>$ aspell --add-extra-dicts my-dictionary.dic --add-extra-dicts my-other-dictionary.dic\n</code></pre>"},{"location":"pipeline/","title":"Spelling Pipeline","text":""},{"location":"pipeline/#overview","title":"Overview","text":"<p>PySpelling's pipeline utilizes special plugins to provide text filtering and to control the flow of the text down the pipeline. The plugins can be arranged in any order and even included multiple times, the only restriction is that you can't start the pipeline with <code>FlowControl</code> plugins, the first plugin must be a <code>Filter</code> plugin.</p> <p>A number of plugins are included with PySpelling, but additional plugins can be written using the plugin API.</p>"},{"location":"pipeline/#filter","title":"Filter","text":"<p><code>Filter</code> plugins are used to augment and/or filter a given chunk of text returning only the portions that are desired. Once a plugin is done with the text, it passes it down the pipeline. A filter may return one or many chunks, each with a little contextual information. Some filters may return only one chunk of text that is the entirety of the file, and some may return context specific chunks: one for each docstring, one for each comment, etc. The metadata associated with the chunks can also be used by <code>FlowControl</code> plugins to allow certain types of text to skip certain filters.</p> <p>Aside from filtering the text, the first filter in the pipeline is always responsible for initially reading the file from disk and getting the file content into a Unicode buffer that PySpelling can work with. It is also responsible for setting the default encoding and/or identifying the encoding from the file header if there is special logic to determine such things.</p> <p>The following <code>Filter</code> plugins are included:</p> Name Include\u00a0Path Context <code>pyspelling.filters.context</code> CPP <code>pyspelling.filters.cpp</code> HTML <code>pyspelling.filters.html</code> JavaScript <code>pyspelling.filters.javascript</code> Markdown <code>pyspelling.filters.markdown</code> ODF <code>pyspelling.filters.odf</code> OOXML <code>pyspelling.filters.ooxml</code> Python <code>pyspelling.filters.python</code> Stylesheets <code>pyspelling.filters.stylesheets</code> Text <code>pyspelling.filters.text</code> URL <code>pyspelling.filters.url</code> XML <code>pyspelling.filters.xml</code>"},{"location":"pipeline/#flow-control","title":"Flow Control","text":"<p><code>FlowControl</code> plugins are responsible for controlling the flow of the text down the pipeline. The category of a text chunk is passed to the plugin, and it will return one of three directives:</p> <ul> <li><code>ALLOW</code>: the chunk(s) of text is allowed to be evaluated by the next filter.</li> <li><code>SKIP</code>: the chunk(s) of text should skip the next filter.</li> <li><code>HALT</code>: halts the progress of the text chunk(s) down the pipeline and sends it directly to the spell checker.</li> </ul> <p>The following <code>FlowControl</code> plugins are included:</p> Name Include\u00a0Path Wildcard <code>pyspelling.flow_control.wildcard</code>"},{"location":"about/changelog/","title":"Changelog","text":""},{"location":"about/changelog/#210","title":"2.10","text":"<ul> <li>NEW: Allow specifying parallel processes to speed up spell checking. Number of jobs can be specified either     by command line or via the config. Command line overrides the config.</li> </ul>"},{"location":"about/changelog/#29","title":"2.9","text":"<ul> <li>NEW: Officially support Python 3.11 and 3.12.</li> <li>NEW: Drop support for Python 3.7.</li> </ul>"},{"location":"about/changelog/#282","title":"2.8.2","text":"<ul> <li>FIX: Ensure that Aspell actually uses the encoding passed to it for dictionaries.</li> <li>FIX: Use a disallow list for problematic or unsupported arguments to the underlying spell checker instead of     using a more restrictive allow list.</li> <li>FIX: Fix logic bug in JavaScript filter.</li> </ul>"},{"location":"about/changelog/#281","title":"2.8.1","text":"<ul> <li>FIX: Fix missing command line application after migration to new build system.</li> </ul>"},{"location":"about/changelog/#280","title":"2.8.0","text":"<ul> <li>NEW: Officially drop support for Python 3.6 and add support for Python 3.10.</li> <li>NEW: Switch build system to Hatchling.</li> <li>FIX: Fix typo in Aspell accepted parameters.</li> </ul>"},{"location":"about/changelog/#273","title":"2.7.3","text":"<ul> <li>FIX: Fix context reporting in the XML, HTML, and other filters derived from XML the filter.</li> </ul>"},{"location":"about/changelog/#272","title":"2.7.2","text":"<ul> <li>FIX: Add note in <code>--help</code> option about <code>--source</code> behavior.</li> <li>FIX: Better documentation on language options and Unicode normalization in international languages.</li> </ul>"},{"location":"about/changelog/#271","title":"2.7.1","text":"<ul> <li>FIX: Allow camel case options in Aspell.</li> </ul>"},{"location":"about/changelog/#27","title":"2.7","text":"<ul> <li>NEW: Check for <code>.pyspelling.yml</code> or <code>.pyspelling.yaml</code> by default.</li> <li>FIX: Fix documentation about how to specify languages in Aspell and how to specify languages when compiling     custom wordlists. In short, <code>d</code> should be used for specifying languages in general, but when using custom wordlists,     <code>lang</code> should be specified, and it should reference the <code>.dat</code> file name.</li> <li>FIX: Fix spelling in help output.</li> <li>FIX: Raise error in cases where pipeline options are not indented enough and parsed as an additional pipeline     name.</li> <li>FIX: Drop Python 3.5 support and officially support Python 3.9.</li> </ul>"},{"location":"about/changelog/#261","title":"2.6.1","text":"<ul> <li>FIX: Upgrade to <code>wcmatch</code> 6.0.3 which fixes issues dealing with dot files and globstar (<code>**</code>) when dot globbing     is not enabled. Also fixes a small logic error with symlink following and globstar. 6.0.3 is now the minimum     requirement.</li> </ul>"},{"location":"about/changelog/#26","title":"2.6","text":"<ul> <li>NEW: Add support for <code>wcmatch</code> version <code>6.0</code>.</li> <li>NEW: <code>wcmatch</code> version <code>6.0</code> adds a default pattern limit of <code>1000</code> to help protect against really large pattern     expansions such as <code>{1..1000000}</code>. If you wish to control this default, or disable it entirely, you can via the new     <code>glob_pattern_limit</code> configuration option.</li> </ul>"},{"location":"about/changelog/#251","title":"2.5.1","text":"<ul> <li>FIX: Add workaround for <code>wcmatch</code> version <code>5.0</code>.</li> </ul>"},{"location":"about/changelog/#25","title":"2.5","text":"<ul> <li>NEW: Add <code>expect_match</code> option to prevent a rule from failing if it finds no matching files.</li> <li>NEW: Formally support Python 3.8.</li> </ul>"},{"location":"about/changelog/#24","title":"2.4","text":"<ul> <li>NEW: Disable Aspell filters by default. Users must explicitly set the <code>mode</code> parameter under the <code>aspell</code> option     to enable default Aspell filters.</li> <li>New: Throw an exception with a message if no configuration is found or there is some other issue.</li> <li>New: Throw an exception with a message when no tasks are found in the matrix or when no tasks match a given name     or group.</li> <li>New: Throw an exception with a message when a task is run but no files are found.</li> </ul>"},{"location":"about/changelog/#231","title":"2.3.1","text":"<ul> <li>FIX: Properly handle docstring content and detection in files that have single line functions.</li> </ul>"},{"location":"about/changelog/#23","title":"2.3","text":"<ul> <li>NEW: Support new <code>wcmatch</code> glob feature flags and upgrade to <code>wcmatch</code> 4.0.</li> <li>FIX: Don't use recursion when parsing XML or HTML documents.</li> </ul>"},{"location":"about/changelog/#226","title":"2.2.6","text":"<ul> <li>FIX: Require <code>wcmatch</code> 3.0 for <code>glob</code> related fixes.</li> </ul>"},{"location":"about/changelog/#225","title":"2.2.5","text":"<ul> <li>FIX: Rework comment extraction in XML plugin.</li> <li>FIX: Newer versions of Soup Sieve will not compile an empty string, so adjust XML and HTML plugin logic to     account for this behavior.</li> </ul>"},{"location":"about/changelog/#224","title":"2.2.4","text":"<ul> <li>FIX: Explicitly require Beautiful Soup 4 dependency.</li> </ul>"},{"location":"about/changelog/#223","title":"2.2.3","text":"<ul> <li>FIX: There is no need to un-escape content for HTML/XML as it is already un-escaped in the <code>bs4</code> objects.</li> <li>FIX: Upgrade to latest beta of Soup Sieve.</li> </ul>"},{"location":"about/changelog/#222","title":"2.2.2","text":"<ul> <li> <p>FIX: Fix <code>:empty</code> and <code>:root</code> and <code>:nth-*</code> selectors not working properly without a tag name specified before.     This is now done via our external lib called <code>soupsieve</code> which is the same homegrown CSS library that we were using     internally.</p> </li> <li> <p>FIX: Potential infinite loop when using <code>:nth-child()</code>.</p> </li> </ul>"},{"location":"about/changelog/#221","title":"2.2.1","text":"<ul> <li>FIX: Comments in HTML/XML should be returned regardless of whether they are in an ignored tag or not.</li> </ul>"},{"location":"about/changelog/#22","title":"2.2","text":"<ul> <li>NEW: Add support for CSS4 selectors: <code>:empty</code>, <code>:first-child</code>, <code>:last-child</code>, <code>:only-child</code>, <code>:first-of-type</code>,     <code>:last-of-type</code>, <code>:only-of-type</code>, <code>:nth-child(an+b [of S]?)</code>, <code>:nth-last-child(an+b [of S]?)</code>, <code>:nth-of-type(an+b)</code>,     and <code>:nth-last-of-type(an+b)</code>. (#58)</li> </ul>"},{"location":"about/changelog/#211","title":"2.1.1","text":"<ul> <li>FIX: CSS4 allows <code>:not()</code>, <code>:has()</code>, and <code>:is()</code> to be nested in <code>:not()</code>. (#62)</li> </ul>"},{"location":"about/changelog/#21","title":"2.1","text":"<ul> <li>NEW: Add support for <code>div p</code>, <code>div&gt;p</code>, <code>div+p</code>, <code>div~p</code> in the HTML/XML filter's CSS selectors. (#51)</li> <li>NEW: Add support for the <code>:root</code> CSS selector. (#57)</li> <li>NEW: Add support for experimental <code>:has()</code> selector. (#54)</li> <li>FIX: According to CSS4 specification, <code>:is()</code> is the final name for <code>:matches()</code> but the <code>:matches()</code> is an     allowed alias. (#53)</li> <li>FIX: Allow <code>:not()</code> to be nested in <code>:is()</code>/<code>:matches()</code>. (#56)</li> </ul>"},{"location":"about/changelog/#20","title":"2.0","text":"<ul> <li>NEW: (Breaking change) Task names should be unique and using <code>--name</code> from the command line will only target one     <code>name</code> (the last task defined with that name). If you were not using <code>name</code> to run a group of tasks, you will not     notice any changes.</li> <li>NEW: Task option <code>group</code> has been added to target multiple tasks with the <code>--group</code> command line option. <code>group</code>     name can be shared across different tasks.</li> <li>NEW: Add XML filter (PySpelling now has a dependency on <code>lxml</code>).</li> <li>NEW: Add Open Document Format (ODF) filter for <code>.odt</code>, <code>.ods</code>, and <code>.odp</code> files.</li> <li>NEW: Add Office Open XML format (newer Microsoft document format) for <code>.docx</code>, <code>.xlsx</code>, and <code>.pptx</code> files.</li> <li>NEW: CSS selectors in XML and HTML filters now support <code>:not()</code> and <code>:matches()</code> pseudo class.</li> <li>NEW: CSS selectors now support <code>,</code> in patterns.</li> <li>NEW: CSS selectors now support <code>i</code> in attribute selectors: <code>[attr=value i]</code>.</li> <li>NEW: CSS selectors now support namespaces (some configuration required).</li> <li>NEW: For better HTML context, display a tag's ancestry (just tag name of parents).</li> <li>NEW: Captured tags are now configurable via <code>captures</code>, but tags that are not captured still have their children     crawled unless they are under <code>ignores</code>.</li> <li>NEW: Support modes added for HTML filter: <code>html</code>, <code>html5</code>, and <code>xhtml</code>.</li> <li>NEW: <code>CHECK_BOM</code> plugin attribute has been deprecated in favor of overriding the exposed <code>has_bom</code> function.</li> <li>NEW: Tasks can be hidden with the <code>hidden</code> configuration option. Tasks with <code>hidden</code> enabled will only run if     they are explicitly called by name.</li> <li>NEW: Add normal string support to Python filter.</li> <li>NEW: Add string and template literal support for JavaScript filter.</li> <li>NEW: Add string support for CPP filter.</li> <li>NEW: Add <code>generic_mode</code> option to CPP to allow for generic C/C++ comment style capture from non C/C++ file     types.</li> <li>NEW: Context will normalize line endings before applying context (can be disabled).</li> <li>NEW: CPP, Stylesheet, and JavaScript plugins now normalize line endings of block comments.</li> <li>NEW: UTF-16 and UTF-32 is not really supported by Aspell and Hunspell, so at the end of the pipeline, Unicode     strings that have the associated encoding of UTF-16 or UTF-32 will encoding with the compatible UTF-8. This does not     apply to files being processed with a disabled pipeline. When the pipeline is disabled, files are sent directly to     the spell checker with no modifications.</li> <li>FIX: Case related issues when comparing tags and attributes in HTML.</li> <li>FIX: CSS selectors should only compare case insensitive for ASCII characters A-Z and a-z.</li> <li>FIX: Allow CSS escapes in selectors.</li> <li>FIX: Don't send empty (or strings that are just whitespace) to spell checker to prevent Aspell 0.50 series from     crashing (also to increase performance).</li> <li>FIX: Catch and bubble up errors better.</li> <li>FIX: Fix issue where Python module docstrings would not get spell checked if they followed a shebang.</li> </ul>"},{"location":"about/changelog/#11","title":"1.1","text":"<ul> <li>NEW: Add URL/email address filter. (#30)</li> <li>NEW: If <code>pipeline</code> configuration key is set to <code>null</code>, do not use any filters, and send the filename, not the     content, to the spell checker.</li> <li>NEW: Add <code>encoding</code> option to <code>dictionary</code> configuration for the purpose of communicating what encoding the main     dictionary is when compiling wordlists (only Aspell takes advantage of this).</li> <li>FIX: Fix Hunspell <code>-O</code> option which was mistakenly <code>-o</code>. (#31)</li> </ul>"},{"location":"about/changelog/#10","title":"1.0","text":"<ul> <li>NEW: Allow multiple names on command line via: <code>pyspelling -n name1 -n name2</code>.</li> <li>FIX: Fix empty HTML tags not properly having their attributes evaluated.</li> <li>FIX: Fix case where a deprecation warning for <code>filters</code> is shown when it shouldn't.</li> <li>FIX: Better docstring recognition in Python filter.</li> <li>FIX: Catch comments outside of the <code>&lt;HTML&gt;</code> tag.</li> <li>FIX: Filter out <code>Doctype</code>, <code>CData</code>, and other XML or non-content type information.</li> </ul>"},{"location":"about/changelog/#10b2","title":"1.0b2","text":"<ul> <li>FIX: Fix CPP comment regular expression.</li> </ul>"},{"location":"about/changelog/#10b1","title":"1.0b1","text":"<ul> <li>NEW: Better context for HTML elements. HTML is now returned by block level elements, and the elements selector     is given as context. Attributes also return a selector as context and are returned individually. HTML comments are     returned as individual hunks.</li> <li>NEW: Add Stylesheet and CPP filters (#17)</li> <li>NEW: JavaScript is now derived from CPP.</li> <li>NEW: PySpelling looks for <code>.spelling.yml</code> or <code>.pyspelling.yml</code> with a priority for the latter. (#12)</li> <li>NEW: Spelling pipeline adjustments: you can now explicitly allow only certain categories, skip categories, or     halt them in the pipeline. Pipeline flow control is now done via a new <code>FlowControl</code> plugin. When avoiding,     including, or skipping categories, they are now done with wildcard patterns. (#16)</li> <li>NEW: Drop scanning python normal strings in plugin.</li> <li>NEW: Use <code>get_plugin</code> instead of <code>get_filter</code>, but allow a backwards compatible path for now.</li> <li>NEW: In configuration, <code>documents</code> is now <code>matrix</code> and <code>filters</code> is now <code>pipeline</code>, but a deprecation path has     been added. (#15)</li> <li>NEW: Provide a class attribute that will cause a Filter object to avoid BOM detection if it is not appropriate     for the given file.</li> <li>NEW: Wordlists should get the desired language/dictionary from the spell checker specific options.</li> <li>NEW: Add global configuration option to specify the preferred spell checker, but it is still overridable via     command line.</li> <li>FIX: Internal cleanup in regards to error handling and debug.</li> <li>FIX: Fix context issue when no escapes are defined.</li> </ul>"},{"location":"about/changelog/#02a4","title":"0.2a4","text":"<ul> <li>NEW: Text filter can handle Unicode normalization and converting to other encodings.</li> <li>NEW: Default encoding is now <code>utf-8</code> for all filters.</li> <li>FIX: Internal encoding handling.</li> </ul>"},{"location":"about/changelog/#02a3","title":"0.2a3","text":"<ul> <li>FIX: Text filter was returning old Parser name instead of new Filter name.</li> </ul>"},{"location":"about/changelog/#02a2","title":"0.2a2","text":"<ul> <li>NEW: Incorporate the Decoder class into the filter class.</li> <li>NEW: Add Hunspell support.</li> <li>NEW: Drop specifying spell checker in configuration file. It must be set from command line.</li> <li>FIX: Add missing documentation about Context filter.</li> </ul>"},{"location":"about/changelog/#02a1","title":"0.2a1","text":"<ul> <li>NEW: Better filters (combine filters and parsers into just filters).</li> <li>NEW: Drop Python 2 support.</li> <li>NEW: Better Python encoding detection.</li> <li>NEW: Better HTML encoding detection.</li> <li>NEW: Drop <code>file_extensions</code> option and <code>parser</code> option.</li> <li>NEW: Filters no longer define file extensions. Sources must specify a wildcard path that matches desired files.</li> <li>NEW: Drop regular expression support for sources.</li> <li>NEW: Drop raw filter.</li> </ul>"},{"location":"about/changelog/#01a3","title":"0.1a3","text":"<ul> <li>NEW: Add JavaScript parser.</li> </ul>"},{"location":"about/changelog/#01a2","title":"0.1a2","text":"<ul> <li>NEW: Add option to group consecutive Python comments.</li> <li>FIX: Properly return error.</li> <li>FIX: Only retry with default encoding if exception thrown was a <code>UnicodeDecodeError</code>.</li> </ul>"},{"location":"about/changelog/#01a1","title":"0.1a1","text":"<ul> <li>NEW: Initial alpha release.</li> </ul>"},{"location":"about/contributing/","title":"Contributing &amp; Support","text":""},{"location":"about/contributing/#become-a-sponsor","title":"Become a Sponsor","text":"<p>Open source projects take time and money. Help support the project by becoming a sponsor. You can add your support at any tier you feel comfortable with. No amount is too little. We also accept one time contributions via PayPal.</p> <p> GitHub Sponsors  PayPal</p>"},{"location":"about/contributing/#bug-reports","title":"Bug Reports","text":"<ol> <li> <p>Please read the documentation and search the issue tracker to try and find the answer to your question     before posting an issue.</p> </li> <li> <p>When creating an issue on the repository, please provide as much information as possible:</p> <ul> <li>Version being used.</li> <li>Operating system.</li> <li>Version of Python.</li> <li>Errors in console.</li> <li>Detailed description of the problem.</li> <li>Examples for reproducing the error.  You can post pictures, but if specific text or code is required to     reproduce the issue, please provide the text in a plain text format for easy copy/paste.</li> </ul> <p>The more info provided the greater the chance someone will take the time to answer, implement, or fix the issue.</p> </li> <li> <p>Be prepared to answer questions and provide additional information if required.  Issues in which the creator refuses     to respond to follow up questions will be marked as stale and closed.</p> </li> </ol>"},{"location":"about/contributing/#reviewing-code","title":"Reviewing Code","text":"<p>Take part in reviewing pull requests and/or reviewing direct commits.  Make suggestions to improve the code and discuss solutions to overcome weakness in the algorithm.</p>"},{"location":"about/contributing/#answer-questions-in-issues","title":"Answer Questions in Issues","text":"<p>Take time and answer questions and offer suggestions to people who've created issues in the issue tracker. Often people will have questions that you might have an answer for.  Or maybe you know how to help them accomplish a specific task they are asking about. Feel free to share your experience to help others.</p>"},{"location":"about/contributing/#pull-requests","title":"Pull Requests","text":"<p>Pull requests are welcome, and a great way to help fix bugs and add new features. If you are interested in directly contributing to the code, please check out Development for more information on the environment and processes.</p>"},{"location":"about/contributing/#documentation-improvements","title":"Documentation Improvements","text":"<p>A ton of time has been spent not only creating and supporting this tool and related extensions, but also spent making this documentation.  If you feel it is still lacking, show your appreciation for the tool by helping to improve the documentation. Check out Development for more info on documentation.</p>"},{"location":"about/development/","title":"Development","text":""},{"location":"about/development/#project-layout","title":"Project Layout","text":"<p>There are a number of files for build, test, and continuous integration in the root of the project, but in general, the project is broken up like so.</p> <pre><code>\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 src\n\u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 dictionary\n\u2502\u00a0\u00a0  \u00a0\u00a0 \u2514\u2500\u2500 markdown\n\u251c\u2500\u2500 pyspelling\n\u2514\u2500\u2500 requirements\n</code></pre> Directory Description <code>docs/src/dictionary</code> This contains the spell check wordlist(s) for the project. <code>docs/src/markdown</code> This contains the content for the documentation. <code>pyspelling</code> This contains the source code for the project. <code>requirements</code> This contains files with lists of dependencies required dependencies for continuous integration."},{"location":"about/development/#coding-standards","title":"Coding Standards","text":"<p>When writing code, the code should roughly conform to PEP8 and PEP257 suggestions along with some other requirements. The project utilizes the astral-sh/ruff linter that helps to ensure code conforms (give or take some of the rules). When in doubt, follow the formatting hints of existing code when adding files or modifying existing files.</p> <p>Usually this can be automated with Tox (assuming it is installed): <code>tox -e lint</code>.</p>"},{"location":"about/development/#building-and-editing-documents","title":"Building and Editing Documents","text":"<p>Documents are in Markdown (with with some additional syntax provided by extensions) and are converted to HTML via Python Markdown. If you would like to build and preview the documentation, you must have these packages installed:</p> <ul> <li>Python-Markdown/markdown: the Markdown parser.</li> <li>mkdocs/mkdocs: the document site generator.</li> <li>squidfunk/mkdocs-material: a material theme for MkDocs.</li> <li>facelessuser/pymdown-extensions: this Python Markdown extension bundle.</li> </ul> <p>In order to build and preview the documents, just run the command below from the root of the project and you should be able to view the documents at <code>localhost:8000</code> in your browser. After that, you should be able to update the documents and have your browser preview update live.</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"about/development/#spell-checking-documents","title":"Spell Checking Documents","text":"<p>During validation we build the docs and spell check various files in the project. This project is used to spell check itself, but Aspell must be installed.  Currently this project uses one of the more recent versions of Aspell. Since the latest Aspell is not available on Windows, and this has not been tested with older versions, it is not expected that everyone will install and run Aspell locally, but it will be run in CI tests for pull requests.</p> <p>In order to perform the spell check, it is expected you are setup to build the documents, and that you have Aspell installed in your system path (if needed you can use the <code>--binary</code> option to point to the location of your Aspell binary). It is also expected that you have the <code>en</code> dictionary installed as well. To initiate the spell check, run the following command from the root of the project.</p> <p>You will need to make sure the documents are built first:</p> <pre><code>mkdocs build --clean\n</code></pre> <p>And then run the spell checker. Using <code>python -m</code> from the project root will load your checked out version of PySpelling instead of your system installed version:</p> <pre><code>python -m pyspelling\n</code></pre> <p>It should print out the files with the misspelled words if any are found.  If you find it prints words that are not misspelled, you can add them in <code>docs/src/dictionary/en-custom.text</code>.</p>"},{"location":"about/development/#validation-tests","title":"Validation Tests","text":"<p>In order to preserve good code health, a test suite has been put together with pytest (pytest-dev/pytest). There are currently two kinds of tests: syntax and targeted.  To run these tests, you can use the following command:</p> <pre><code>python run_tests.py\n</code></pre>"},{"location":"about/development/#running-validation-with-tox","title":"Running Validation With Tox","text":"<p>Tox (tox-dev/tox) is a great way to run the validation tests, spelling checks, and linting in virtual environments so as not to mess with your current working environment. Tox will use the specified Python version for the given environment and create a virtual environment and install all the needed requirements (minus Aspell).  You could also setup your own virtual environments with the Virtualenv module without Tox, and manually do the same.</p> <p>First, you need to have Tox installed:</p> <pre><code>pip install tox\n</code></pre> <p>By running Tox, it will walk through all the environments and create them (assuming you have all the python versions on your machine) and run the related tests.  See <code>tox.ini</code> to learn more.</p> <pre><code>tox\n</code></pre> <p>If you don't have all the Python versions needed to test all the environments, those entries will fail.  You can ignore those.  Spelling will also fail if you don't have the correct version of Aspell.</p> <p>As most people will not have all the Python versions on their machine, it makes more sense to target specific environments. To target a specific environment to test, you use the <code>-e</code> option to select the environment of interest. To select lint:</p> <pre><code>tox -elint\n</code></pre> <p>To select Python 3.7 unit tests (or other versions \u2013 change accordingly):</p> <pre><code>tox -epy37\n</code></pre> <p>To select spelling and document building:</p> <pre><code>tox -edocuments\n</code></pre>"},{"location":"about/development/#code-coverage","title":"Code Coverage","text":"<p>When running the validation tests through Tox, it is setup to track code coverage via the Coverage (ned/coveragepy) module.  Coverage is run on each <code>pyxx</code> environment.  If you've made changes to the code, you can clear the old coverage data:</p> <pre><code>coverage erase\n</code></pre> <p>Then run each unit test environment to and coverage will be calculated. All the data from each run is merged together. HTML is output for each file in <code>.tox/pyXX/tmp</code>.  You can use these to see areas that are not covered/exercised yet with testing.</p> <p>You can checkout <code>tox.ini</code> to see how this is accomplished.</p>"},{"location":"about/license/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2017 - 2024 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"filters/context/","title":"Context","text":""},{"location":"filters/context/#usage","title":"Usage","text":"<p>The Context filter is used to create regular expression context delimiters for filtering out content you want from content you don't want. It takes a text buffer in and will return one or more text buffer with undesirable content filtered out.</p> <p>Depending on how the filter is configured, the opening delimiter will swap from ignoring text to gathering text. When the closing delimiter is met, the filter will swap back from gathering text to ignoring text. If <code>context_visible_first</code> is set to <code>true</code>, the logic will be reversed.</p> <p>Regular expressions are compiled with the MULTILINE flag so that <code>^</code> represents the start of a line and <code>$</code> represents the end of a line. <code>\\A</code> and <code>\\Z</code> would represent the start and end of the buffer.</p> <p>Delimiters require an <code>open</code> and a <code>close</code> pattern. You can optionally define a <code>content</code> pattern, but if you don't, it is <code>.*?</code> by default. The three patterns are used to create a single regular expression in the form <code>r'{0}(?P&lt;special_group_name&gt;{1})(?:{2}|\\Z)'</code>, where <code>{0}</code> is the opening pattern is inserted, <code>{1}</code> is the content pattern , and <code>{2}</code> is the closing pattern. <code>special_group_name</code> is randomly generated to ensure it doesn't conflict with anything in the pattern. Keeping in mind that these are all compiled into one pattern, you are able to define capture group names in opening, and reference them in the closing if needed.</p> <p>You can also able to define a global escape pattern to prevent escaped delimiters from being captured.</p> <p>The filter can be included via <code>pyspelling.filters.context</code>.</p> <pre><code>matrix:\n- name: python\n  sources:\n  - pyspelling\n  pipeline:\n  - pyspelling.filters.python:\n      comments: false\n  - pyspelling.filters.context:\n      context_visible_first: true\n      escapes: '\\\\[\\\\`~]'\n      delimiters:\n      # Ignore multiline content between fences (fences can have 3 or more back ticks)\n      # ```\n      # content\n      # ```\n      - open: '(?s)^(?P&lt;open&gt; *`{3,})$'\n        close: '^(?P=open)$'\n      # Ignore text between inline back ticks\n      - open: '(?P&lt;open&gt;`+)'\n        close: '(?P=open)'\n</code></pre>"},{"location":"filters/context/#options","title":"Options","text":"Options Type Default Description <code>escapes</code> string <code>''</code> Regular expression pattern for character escapes outside delimiters. <code>context_visible_first</code> bool <code>False</code> Context will start as invisible and will be invisible between delimiters. <code>delimiters</code> [dict] <code>[]</code> A list of dicts that define the regular expression delimiters."},{"location":"filters/context/#categories","title":"Categories","text":"<p>Context returns text with the following categories.</p> Category Description <code>context</code> Text captured by the via context."},{"location":"filters/cpp/","title":"CPP","text":""},{"location":"filters/cpp/#usage","title":"Usage","text":"<p>The CPP filter is designed to find and return C/C++ style comments and strings. It accepts a text buffer and will return one or more text buffers containing content from comments and/or strings.</p> <p>When first in the chain, the CPP filter uses no special encoding detection. It will assume <code>utf-8</code> if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text: block, inline, or string (if enabled).</p> <p>When the <code>strings</code> option is enabled, content will be extracted from strings (not character constants). Support is available for all the modern C++ strings shown below. CPP will also handle decoding string escapes as well, but as string character width and encoding can be dependent on implementation and configuration, some additional setup may be required via option. Strings will be returned with the specified encoding, even if it differs from the file's encoding (this is the associated encoding specified in the <code>SourceText</code> object, the content itself is still in Unicode).</p> <pre><code>    auto s0 =    \"Narrow character string\";                   // char\n    auto s1 =   L\"Wide character string\";                     // wchar_t\n    auto s2 =  u8\"UTF-8 strings\";                             // char\n    auto s3 =   u\"UTF-16 strings\";                            // char16_t\n    auto s4 =   U\"UTF-32 strings\";                            // char32_t\n    auto R0 =   R\"(\"Raw strings\")\";                           // const char*\n    auto R1 =   R\"delim(\"Raw strings with delimiters\")delim\"; // const char*\n    auto R3 =  LR\"(\"Raw wide character strings\")\";            // const wchar_t*\n    auto R4 = u8R\"(\"Raw UTF-8 strings\")\";                     // const char*, encoded as UTF-8\n    auto R5 =  uR\"(\"Raw UTF-16 strings\")\";                    // const char16_t*, encoded as UTF-16\n    auto R6 =  UR\"(\"Raw UTF-32 strings\")\";                    // const char32_t*, encoded as UTF-32\n</code></pre> <p>As C++ style comments are fairly common convention in other languages, this filter can often be used for other languages as well using <code>generic_mode</code>. In Generic Mode, many C/C++ specific considerations and options will be disabled. See Generic Mode for more information.</p> <pre><code>matrix:\n- name: cpp\n  pipeline:\n  - pyspelling.filters.cpp\n      line_comments: false\n  sources:\n  - js_files/**/*.{cpp,hpp,c,h}\n</code></pre>"},{"location":"filters/cpp/#filtering-string-types","title":"Filtering String types","text":"<p>When <code>strings</code> is enabled, you can specify which strings you want to allow via the <code>string_types</code> option. Valid string types are <code>S</code> for standard, <code>L</code> for long/wide, <code>U</code> for Unicode (all variants), and <code>R</code> for raw.  Case is not important, and the default value is <code>sul</code>.  </p> <p>If specifying <code>R</code>, you must also specify either <code>U</code>, <code>L</code>, or <code>S</code> as raw strings are also either <code>S</code>, <code>L</code>, or <code>S</code> strings. Selecting <code>UR</code> will select both Unicode strings and Unicode raw strings. If you need to target just raw strings, you can use <code>R*</code> which will target all raw string types: raw Unicode, raw wide, and raw standard. You can use <code>*</code> for other types as well. You can also just specify <code>*</code> by itself to target all string types.</p>"},{"location":"filters/cpp/#generic-mode","title":"Generic Mode","text":"<p>C/C++ style comments are not exclusive to C/C++. Many different file types have adopted similar style comments. The CPP filter has a generic mode which allows for a C/C++ style comment extraction without all the C/C++ specific considerations. Simply enable <code>generic_mode</code> via the options.</p> <p>Generic Mode disables the C/C++ specific nuance of allowing multiline comments via escaping newlines. This is a very C/C++ specific thing that is rarely carried over by others that have adopted C/C++ style comments:</p> <pre><code>// Generic mode will \\\n   not allow this.\n</code></pre> <p>Generic Mode will not decode any character escapes in strings when enabled. C/C++ has very specific rules for handling string escapes, only a handful of which may translate to other languages. Generic Mode is mainly meant for comments and not strings, but will return content of single quoted and double quoted strings if <code>strings</code> is enabled. All related escape decoding options do not apply to Generic Mode.</p> <p>Trigraphs are very C/C++ specific, and will never be evaluated in Generic Mode.</p> <p>Lastly, when using this filter in Generic Mode, you can also adjust the category prefix from <code>cpp</code> to whatever you would like via the <code>prefix</code> option.</p>"},{"location":"filters/cpp/#options","title":"Options","text":"Options Type Default Description <code>block_comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each block comment. <code>line_comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each line comment. <code>strings</code> bool <code>False</code> Return <code>SourceText</code> entries for each string. <code>group_comments</code> bool <code>False</code> Group consecutive inline comments as one <code>SourceText</code> entry. <code>trigraphs</code> bool <code>False</code> Account for trigraphs in C/C++ code. Trigraphs are never evaluated in Generic Mode. <code>generic_mode</code> bool <code>False</code> Parses files with a generic C++ like mode for parsing C++ style comments from non C++ files. See Generic Mode for more info. <code>decode_escapes</code> bool <code>True</code> Enable/disable string escape decoding. Strings are never decoded in Generic Mode. <code>charset_size</code> int <code>1</code> Set normal string character byte width. <code>exec_charset</code> string <code>'utf-8</code> Set normal string encoding. <code>wide_charset_size</code> int <code>4</code> Set wide string character byte width. <code>wide_exec_charset</code> string <code>'utf-32</code> Set wide string encoding. <code>string_types</code> string <code>\"sul\"</code> Set the allowed string types to capture: standard strings (<code>s</code>),  wide (<code>l</code>), Unicode (<code>u</code>), and raw (<code>r</code>). <code>*</code> captures all strings, or when used with a type, captures all variants of that type <code>r*</code>. <code>prefix</code> string <code>'cpp'</code> Change the category prefix."},{"location":"filters/cpp/#categories","title":"Categories","text":"<p>CPP returns text with the following categories. <code>cpp</code> prefix can be changed via the <code>prefix</code> option.</p> Category Description <code>cpp-block-comment</code> Text captured from C++ style block comments. <code>cpp-line-comment</code> Text captured from C++ style line comments. <code>cpp-string</code> Text captured from strings."},{"location":"filters/html/","title":"HTML","text":""},{"location":"filters/html/#usage","title":"Usage","text":"<p>The HTML filter is designed to capture HTML content, comments, and even attributes. It allows for filtering out specific  tags, and you can even filter them out with basic selectors.</p> <p>The filter accepts an HTML content buffer and will return one or more buffers containing just the text from HTML attributes and/or tags. The content will no longer be considered HTML.</p> <p>When first in the chain, the HTML filter will look for the encoding of the HTML in its header and convert the buffer to Unicode. It will assume <code>utf-8</code> if no encoding header is found, and the user has not overridden the fallback encoding.</p> <p>The HTML filter uses BeautifulSoup4 to convert the Unicode content to HTML. Content is returned in individual chunks by block tags. While this causes more overhead, as each block is processed individually through the command line tool, it provides context for where the spelling errors are. If enabled, the HTML filter will also return chunks for comments and even attributes. Each type of text chunk is returned with their own category type.</p> <p>Tags can be captured or ignored with the <code>captures</code> and <code>ignores</code> options. These options work by employing CSS selectors to target the tags. The CSS selectors are based on a limited subset of CSS4 selectors.</p> <pre><code>matrix:\n- name: html\n  pipeline:\n  - pyspelling.filters.html:\n      comments: false\n      attributes:\n      - title\n      - alt\n      ignores:\n      - :matches(code, pre)\n      - a:matches(.magiclink-compare, .magiclink-commit)\n      - span.keys\n      - :matches(.MathJax_Preview, .md-nav__link, .md-footer-custom-text, .md-source__repository, .headerlink, .md-icon)\n  sources:\n  - site/*.html\n</code></pre>"},{"location":"filters/html/#supported-css-selectors","title":"Supported CSS Selectors","text":"<p>The CSS selectors are based on a limited subset of CSS4 selectors. Support is provided via Soup Sieve. Please reference Soup Sieve's documentation for more info.</p>"},{"location":"filters/html/#options","title":"Options","text":"Options Type Default Description <code>comments</code> bool <code>True</code> Include comment text in the output. <code>attributes</code> [string] <code>[]</code> Attributes whose content should be included in the output. <code>ignores</code> [string] <code>[]</code> CSS style selectors that identify tags to ignore. Child tags will not be crawled. <code>captures</code> [string] <code>['*|*:not(script,style)']</code> CSS style selectors used to narrow which tags that text is collected from. Unlike <code>ignores</code>, tags which text is not captured from still have their children crawled. <code>mode</code> string <code>'html</code> Mode to use when parsing HTML: <code>html</code>, <code>xhtml</code>, <code>html5</code>. <code>namespaces</code> dict <code>{}</code> Dictionary containing key value pairs of namespaces to use for CSS selectors (equivalent to <code>@namespace</code> in CSS). Use the an empty string for the key to define default the namespace. See below for example. <code>break_tags</code> [string] <code>[]</code> Additional tags (in addition to the default, defined block tags), to break on for context. Useful for new or currently unsupported block tags. <p>Namespace example</p> <pre><code>matrix:\n- name: html\n  pipeline:\n  - pyspelling.filters.html:\n      mode: xhtml\n      namespaces:\n        \"\": http://www.w3.org/1999/xhtml\n        svg: http://www.w3.org/2000/svg\n        xlink: http://www.w3.org/1999/xlink\n</code></pre>"},{"location":"filters/html/#categories","title":"Categories","text":"<p>HTML returns text with the following categories.</p> Category Description <code>html-content</code> Text captured from HTML blocks. <code>html-attribute</code> Text captured from HTML attributes. <code>html-comment</code> Text captured from HTML comments."},{"location":"filters/javascript/","title":"JavaScript","text":""},{"location":"filters/javascript/#usage","title":"Usage","text":"<p>The JavaScript filter is designed to find and return only content from comments and/or strings. It takes a JavaScript buffer and returns one or more buffers containing the content of the comments and/or strings.</p> <p>When first in the chain, the JavaScript filter uses no special encoding detection. It will assume <code>utf-8</code> if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text.  The filter can return JSDoc comments, block comment, inline comment, string, and template literal content.</p> <pre><code>matrix:\n- name: javascript\n  pipeline:\n  - pyspelling.filters.javascript:\n      jsdocs: true\n      line_comments: false\n      block_comments: false\n  sources:\n  - js_files/**/*.js\n</code></pre>"},{"location":"filters/javascript/#options","title":"Options","text":"Options Type Default Description <code>block_comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each block comment. <code>line_comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each line comment. <code>jsdocs</code> bool <code>False</code> Return <code>SourceText</code> entries for each JSDoc comment. <code>strings</code> bool <code>False</code> Return <code>SourceText</code> entries for each string. <code>group_comments</code> bool <code>False</code> Group consecutive inline JavaScript comments as one <code>SourceText</code> entry. <code>decode_escapes</code> bool <code>True</code> Enable/disable decoding of string escapes."},{"location":"filters/javascript/#categories","title":"Categories","text":"<p>JavaScript returns text with the following categories.</p> Category Description <code>js-block-comment</code> Text captured from JavaScript block comments. <code>js-line-comment</code> Text captured from JavaScript line comments. <code>js-docs</code> Text captured from JSDoc comments. <code>js-string</code> Text captured from strings and template literals."},{"location":"filters/markdown/","title":"Markdown","text":""},{"location":"filters/markdown/#usage","title":"Usage","text":"<p>The Markdown filter converts a text file's buffer using Python Markdown and returns a single <code>SourceText</code> object containing the text as HTML. It can be included via <code>pyspelling.filters.markdown</code>. When first in the chain, the file's default, assumed encoding is <code>utf-8</code> unless otherwise overridden by the user.</p> <p>Tip</p> <p>The Markdown filter is not always needed. While Aspell has a built-in Markdown mode, it can be somewhat limited in ignoring content for advanced cases, but if all you need is basic Markdown support, then you can often just use Aspell's Markdown mode.</p> <pre><code>- name: markdown\n  group: docs\n  sources:\n  - README.md\n  - INSTALL.md\n  - LICENSE.md\n  - CODE_OF_CONDUCT.md\n  aspell:\n    lang: en\n    d: en_US\n    mode: markdown\n  dictionary:\n    wordlists:\n    - .spell-dict\n    output: build/dictionary/markdown.dic\n</code></pre> <p>PySpelling's Markdown filter is useful if you:</p> <ul> <li>Already use Python Markdown and it's custom extensions and need support for the custom extensions.</li> <li>Need to convert the content to HTML to use PySpelling's advanced HTML filter to ignore content with CSS selectors.</li> </ul> <p>Python Markdown is not a CommonMark parser either, so if you need such a parser, you may have find and/or write your own.</p> <p>To configure the Python Markdown filter, you can include it in the pipeline and setup various Markdown extensions if desired.</p> <pre><code>matrix:\n- name: markdown\n  pipeline:\n  - pyspelling.filters.markdown:\n      markdown_extensions:\n      - markdown.extensions.toc:\n          slugify: !!python/name:pymdownx.slugs.uslugify\n          permalink: \"\\ue157\"\n      - markdown.extensions.admonition\n      - markdown.extensions.smarty\n  source:\n  - **/*.md\n</code></pre>"},{"location":"filters/markdown/#options","title":"Options","text":"Options Type Default Description <code>markdown_extensions</code> [string/dict] <code>[]</code> A list of strings defining markdown extensions to use. You can substitute the string with a dict that defines the extension as the key and the value as a dictionary of options."},{"location":"filters/markdown/#categories","title":"Categories","text":"<p>Markdown returns text with the following categories.</p> Category Description <code>markdown</code> Text rendered in HTML."},{"location":"filters/odf/","title":"ODF","text":""},{"location":"filters/odf/#usage","title":"Usage","text":"<p>The ODF filter provides support for the Open Document Format. It supports documents (<code>odt</code>), spreadsheets (<code>ods</code>), and presentations (<code>odp</code>). It also supports their flat format as well: <code>fodt</code>, <code>fods</code>, and <code>fodp</code>. In general, it will return one chunk containing all the checkable strings in the file. In the case of presentations, it will actually send multiple chunks, one for each slide.</p> <p>Under the hood, content is parsed via the XML filter.</p> <pre><code>- name: odf\n  sources:\n  - '**/*.{odt,fodt,ods,odp}'\n  pipeline:\n  - pyspelling.filters.odf:\n</code></pre>"},{"location":"filters/odf/#options","title":"Options","text":"<p>There are currently no additional options when using the ODF filter.</p>"},{"location":"filters/odf/#categories","title":"Categories","text":"<p>HTML returns text with the following categories.</p> Category Description <code>odt-content</code> Text captured from document files. <code>odp-content</code> Text captured from presentation files. <code>ods-content</code> Text captured from spreadsheet files."},{"location":"filters/ooxml/","title":"OOXML","text":""},{"location":"filters/ooxml/#usage","title":"Usage","text":"<p>The OOXML filter provides support for the Office Open XML format (latest format for Microsoft Office files). It supports documents (<code>docx</code>), spreadsheets (<code>xlsx</code>), and presentations (<code>pptx</code>). In general, it will return one chunk containing all the checkable strings in the file. In the case of presentations, it will actually send multiple chunks, one for each slide. Documents may return additional chunks for headers, footers, etc.</p> <p>Under the hood, content is parsed via the XML filter.</p> <pre><code>- name: ooxml\n  sources:\n  - '**/*.{docx,pptx,xlsx}'\n  pipeline:\n  - pyspelling.filters.ooxml:\n</code></pre>"},{"location":"filters/ooxml/#options","title":"Options","text":"<p>There are currently no additional options when using the OOXML filter.</p>"},{"location":"filters/ooxml/#categories","title":"Categories","text":"<p>HTML returns text with the following categories.</p> Category Description <code>docx-content</code> Text captured from document files. <code>pptx-content</code> Text captured from presentation files. <code>xlsx-content</code> Text captured from spreadsheet files."},{"location":"filters/python/","title":"Python","text":""},{"location":"filters/python/#usage","title":"Usage","text":"<p>The Python filter is designed to find and return only content from comments and/or strings. It takes a Python buffer and returns one or more buffers containing the content of the comments and/or strings.</p> <p>When first in the chain, the Python filter will look for the encoding of the file in the header, and convert to Unicode accordingly. It will assume <code>utf-8</code> if no encoding header is found, and the user has not overridden the fallback encoding.</p> <p>Text is returned in chunks based on the context of the captured text. Each docstring, inline comment, or normal string is returned as their own chunk.</p> <p>In general, regardless of Python version, strings should be parsed almost identically. PySpelling, unless configured otherwise, will decode string escapes and strip out format variables from f-strings. Decoding of escapes and stripping format variables is not dependent on what version of Python PySpelling is run on, but is based on the string prefixes that PySpelling encounters. There are two cases that may cause quirks related to Python version:</p> <ol> <li> <p>PySpelling doesn't support being run from Python 2, but it will still find strings and comments in Python 2 code as     many Python 3 projects support Python 2 as well. If you run this on Python 2 code that is not using     <code>from __future__ import unicode_literals</code>, it will still treat the default strings in Python 2 code as Unicode     as it has no way of knowing that a file is specifically meant for Python 2 parsing only. In general, you should use     <code>unicode_literals</code> if you are supporting both Python 2 and 3.</p> </li> <li> <p>Use of <code>\\N{NAMED UNICODE}</code> might produce different results if one Python version defines a specific Unicode name     while another does not. I'm not sure how greatly the named Unicode database varies from Python version to Python     version, but if this is experienced, and is problematic, you can always disable <code>decode_escapes</code> in the options for     a more consistent behavior.</p> </li> </ol> <pre><code>matrix:\n- name: python\n  pipeline:\n  - pyspelling.filters.python:\n      strings: false\n      comments: false\n  sources:\n  - pyspelling/**/*.py\n</code></pre>"},{"location":"filters/python/#filtering-string-types","title":"Filtering String types","text":"<p>When <code>strings</code> is enabled, you can specify which strings you want to allow via the <code>string_types</code> option. Valid string types are <code>b</code> for bytes, <code>f</code> for format, <code>u</code> for Unicode, and <code>r</code> for raw.  <code>f</code> refers to f-strings, not strings in the form <code>\"my string {}\".format(value)\"</code>, and though f-strings are Unicode, they are treated as a separate string type from Unicode. Case is not important, and the default value is <code>fu</code>.</p> <p>If specifying <code>r</code>, you must also specify either <code>u</code>, <code>b</code>, or <code>f</code> as raw strings are also either <code>u</code>, <code>b</code>, or <code>f</code> strings. Selecting <code>ur</code> will select both Unicode strings and Unicode raw strings. If you need to target just raw strings, you can use <code>r*</code> which will target all raw strings types: raw Unicode, raw format, and raw bytes. You can use <code>*</code> for other types as well. You can also just specify <code>*</code> by itself to target all string types.</p>"},{"location":"filters/python/#options","title":"Options","text":"Options Type Default Description <code>comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each comment. <code>docstrings</code> bool <code>True</code> Return <code>SourceText</code> entries for each docstrings. <code>group_comments</code> bool <code>False</code> Group consecutive Python comments as one <code>SourceText</code> entry. <code>decode_escapes</code> bool <code>True</code> Decode escapes and strip out format variables. Behavior is based on the string type that is encountered. This affects both docstrings and non-docstrings. <code>strings</code> string <code>False</code> Return <code>SourceText</code> entries for each string (non-docstring). <code>string_types</code> string <code>fu</code> Specifies which string types <code>strings</code> searches: bytes (<code>b</code>), format (<code>f</code>), raw (<code>r</code>), and Unicode (<code>u</code>).  <code>*</code> captures all strings, or when used with a type, captures all variants of that type <code>r*</code>. This does not affect docstrings. When <code>docstrings</code> is enabled, all docstrings are parsed."},{"location":"filters/python/#categories","title":"Categories","text":"<p>Python returns text with the following categories.</p> Category Description <code>py-comments</code> Text captured from inline comments. <code>py-docstring</code> Text captured from docstrings. <code>py-string</code> Text captured from strings."},{"location":"filters/stylesheets/","title":"Stylesheets","text":""},{"location":"filters/stylesheets/#usage","title":"Usage","text":"<p>The Stylesheets plugin is designed to find and return comments in CSS, SCSS, and SASS (CSS does not support inline comments). The filters takes a CSS buffer and returns one or more buffers containing the content of comments.</p> <p>When first in the chain, the filter uses no special encoding detection. It will assume <code>utf-8</code> if no encoding BOM is found, and the user has not overridden the fallback encoding. Text is returned in chunks based on the context of the text: block or inline.</p> <p>You can specify <code>sass</code> or <code>scss</code> in the option <code>stylesheets</code> if you need to capture inline comments.</p> <pre><code>matrix:\n- name: scss\n  pipeline:\n  - pyspelling.filters.stylesheets:\n      stylesheets: scss\n  default_encoding: utf-8\n  sources:\n  - docs/src/scss/*.scss\n  aspell:\n    lang: en\n    d: en_US\n  dictionary:\n    wordlists:\n    - docs/src/dictionary/en-custom.txt\n    output: build/dictionary/scss.dic\n</code></pre>"},{"location":"filters/stylesheets/#options","title":"Options","text":"Options Type Default Description <code>stylesheets</code> string <code>\"css\"</code> The stylesheet mode. <code>block_comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each block comment. <code>line_comments</code> bool <code>True</code> Return <code>SourceText</code> entries for each line comment. <code>group_comments</code> bool <code>False</code> Group consecutive inline comments as one <code>SourceText</code> entry."},{"location":"filters/stylesheets/#categories","title":"Categories","text":"<p>Stylesheets returns text with the following categories depending on what stylesheet mode is enabled. Categories prefixed with <code>css</code> are for CSS etc.</p> Category Description <code>css-block-comment</code> Text captured from CSS block comments. <code>scss-block-comment</code> Text captured from SCSS block comments. <code>scss-line-comment</code> Text captured from SCSS line comments. <code>sass-block-comment</code> Text captured from SASS block comments. <code>sass-line-comment</code> Text captured from SASS line comments."},{"location":"filters/text/","title":"Text","text":""},{"location":"filters/text/#usage","title":"Usage","text":"<p>This is a filter that simply retrieves the buffer's text and returns it as Unicode.  It takes a file or file buffer and returns a single <code>SourceText</code> object containing all the text in the file.  It is the default filter when there is no filter specified, though it can be manually included via <code>pyspelling.filters.text</code>. When first in the chain, the file's default, assumed encoding is <code>utf-8</code> unless otherwise overridden by the user.</p> <p>The Text filter can also be used convert from one encoding to another.</p> <pre><code>matrix:\n- name: text\n  default_encoding: cp1252\n  pipeline:\n  - pyspelling.filters.text:\n      convert_encoding: utf-8\n  source:\n  - \"**/*.txt\"\n</code></pre>"},{"location":"filters/text/#options","title":"Options","text":"Options Type Default Description <code>normalize</code> string <code>''</code> Performs Unicode normalization. Valid values are <code>NFC</code>, <code>NFD</code>, <code>NFKC</code>, and <code>NFKD</code>. <code>convert_encoding</code> string <code>''</code> Assuming a valid encoding, the text will be converted to the specified encoding. <code>errors</code> string <code>'strict'</code> Specifies what to do when converting the encoding, and a character can't be converted. Valid values are <code>strict</code>, <code>ignore</code>, <code>replace</code>, <code>xmlcharrefreplace</code>, <code>backslashreplace</code>, and <code>namereplace</code>."},{"location":"filters/text/#categories","title":"Categories","text":"<p>Text returns text with the following categories.</p> Category Description <code>text</code> The extracted text."},{"location":"filters/url/","title":"URL","text":""},{"location":"filters/url/#usage","title":"Usage","text":"<p>This is a filter that simply strips URLs and/or email address from a file or text buffer. It takes an input buffer and will return the buffer with URLs and/or emails addresses removed.</p> <p>When first in the chain, the file's default, assumed encoding is <code>utf-8</code> unless otherwise overridden by the user.</p> <pre><code>matrix:\n- name: url\n  pipeline:\n  - pyspelling.filters.url:\n  source:\n  - \"**/*.txt\"\n</code></pre>"},{"location":"filters/url/#options","title":"Options","text":"Options Type Default Description <code>urls</code> bool <code>True</code> Enables or disables URL stripping. <code>emails</code> bool <code>True</code> Enables or disables email address stripping."},{"location":"filters/url/#categories","title":"Categories","text":"<p>Text returns text with the following categories.</p> Category Description <code>url-free</code> The text without URLs and/or emails."},{"location":"filters/xml/","title":"XML","text":""},{"location":"filters/xml/#usage","title":"Usage","text":"<p>The XML filter is designed to capture XML content, comments, and even attributes. It allows for filtering out specific tags, and you can even filter them out with CSS selectors (even though this is XML content :slightly_smiling:). The filters takes an XML buffer and returns one or more text buffers containing the content of XML comments, attributes, etc. The returned content should no longer be considered XML.</p> <p>When first in the chain, the XML filter will look for the encoding of the file in its header and convert the buffer to Unicode. It will assume <code>utf-8</code> if no encoding header is found, and the user has not overridden the fallback encoding.</p> <p>The HTML filter uses BeautifulSoup4 to convert the Unicode content to XML structure. Tag content is as one block for the whole file. If enabled, the XML filter will also return chunks for comments and even attributes. Each type of text chunk is returned with their own category type.</p> <p>Tags can be captured or ignored with the <code>captures</code> and <code>ignores</code> options. These options work by employing CSS selectors to target the tags. The CSS selectors are based on a limited subset of CSS4 selectors.</p> <pre><code>matrix:\n- name: xml\n  pipeline:\n  - pyspelling.filters.xml:\n      comments: false\n      attributes:\n      - some-data\n      ignores:\n      - :matches(ignore_tag, [ignore_attribute])\n  sources:\n  - site/*.xml\n</code></pre>"},{"location":"filters/xml/#supported-css-selectors","title":"Supported CSS Selectors","text":"<p>The CSS selectors are based on a limited subset of CSS4 selectors. Support is provided via Soup Sieve. Please reference Soup Sieve's documentation for more info.</p>"},{"location":"filters/xml/#options","title":"Options","text":"Options Type Default Description <code>comments</code> bool <code>True</code> Include comment text in the output. <code>attributes</code> [string] <code>[]</code> Attributes whose content should be included in the output. <code>ignores</code> [string] <code>[]</code> CSS style selectors that identify tags to ignore. Child tags will not be crawled. <code>captures</code> [string] <code>['*|*']</code> CSS style selectors used to narrow which tags that text is collected from. Unlike <code>ignores</code>, tags which text is not captured from still have their children crawled. <code>namespaces</code> dict <code>{}</code> Dictionary containing key value pairs of namespaces to use for CSS selectors (equivalent to <code>@namespace</code> in CSS). Use the an empty string for the key to define default the namespace. See below for example. <code>break_tags</code> [string] <code>[]</code> Tags to break on for context. Causes more calls to the spell checker. <p>Namespace example</p> <pre><code>matrix:\n- name: xml\n  pipeline:\n  - pyspelling.filters.xml:\n      namespaces:\n        \"\": http://www.w3.org/1999/xhtml\n        svg: http://www.w3.org/2000/svg\n        xlink: http://www.w3.org/1999/xlink\n</code></pre>"},{"location":"filters/xml/#categories","title":"Categories","text":"<p>HTML returns text with the following categories.</p> Category Description <code>xml-content</code> Text captured from XML tags. <code>xml-attribute</code> Text captured from XML attributes. <code>xml-comment</code> Text captured from XML comments."},{"location":"flow_control/wildcard/","title":"Wildcard","text":""},{"location":"flow_control/wildcard/#usage","title":"Usage","text":"<p>The Wildcard plugin is a flow control plugin. It uses Wildcard Match's <code>fnmatch</code> library to perform wildcard matches on categories passed to it from the pipeline in order to determine if the text should be passed to the next filter.</p> <p>You can define patterns for the following cases:</p> <ul> <li><code>allow</code>: the chunk of text is allowed to be evaluated by the next filter.</li> <li><code>skip</code>: the chunk of text should skip the next filter.</li> <li><code>halt</code>: halts the progress of the text chunk(s) down the pipeline and sends it directly to the spell checker.</li> </ul> <p>Under each option, you can define a list of different patterns. The plugin will loop through the patterns until it has determined what should be done with the text.</p> <p>The <code>fnmatch</code> library is configured with the <code>NEGATE</code>, <code>BRACE</code> and <code>IGNORECASE</code> flags. It also allows you to specify multiple patterns on one line separated with <code>|</code>.  See Wildcard Match's documentation to learn more about its behavior in regards to features and flags.</p> <p>In this example, we wish to specifically target inline Python text and ignore <code>noqa</code>, <code>pragma</code>, and <code>shebang</code> lines.  So after the Python step, which returns both docstrings and inline comments, we specify that we only want to allow <code>py-comment</code> categories in the next filter. The context filter removes the lines that start with the aforementioned things, and passes the text down the pipe.  The last filter step receives both the <code>context</code> text objects and the <code>py-docstrings</code> from earlier.</p> <pre><code>matrix:\n- name: python\n  sources:\n  - setup.py\n  - pyspelling/**/*.py\n  aspell:\n    lang: en\n  dictionary:\n    wordlists:\n    - docs/src/dictionary/en-custom.txt\n    output: build/dictionary/python.dic\n  pipeline:\n  - pyspelling.filters.python:\n  - pyspelling.flow_control.wildcard:\n      allow:\n      - py-comment\n  - pyspelling.filters.context:\n      context_visible_first: true\n      delimiters:\n      # Ignore lint (noqa) and coverage (pragma) as well as shebang (#!)\n      - open: '^(?: *(?:noqa\\b|pragma: no cover)|!)'\n        close: '$'\n      # Ignore Python encoding string -*- encoding stuff -*-\n      - open: '^ *-\\*-'\n        close: '-\\*-$'\n  - pyspelling.filters.context:\n      context_visible_first: true\n      escapes: \\\\[\\\\`~]\n      delimiters:\n      # Ignore multiline content between fences (fences can have 3 or more back ticks)\n      # ```\n      # content\n      # ```\n      - open: '(?s)^(?P&lt;open&gt; *`{3,})$'\n        close: '^(?P=open)$'\n      # Ignore text between inline back ticks\n      - open: '(?P&lt;open&gt;`+)'\n        close: '(?P=open)'\n</code></pre>"},{"location":"flow_control/wildcard/#options","title":"Options","text":"Options Type Default Description <code>allow</code> [string] <code>[\"*\"]</code> The chunk of text is allowed to be evaluated by the next filter. <code>skip</code> [string] <code>[]</code> The chunk of text should skip the next filter. <code>halt</code> [string] <code>[]</code> Halts the progress of the text chunk down the pipeline and sends it directly to the spell checker."}]}